{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Validation/Test Rigor + Regression Metrics + Baseline Modeling\n",
    "\n",
    "<hr>\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/notebooks/figures/mgmt_474_ai_logo_02-modified.png\" width=\"200\"/>\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "# <center><a class=\"tocSkip\"></center>\n",
    "# <center>MGMT47400 Predictive Analytics</center>\n",
    "# <center>Professor: Davi Moreira </center>\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/blob/main/notebooks/03_regression_metrics_baselines.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Choose regression metrics aligned to business loss (MAE vs RMSE)\n",
    "2. Establish a baseline model and interpret it correctly\n",
    "3. Run holdout evaluation without contaminating the test set\n",
    "4. Use quick diagnostic plots to spot obvious modeling issues\n",
    "5. Document evaluation decisions (metric, split, baseline, assumptions)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\n\nwarnings.filterwarnings('ignore')\npd.set_option('display.precision', 4)\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (10, 6)\n\nRANDOM_SEED = 474\nnp.random.seed(RANDOM_SEED)\n\nprint(\"\u2713 Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data and Create Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "california = fetch_california_housing(as_frame=True)\n",
    "df = california.frame\n",
    "\n",
    "X = df.drop(columns=['MedHouseVal'])\n",
    "y = df['MedHouseVal']\n",
    "\n",
    "# Create splits\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.20, random_state=RANDOM_SEED)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=RANDOM_SEED)\n",
    "\n",
    "print(f\"Train: {len(X_train)} | Validation: {len(X_val)} | Test: {len(X_test)} (LOCKED)\")\n",
    "print(f\"\\n\u26a0\ufe0f TEST SET IS LOCKED - Do not use until final evaluation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Regression Metrics: MAE, RMSE, R\u00b2\n",
    "\n",
    "### 3.1 Understanding the Metrics\n",
    "\n",
    "**Mean Absolute Error (MAE)**\n",
    "- Average of absolute differences\n",
    "- Same units as target\n",
    "- Less sensitive to outliers\n",
    "- Use when: All errors cost the same\n",
    "\n",
    "**Root Mean Squared Error (RMSE)**\n",
    "- Square root of average squared differences\n",
    "- Same units as target\n",
    "- More sensitive to outliers (penalizes large errors)\n",
    "- Use when: Large errors are disproportionately costly\n",
    "\n",
    "**R\u00b2 (Coefficient of Determination)**\n",
    "- Proportion of variance explained\n",
    "- Scale: 0 (no better than mean) to 1 (perfect)\n",
    "- Can be negative if model is worse than mean\n",
    "- Use when: You want a relative improvement metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_regression(y_true, y_pred, name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Compute standard regression metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True target values\n",
    "    y_pred : array-like\n",
    "        Predicted values\n",
    "    name : str\n",
    "        Name for reporting\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary of metrics\n",
    "    \"\"\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    metrics = {\n",
    "        'Model': name,\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'R\u00b2': r2\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"\u2713 Evaluation function created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baseline Models\n",
    "\n",
    "### 4.1 Why Baselines Matter\n",
    "\n",
    "A baseline model provides a reference point. Any model should beat these simple strategies:\n",
    "- **Mean baseline**: Predict the training set mean for all samples\n",
    "- **Median baseline**: Predict the training set median for all samples\n",
    "\n",
    "If your model doesn't beat the baseline, something is wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean baseline\n",
    "baseline_mean = DummyRegressor(strategy='mean')\n",
    "baseline_mean.fit(X_train, y_train)\n",
    "y_pred_mean_train = baseline_mean.predict(X_train)\n",
    "y_pred_mean_val = baseline_mean.predict(X_val)\n",
    "\n",
    "# Median baseline\n",
    "baseline_median = DummyRegressor(strategy='median')\n",
    "baseline_median.fit(X_train, y_train)\n",
    "y_pred_median_train = baseline_median.predict(X_train)\n",
    "y_pred_median_val = baseline_median.predict(X_val)\n",
    "\n",
    "print(\"\u2713 Baseline models fitted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd PAUSE-AND-DO Exercise 1 (10 minutes)\n",
    "\n",
    "**Task:** Write `evaluate_regression(y_true, y_pred)` returning MAE/RMSE/R\u00b2.\n",
    "\n",
    "The function is already implemented above. Now:\n",
    "1. Use it to evaluate both baselines on train and validation sets\n",
    "2. Create a comparison table\n",
    "3. Interpret the results\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate baselines\n",
    "results = []\n",
    "\n",
    "# Mean baseline - train\n",
    "results.append(evaluate_regression(y_train, y_pred_mean_train, \"Mean Baseline (Train)\"))\n",
    "# Mean baseline - validation\n",
    "results.append(evaluate_regression(y_val, y_pred_mean_val, \"Mean Baseline (Val)\"))\n",
    "\n",
    "# Median baseline - train\n",
    "results.append(evaluate_regression(y_train, y_pred_median_train, \"Median Baseline (Train)\"))\n",
    "# Median baseline - validation\n",
    "results.append(evaluate_regression(y_val, y_pred_median_val, \"Median Baseline (Val)\"))\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"=== BASELINE COMPARISON ===\")\n",
    "print(results_df)\n",
    "\n",
    "print(f\"\\n\ud83d\udca1 Key insights:\")\n",
    "print(f\"  - R\u00b2 = 0 means the model is no better than predicting the mean\")\n",
    "print(f\"  - MAE = {results_df.loc[1, 'MAE']:.3f} means on average we're off by ${results_df.loc[1, 'MAE']:.1f}00,000\")\n",
    "print(f\"  - RMSE > MAE indicates some large errors in predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Simple Linear Model\n",
    "\n",
    "Now let's fit a simple linear regression and compare to baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline with scaling + linear regression\n",
    "linear_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Fit on training data\n",
    "linear_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_linear_train = linear_pipeline.predict(X_train)\n",
    "y_pred_linear_val = linear_pipeline.predict(X_val)\n",
    "\n",
    "print(\"\u2713 Linear model fitted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd PAUSE-AND-DO Exercise 2 (10 minutes)\n",
    "\n",
    "**Task:** Compare baseline vs linear regression and interpret the delta.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add linear model results\n",
    "results.append(evaluate_regression(y_train, y_pred_linear_train, \"Linear Regression (Train)\"))\n",
    "results.append(evaluate_regression(y_val, y_pred_linear_val, \"Linear Regression (Val)\"))\n",
    "\n",
    "# Create full comparison table\n",
    "full_results_df = pd.DataFrame(results)\n",
    "print(\"=== FULL MODEL COMPARISON ===\")\n",
    "print(full_results_df)\n",
    "\n",
    "# Calculate improvements\n",
    "baseline_mae = full_results_df.loc[full_results_df['Model'] == 'Mean Baseline (Val)', 'MAE'].values[0]\n",
    "linear_mae = full_results_df.loc[full_results_df['Model'] == 'Linear Regression (Val)', 'MAE'].values[0]\n",
    "improvement_pct = ((baseline_mae - linear_mae) / baseline_mae) * 100\n",
    "\n",
    "print(f\"\\n=== IMPROVEMENT OVER BASELINE ===\")\n",
    "print(f\"Baseline MAE: {baseline_mae:.4f}\")\n",
    "print(f\"Linear MAE: {linear_mae:.4f}\")\n",
    "print(f\"Improvement: {improvement_pct:.2f}%\")\n",
    "print(f\"\\nLinear R\u00b2 on validation: {full_results_df.loc[full_results_df['Model'] == 'Linear Regression (Val)', 'R\u00b2'].values[0]:.4f}\")\n",
    "print(f\"This means the model explains {full_results_df.loc[full_results_df['Model'] == 'Linear Regression (Val)', 'R\u00b2'].values[0]*100:.2f}% of variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR INTERPRETATION HERE:\n",
    "\n",
    "**Observation 1: Performance**  \n",
    "[How much better is linear vs baseline?]\n",
    "\n",
    "**Observation 2: Overfitting Check**  \n",
    "[Compare train vs validation scores - is there overfitting?]\n",
    "\n",
    "**Observation 3: Business Context**  \n",
    "[What does this MAE mean in practical terms?]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Residual Analysis\n",
    "\n",
    "Residuals = True values - Predicted values\n",
    "\n",
    "Good residuals should:\n",
    "- Be centered around zero\n",
    "- Show no patterns (random scatter)\n",
    "- Have roughly constant variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals\n",
    "residuals_train = y_train - y_pred_linear_train\n",
    "residuals_val = y_val - y_pred_linear_val\n",
    "\n",
    "# Create residual plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Residuals vs Predicted (Train)\n",
    "axes[0, 0].scatter(y_pred_linear_train, residuals_train, alpha=0.5)\n",
    "axes[0, 0].axhline(y=0, color='r', linestyle='--')\n",
    "axes[0, 0].set_xlabel('Predicted Values')\n",
    "axes[0, 0].set_ylabel('Residuals')\n",
    "axes[0, 0].set_title('Residual Plot - Training Set')\n",
    "\n",
    "# Plot 2: Residuals vs Predicted (Validation)\n",
    "axes[0, 1].scatter(y_pred_linear_val, residuals_val, alpha=0.5, color='orange')\n",
    "axes[0, 1].axhline(y=0, color='r', linestyle='--')\n",
    "axes[0, 1].set_xlabel('Predicted Values')\n",
    "axes[0, 1].set_ylabel('Residuals')\n",
    "axes[0, 1].set_title('Residual Plot - Validation Set')\n",
    "\n",
    "# Plot 3: Distribution of Residuals (Train)\n",
    "axes[1, 0].hist(residuals_train, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].axvline(x=0, color='r', linestyle='--')\n",
    "axes[1, 0].set_xlabel('Residual')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Residual Distribution - Training')\n",
    "\n",
    "# Plot 4: Distribution of Residuals (Validation)\n",
    "axes[1, 1].hist(residuals_val, bins=50, alpha=0.7, color='orange', edgecolor='black')\n",
    "axes[1, 1].axvline(x=0, color='r', linestyle='--')\n",
    "axes[1, 1].set_xlabel('Residual')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Residual Distribution - Validation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=== RESIDUAL STATISTICS ===\")\n",
    "print(f\"Training residuals: mean={residuals_train.mean():.6f}, std={residuals_train.std():.4f}\")\n",
    "print(f\"Validation residuals: mean={residuals_val.mean():.6f}, std={residuals_val.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Predicted vs Actual Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted vs Actual\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training set\n",
    "axes[0].scatter(y_train, y_pred_linear_train, alpha=0.5)\n",
    "axes[0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)\n",
    "axes[0].set_xlabel('Actual Values')\n",
    "axes[0].set_ylabel('Predicted Values')\n",
    "axes[0].set_title('Training Set: Predicted vs Actual')\n",
    "\n",
    "# Validation set\n",
    "axes[1].scatter(y_val, y_pred_linear_val, alpha=0.5, color='orange')\n",
    "axes[1].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2)\n",
    "axes[1].set_xlabel('Actual Values')\n",
    "axes[1].set_ylabel('Predicted Values')\n",
    "axes[1].set_title('Validation Set: Predicted vs Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\ud83d\udca1 Points close to the red line indicate good predictions\")\n",
    "print(\"   Points far from the line show prediction errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Set Lockbox Discipline\n",
    "\n",
    "### The Test Set Rule:\n",
    "\n",
    "> **\"Touch the test set ONCE, at the very end\"**\n",
    "\n",
    "**Why?**\n",
    "- Every time you look at test performance, you risk making decisions based on it\n",
    "- This causes \"leakage\" of test information into your modeling choices\n",
    "- The test set must remain a true \"unseen\" evaluation\n",
    "\n",
    "**What to do instead:**\n",
    "- Use validation set for all model selection and tuning\n",
    "- Use cross-validation for robust comparisons\n",
    "- Only evaluate on test when you're completely done\n",
    "\n",
    "**Warning signs you're peeking:**\n",
    "- \"Let me just check test performance real quick\"\n",
    "- \"The test score is lower, let me adjust...\"\n",
    "- Running multiple experiments and looking at test each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== TEST SET STATUS ===\")\nprint(f\"Test set size: {len(X_test)} samples\")\nprint(f\"Test set is LOCKED \ud83d\udd12\")\nprint(f\"\\n\u2713 We will NOT evaluate on test until the final submission\")\nprint(f\"\u2713 All model development uses only train + validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluation Documentation Template\n",
    "\n",
    "Every evaluation should document:\n",
    "\n",
    "### Evaluation Plan\n",
    "\n",
    "**Primary Metric:** [MAE / RMSE / R\u00b2]  \n",
    "**Rationale:** [Why this metric aligns with business goals]\n",
    "\n",
    "**Split Strategy:**\n",
    "- Training: 60% (for fitting)\n",
    "- Validation: 20% (for selection)\n",
    "- Test: 20% (for final evaluation)\n",
    "\n",
    "**Baseline:** [Mean / Median predictor]  \n",
    "**Baseline Performance:** [Validation score]\n",
    "\n",
    "**Model:** [Linear Regression with Standard Scaling]  \n",
    "**Model Performance:** [Validation score]  \n",
    "**Improvement over Baseline:** [Percentage or absolute difference]\n",
    "\n",
    "**Assumptions:**\n",
    "- Features are available at prediction time\n",
    "- Relationship is approximately linear\n",
    "- No major data quality issues\n",
    "\n",
    "**Risks:**\n",
    "- Model may not generalize to different time periods\n",
    "- Assumes no distribution shift\n",
    "- Sensitive to outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Wrap-Up: Key Takeaways\n\n### What We Learned Today:\n\n1. **Metrics Matter**: Choose MAE/RMSE/R\u00b2 based on business loss function\n2. **Baselines First**: Always establish a simple baseline for comparison\n3. **Holdout Discipline**: Train on train, evaluate on validation, lock test away\n4. **Residuals Tell Stories**: Use diagnostic plots to spot issues\n5. **Document Everything**: Clear evaluation plans prevent confusion later\n\n### Critical Rules:\n\n> **\"If your model can't beat the mean, debug before proceeding\"**\n\n> **\"The test set is a lockbox - open it once\"**\n\n### Next Steps:\n\n- Next notebook: Linear regression with features and diagnostics\n- We'll build on today's evaluation framework\n- Start thinking about which metrics matter for your project\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Submission Instructions\n\n### To Submit:\n\n1. Run all cells\n2. Complete both exercises\n3. Write a 3-sentence evaluation note answering:\n   - Which metric is most appropriate for this problem and why?\n   - How much better is linear regression than the baseline?\n   - What does the residual analysis suggest?\n4. Submit Colab link + evaluation note in LMS\n\n---\n\n## Bibliography\n\n- James, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). *An Introduction to Statistical Learning with Python* - Chapter on Model Assessment and Selection\n- Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning* - Test error, training error, bias-variance\n- scikit-learn User Guide: [Regression metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics)\n- scikit-learn User Guide: [Common pitfalls](https://scikit-learn.org/stable/common_pitfalls.html)\n\n---\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "Thank you!\n",
    "\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}