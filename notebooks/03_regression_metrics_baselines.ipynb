{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Validation/Test Rigor + Regression Metrics + Baseline Modeling\n",
    "\n",
    "<hr>\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/notebooks/figures/mgmt_474_ai_logo_02-modified.png\" width=\"200\"/>\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "# <center><a class=\"tocSkip\"></center>\n",
    "# <center>MGMT47400 Predictive Analytics</center>\n",
    "# <center>Professor: Davi Moreira </center>\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/blob/main/notebooks/03_regression_metrics_baselines.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Choose regression metrics aligned to business loss (MAE vs RMSE)\n",
    "2. Establish a baseline model and interpret it correctly\n",
    "3. Run holdout evaluation without contaminating the test set\n",
    "4. Use quick diagnostic plots to spot obvious modeling issues\n",
    "5. Document evaluation decisions (metric, split, baseline, assumptions)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "> **ðŸ“‹ Participation Reminder:** This notebook contains **2 PAUSE-AND-DO exercises**. You are expected to complete all exercises before submitting your notebook.\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Before we can measure model quality, we need the standard scientific-Python stack: **pandas** for tabular data, **NumPy** for numerics, **matplotlib/seaborn** for plotting, and several **scikit-learn** modules for splitting, scoring, and modeling.\n",
    "All random operations use `RANDOM_SEED = 474` so every student sees identical results.\n",
    "Run the cell below once; the checkmark confirms every library loaded successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\n\nwarnings.filterwarnings('ignore')\npd.set_option('display.precision', 4)\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (10, 6)\n\nRANDOM_SEED = 474\nnp.random.seed(RANDOM_SEED)\n\nprint(\"âœ“ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The single line `Setup complete!` confirms that all imports succeeded and the environment is ready. Behind the scenes, the cell loaded **pandas**, **NumPy**, **matplotlib**, **seaborn**, and five scikit-learn modules (`train_test_split`, three metric functions, `DummyRegressor`, `LinearRegression`, `Pipeline`, and `StandardScaler`). The global random seed is set to **RANDOM_SEED = 474**, which controls every random operation in this notebook so that your numbers match the solutions exactly.\n",
    "\n",
    "**Why this matters:** If any import had failed, you would see a `ModuleNotFoundError` here instead of the checkmark. Catching import problems at the top of the notebook prevents cryptic errors later.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data and Create Splits\n",
    "\n",
    "We use the California Housing dataset, which contains **20,640 census-tract records** with 8 numeric features (median income, average rooms, latitude, etc.) and a continuous target: **median house value** in units of $100,000.\n",
    "The data is split 60/20/20 into train, validation, and test sets using our course-standard `RANDOM_SEED = 474`.\n",
    "The test set is immediately locked away; all model development proceeds on train + validation only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Gemini Prompt:** \"Load the California Housing dataset from scikit-learn as a DataFrame. Separate the target column 'MedHouseVal' from the features. Then split the data into train (60%), validation (20%), and test (20%) sets using train_test_split with random_state=474. Print the sizes of each split.\"\n",
    ">\n",
    "> **After running, verify:**\n",
    "> - Train set is ~60% of total samples (about 12,384 rows)\n",
    "> - Validation and test sets are each ~20% (about 4,128 rows)\n",
    "> - The target variable `y` contains median house values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "california = fetch_california_housing(as_frame=True)\n",
    "df = california.frame\n",
    "\n",
    "X = df.drop(columns=['MedHouseVal'])\n",
    "y = df['MedHouseVal']\n",
    "\n",
    "# Create splits\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.20, random_state=RANDOM_SEED)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=RANDOM_SEED)\n",
    "\n",
    "print(f\"Train: {len(X_train)} | Validation: {len(X_val)} | Test: {len(X_test)} (LOCKED)\")\n",
    "print(f\"\\nâš ï¸ TEST SET IS LOCKED - Do not use until final evaluation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The split summary shows three partition sizes that sum to **20,640** total samples: roughly **12,384 training**, **4,128 validation**, and **4,128 test** records (the exact 60/20/20 ratio). The warning `TEST SET IS LOCKED` is intentional: it reminds you that the test partition must remain untouched until the very last evaluation. Any intermediate peeking at test performance would leak information into your model-selection decisions and inflate your reported results.\n",
    "\n",
    "**Key takeaway:** Memorize the counts. When you later compute metrics and something looks wrong, come back here to verify you are evaluating on the correct set (validation, not test).\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Regression Metrics: MAE, RMSE, RÂ²\n",
    "\n",
    "### 3.1 Understanding the Metrics\n",
    "\n",
    "**Mean Absolute Error (MAE)**\n",
    "- Average of absolute differences\n",
    "- Same units as target\n",
    "- Less sensitive to outliers\n",
    "- Use when: All errors cost the same\n",
    "\n",
    "**Root Mean Squared Error (RMSE)**\n",
    "- Square root of average squared differences\n",
    "- Same units as target\n",
    "- More sensitive to outliers (penalizes large errors)\n",
    "- Use when: Large errors are disproportionately costly\n",
    "\n",
    "**RÂ² (Coefficient of Determination)**\n",
    "- Proportion of variance explained\n",
    "- Scale: 0 (no better than mean) to 1 (perfect)\n",
    "- Can be negative if model is worse than mean\n",
    "- Use when: You want a relative improvement metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Gemini Prompt:** \"Write a Python function called `evaluate_regression` that takes `y_true`, `y_pred`, and an optional `name` parameter. It should compute MAE (mean absolute error), RMSE (root mean squared error), and R-squared. Return the results as a dictionary with keys 'Model', 'MAE', 'RMSE', and 'RÂ²'.\"\n",
    ">\n",
    "> **After running, verify:**\n",
    "> - Function returns a dictionary with exactly 4 keys: 'Model', 'MAE', 'RMSE', 'RÂ²'\n",
    "> - RMSE is computed as the square root of mean_squared_error (not MSE itself)\n",
    "> - Function uses scikit-learn's `mean_absolute_error`, `mean_squared_error`, and `r2_score`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_regression(y_true, y_pred, name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Compute standard regression metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True target values\n",
    "    y_pred : array-like\n",
    "        Predicted values\n",
    "    name : str\n",
    "        Name for reporting\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary of metrics\n",
    "    \"\"\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    metrics = {\n",
    "        'Model': name,\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'RÂ²': r2\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"âœ“ Evaluation function created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The message `Evaluation function created` tells you the helper `evaluate_regression()` is now available. This function accepts true values, predicted values, and a label, then returns a dictionary with three metrics: **MAE** (mean absolute error), **RMSE** (root mean squared error), and **RÂ²** (coefficient of determination). No predictions have been made yet; the function simply lives in memory, ready to score any model you hand it.\n",
    "\n",
    "**Why this matters:** Centralizing metric computation in one function prevents copy-paste errors. Every model in this notebook will be scored through the same function, guaranteeing consistent comparisons.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baseline Models\n",
    "\n",
    "### 4.1 Why Baselines Matter\n",
    "\n",
    "A baseline model provides a reference point. Any model should beat these simple strategies:\n",
    "- **Mean baseline**: Predict the training set mean for all samples\n",
    "- **Median baseline**: Predict the training set median for all samples\n",
    "\n",
    "If your model doesn't beat the baseline, something is wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Gemini Prompt:** \"Create two baseline models using scikit-learn's `DummyRegressor`: one with strategy='mean' and one with strategy='median'. Fit each on the training data, then generate predictions for both the training and validation sets.\"\n",
    ">\n",
    "> **After running, verify:**\n",
    "> - Both `DummyRegressor` models are fitted on `X_train` and `y_train`\n",
    "> - Predictions are generated for both train and validation sets (4 prediction arrays total)\n",
    "> - Mean baseline predictions should all be the same value (the training set mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean baseline\n",
    "baseline_mean = DummyRegressor(strategy='mean')\n",
    "baseline_mean.fit(X_train, y_train)\n",
    "y_pred_mean_train = baseline_mean.predict(X_train)\n",
    "y_pred_mean_val = baseline_mean.predict(X_val)\n",
    "\n",
    "# Median baseline\n",
    "baseline_median = DummyRegressor(strategy='median')\n",
    "baseline_median.fit(X_train, y_train)\n",
    "y_pred_median_train = baseline_median.predict(X_train)\n",
    "y_pred_median_val = baseline_median.predict(X_val)\n",
    "\n",
    "print(\"âœ“ Baseline models fitted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The message `Baseline models fitted` confirms that two `DummyRegressor` objects are now trained: one with `strategy='mean'` and one with `strategy='median'`. Despite sounding trivial, fitting a dummy regressor is necessary so that scikit-learn stores the learned constant (the training-set mean or median) and can later call `.predict()` consistently. These baselines represent the floor of performance: any real model must beat them.\n",
    "\n",
    "**Key takeaway:** Baselines require almost no computation, yet they give you the single most important reference number in your evaluation. Always establish a baseline before building complex models.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ PAUSE-AND-DO Exercise 1 (5 minutes)\n",
    "\n",
    "**Task:** Write `evaluate_regression(y_true, y_pred)` returning MAE/RMSE/RÂ².\n",
    "\n",
    "The function is already implemented above. Now:\n",
    "1. Use it to evaluate both baselines on train and validation sets\n",
    "2. Create a comparison table\n",
    "3. Interpret the results\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Gemini Prompt:** \"Evaluate both baseline models (mean and median) on training and validation sets using the `evaluate_regression` function. Collect results into a list, convert to a DataFrame, and print a comparison table. Also print the MAE interpretation in dollar terms (values are in $100,000s).\"\n",
    ">\n",
    "> **After running, verify:**\n",
    "> - Results DataFrame has 4 rows: Mean Baseline (Train), Mean Baseline (Val), Median Baseline (Train), Median Baseline (Val)\n",
    "> - RÂ² is 0.0 (or near 0) for the mean baseline on training data\n",
    "> - MAE values are displayed and interpreted in context (multiply by $100,000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate baselines\n",
    "results = []\n",
    "\n",
    "# Mean baseline - train\n",
    "results.append(evaluate_regression(y_train, y_pred_mean_train, \"Mean Baseline (Train)\"))\n",
    "# Mean baseline - validation\n",
    "results.append(evaluate_regression(y_val, y_pred_mean_val, \"Mean Baseline (Val)\"))\n",
    "\n",
    "# Median baseline - train\n",
    "results.append(evaluate_regression(y_train, y_pred_median_train, \"Median Baseline (Train)\"))\n",
    "# Median baseline - validation\n",
    "results.append(evaluate_regression(y_val, y_pred_median_val, \"Median Baseline (Val)\"))\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"=== BASELINE COMPARISON ===\")\n",
    "print(results_df)\n",
    "\n",
    "print(f\"\\nðŸ’¡ Key insights:\")\n",
    "print(f\"  - RÂ² = 0 means the model is no better than predicting the mean\")\n",
    "print(f\"  - MAE = {results_df.loc[1, 'MAE']:.3f} means on average we're off by ${results_df.loc[1, 'MAE']:.1f}00,000\")\n",
    "print(f\"  - RMSE > MAE indicates some large errors in predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The comparison table shows four rows: mean and median baselines, each scored on both train and validation sets. Notice that **RÂ² = 0.00** (or very close) for both baselines. This is expected: by definition, predicting the mean explains zero variance. The **MAE** column tells you the average prediction error in the original units ($100k). For example, if MAE is roughly **0.91**, that means the mean-predictor is off by about **$91,000** per census tract on average. **RMSE** is larger than MAE, indicating that some predictions have especially large errors that the squared penalty amplifies.\n",
    "\n",
    "**Why this matters:** These baseline numbers become your \"bar to clear.\" When you evaluate linear regression next, you will compare its MAE and RÂ² directly to these rows. Any model that cannot beat the mean predictor adds no value.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Simple Linear Model\n",
    "\n",
    "Baselines tell us how well \"doing nothing smart\" performs. Now let's see if even the simplest machine-learning model, ordinary linear regression, can beat the guess-the-mean strategy.\n",
    "We wrap a `StandardScaler` and `LinearRegression` inside a scikit-learn `Pipeline` so that scaling and fitting happen in a single reproducible step.\n",
    "If the linear model cannot improve over the mean baseline, something fundamental is wrong with our features or our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Gemini Prompt:** \"Create a scikit-learn Pipeline that first applies `StandardScaler` and then fits a `LinearRegression` model. Fit it on the training data and generate predictions for both training and validation sets.\"\n",
    ">\n",
    "> **After running, verify:**\n",
    "> - Pipeline has two steps: 'scaler' (StandardScaler) and 'regressor' (LinearRegression)\n",
    "> - Pipeline is fitted on `X_train` and `y_train` only\n",
    "> - Predictions are generated for both train and validation sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline with scaling + linear regression\n",
    "linear_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Fit on training data\n",
    "linear_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_linear_train = linear_pipeline.predict(X_train)\n",
    "y_pred_linear_val = linear_pipeline.predict(X_val)\n",
    "\n",
    "print(\"âœ“ Linear model fitted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The confirmation `Linear model fitted` means scikit-learn has estimated coefficients for all 8 features inside a `Pipeline` that first applies `StandardScaler` (zero-mean, unit-variance) and then fits `LinearRegression`. No metrics are printed yet; this cell only fits the model and stores predictions. The scaling step is critical because it ensures that coefficient magnitudes are comparable across features with very different scales (e.g., latitude in degrees vs. income in tens of thousands).\n",
    "\n",
    "**Key takeaway:** Wrapping preprocessing and modeling inside a single `Pipeline` prevents a common mistake called *fit-transform leakage*, where the scaler accidentally sees validation data during fitting.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ PAUSE-AND-DO Exercise 2 (5 minutes)\n",
    "\n",
    "**Task:** Compare baseline vs linear regression and interpret the delta.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Gemini Prompt:** \"Evaluate the linear regression model on train and validation sets using `evaluate_regression`, append to the existing results list, and create a full comparison DataFrame. Calculate the percentage improvement in MAE of linear regression over the mean baseline on the validation set.\"\n",
    ">\n",
    "> **After running, verify:**\n",
    "> - Full results DataFrame has 6 rows (4 baseline + 2 linear regression)\n",
    "> - Linear regression RÂ² on validation is substantially above 0 (model learns patterns)\n",
    "> - Improvement percentage is computed as `(baseline_MAE - linear_MAE) / baseline_MAE * 100`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add linear model results\n",
    "results.append(evaluate_regression(y_train, y_pred_linear_train, \"Linear Regression (Train)\"))\n",
    "results.append(evaluate_regression(y_val, y_pred_linear_val, \"Linear Regression (Val)\"))\n",
    "\n",
    "# Create full comparison table\n",
    "full_results_df = pd.DataFrame(results)\n",
    "print(\"=== FULL MODEL COMPARISON ===\")\n",
    "print(full_results_df)\n",
    "\n",
    "# Calculate improvements\n",
    "baseline_mae = full_results_df.loc[full_results_df['Model'] == 'Mean Baseline (Val)', 'MAE'].values[0]\n",
    "linear_mae = full_results_df.loc[full_results_df['Model'] == 'Linear Regression (Val)', 'MAE'].values[0]\n",
    "improvement_pct = ((baseline_mae - linear_mae) / baseline_mae) * 100\n",
    "\n",
    "print(f\"\\n=== IMPROVEMENT OVER BASELINE ===\")\n",
    "print(f\"Baseline MAE: {baseline_mae:.4f}\")\n",
    "print(f\"Linear MAE: {linear_mae:.4f}\")\n",
    "print(f\"Improvement: {improvement_pct:.2f}%\")\n",
    "print(f\"\\nLinear RÂ² on validation: {full_results_df.loc[full_results_df['Model'] == 'Linear Regression (Val)', 'RÂ²'].values[0]:.4f}\")\n",
    "print(f\"This means the model explains {full_results_df.loc[full_results_df['Model'] == 'Linear Regression (Val)', 'RÂ²'].values[0]*100:.2f}% of variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The full comparison table now includes six rows: train and validation scores for the mean baseline, median baseline, and linear regression. The key numbers to focus on are the validation columns. Linear regression should show a dramatically lower **MAE** and a positive **RÂ²** (likely around **0.59--0.61**), whereas both baselines sit at RÂ² near zero. The improvement percentage printed below the table quantifies the MAE drop; a figure around **30--40%** is typical for this dataset. Also compare **train vs. validation RÂ²** for linear regression: a small gap (< 0.02) suggests minimal overfitting.\n",
    "\n",
    "**Why this matters:** This table is the prototype for every model comparison you will build in this course. The pattern -- baselines first, then progressively stronger models, always checking validation -- is the core workflow of applied predictive analytics.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR INTERPRETATION HERE:\n",
    "\n",
    "**Observation 1: Performance**  \n",
    "[How much better is linear vs baseline?]\n",
    "\n",
    "**Observation 2: Overfitting Check**  \n",
    "[Compare train vs validation scores - is there overfitting?]\n",
    "\n",
    "**Observation 3: Business Context**  \n",
    "[What does this MAE mean in practical terms?]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Residual Analysis\n",
    "\n",
    "Residuals = True values - Predicted values\n",
    "\n",
    "Good residuals should:\n",
    "- Be centered around zero\n",
    "- Show no patterns (random scatter)\n",
    "- Have roughly constant variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Gemini Prompt:** \"Calculate residuals (actual minus predicted) for the linear regression model on both training and validation sets. Create a 2x2 subplot figure: top row shows residuals vs predicted values (scatter with y=0 reference line) for train and validation; bottom row shows residual histograms for train and validation. Print residual mean and standard deviation.\"\n",
    ">\n",
    "> **After running, verify:**\n",
    "> - Residuals are computed as `y_true - y_pred` (not the reverse)\n",
    "> - All four subplots display correctly: two scatter plots (top) and two histograms (bottom)\n",
    "> - Residual means are very close to zero (especially for the training set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals\n",
    "residuals_train = y_train - y_pred_linear_train\n",
    "residuals_val = y_val - y_pred_linear_val\n",
    "\n",
    "# Create residual plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Residuals vs Predicted (Train)\n",
    "axes[0, 0].scatter(y_pred_linear_train, residuals_train, alpha=0.5)\n",
    "axes[0, 0].axhline(y=0, color='r', linestyle='--')\n",
    "axes[0, 0].set_xlabel('Predicted Values')\n",
    "axes[0, 0].set_ylabel('Residuals')\n",
    "axes[0, 0].set_title('Residual Plot - Training Set')\n",
    "\n",
    "# Plot 2: Residuals vs Predicted (Validation)\n",
    "axes[0, 1].scatter(y_pred_linear_val, residuals_val, alpha=0.5, color='orange')\n",
    "axes[0, 1].axhline(y=0, color='r', linestyle='--')\n",
    "axes[0, 1].set_xlabel('Predicted Values')\n",
    "axes[0, 1].set_ylabel('Residuals')\n",
    "axes[0, 1].set_title('Residual Plot - Validation Set')\n",
    "\n",
    "# Plot 3: Distribution of Residuals (Train)\n",
    "axes[1, 0].hist(residuals_train, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].axvline(x=0, color='r', linestyle='--')\n",
    "axes[1, 0].set_xlabel('Residual')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Residual Distribution - Training')\n",
    "\n",
    "# Plot 4: Distribution of Residuals (Validation)\n",
    "axes[1, 1].hist(residuals_val, bins=50, alpha=0.7, color='orange', edgecolor='black')\n",
    "axes[1, 1].axvline(x=0, color='r', linestyle='--')\n",
    "axes[1, 1].set_xlabel('Residual')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Residual Distribution - Validation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=== RESIDUAL STATISTICS ===\")\n",
    "print(f\"Training residuals: mean={residuals_train.mean():.6f}, std={residuals_train.std():.4f}\")\n",
    "print(f\"Validation residuals: mean={residuals_val.mean():.6f}, std={residuals_val.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The 2x2 grid shows four panels. **Top row**: residuals (true minus predicted) plotted against predicted values for train (left) and validation (right). Ideally you want a formless cloud centered on the red zero-line; any visible pattern (funnel, curve, clustering) signals a problem. **Bottom row**: histograms of residuals for train and validation. A roughly bell-shaped distribution centered near zero is good; heavy right tails suggest the model under-predicts expensive properties. The printed statistics confirm that the mean residual is near zero (by construction for OLS) and report the standard deviation, which should approximately match the RMSE from earlier.\n",
    "\n",
    "**Key takeaway:** If you see a clear funnel shape (variance increasing with predicted value), the model exhibits *heteroskedasticity*, meaning errors are not constant across the prediction range. This is common in housing data, where price variability is higher for expensive areas.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Predicted vs Actual Plot\n",
    "\n",
    "A predicted-vs-actual scatter plot is the most intuitive diagnostic for a regression model. Every point represents one sample; the x-axis is the true value and the y-axis is the prediction.\n",
    "Perfect predictions fall exactly on the red 45-degree diagonal. Systematic deviations from the line reveal regions where the model consistently over- or under-predicts.\n",
    "We plot train and validation side by side to check whether the model generalizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Gemini Prompt:** \"Create a side-by-side predicted vs actual scatter plot for the linear regression model. Plot training set on the left and validation set on the right. Add a red dashed 45-degree reference line (perfect predictions) on each subplot.\"\n",
    ">\n",
    "> **After running, verify:**\n",
    "> - The 45-degree reference line goes from min to max of actual values\n",
    "> - Points clustered near the red line indicate good predictions\n",
    "> - Training and validation plots show similar scatter patterns (no severe overfitting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted vs Actual\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training set\n",
    "axes[0].scatter(y_train, y_pred_linear_train, alpha=0.5)\n",
    "axes[0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)\n",
    "axes[0].set_xlabel('Actual Values')\n",
    "axes[0].set_ylabel('Predicted Values')\n",
    "axes[0].set_title('Training Set: Predicted vs Actual')\n",
    "\n",
    "# Validation set\n",
    "axes[1].scatter(y_val, y_pred_linear_val, alpha=0.5, color='orange')\n",
    "axes[1].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2)\n",
    "axes[1].set_xlabel('Actual Values')\n",
    "axes[1].set_ylabel('Predicted Values')\n",
    "axes[1].set_title('Validation Set: Predicted vs Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ’¡ Points close to the red line indicate good predictions\")\n",
    "print(\"   Points far from the line show prediction errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The two scatter plots compare true values (x-axis) to predictions (y-axis) for training and validation. Points hugging the red 45-degree line indicate accurate predictions. You will likely notice a horizontal \"ceiling\" near the maximum target value (~5.0, representing $500k), where the model clips because California Housing caps median values. Dispersion around the line is wider at higher actual values, consistent with the heteroskedasticity observed in the residual plots.\n",
    "\n",
    "**Why this matters:** This plot is the fastest way to communicate model quality to a non-technical stakeholder. A tight cluster along the diagonal is immediately interpretable as \"the model works.\" The visible ceiling effect also warns that the dataset has a truncation issue that no linear model can fully resolve.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Set Lockbox Discipline\n",
    "\n",
    "### The Test Set Rule:\n",
    "\n",
    "> **\"Touch the test set ONCE, at the very end\"**\n",
    "\n",
    "**Why?**\n",
    "- Every time you look at test performance, you risk making decisions based on it\n",
    "- This causes \"leakage\" of test information into your modeling choices\n",
    "- The test set must remain a true \"unseen\" evaluation\n",
    "\n",
    "**What to do instead:**\n",
    "- Use validation set for all model selection and tuning\n",
    "- Use cross-validation for robust comparisons\n",
    "- Only evaluate on test when you're completely done\n",
    "\n",
    "**Warning signs you're peeking:**\n",
    "- \"Let me just check test performance real quick\"\n",
    "- \"The test score is lower, let me adjust...\"\n",
    "- Running multiple experiments and looking at test each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== TEST SET STATUS ===\")\nprint(f\"Test set size: {len(X_test)} samples\")\nprint(f\"Test set is LOCKED ðŸ”’\")\nprint(f\"\\nâœ“ We will NOT evaluate on test until the final submission\")\nprint(f\"âœ“ All model development uses only train + validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The printout confirms that the test set contains **4,128 samples** and remains locked. No test-set metrics have been computed anywhere in this notebook. This discipline is central to honest evaluation: the test set is a one-time, final report card. If you evaluate on it repeatedly and adjust your model based on those numbers, you are effectively fitting to the test set and your reported performance will be optimistically biased.\n",
    "\n",
    "**Key takeaway:** Treat the test set like a sealed envelope. You will open it once, at the very end of the modeling process, and report whatever number comes out -- good or bad.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluation Documentation Template\n",
    "\n",
    "Every evaluation should document:\n",
    "\n",
    "### Evaluation Plan\n",
    "\n",
    "**Primary Metric:** [MAE / RMSE / RÂ²]  \n",
    "**Rationale:** [Why this metric aligns with business goals]\n",
    "\n",
    "**Split Strategy:**\n",
    "- Training: 60% (for fitting)\n",
    "- Validation: 20% (for selection)\n",
    "- Test: 20% (for final evaluation)\n",
    "\n",
    "**Baseline:** [Mean / Median predictor]  \n",
    "**Baseline Performance:** [Validation score]\n",
    "\n",
    "**Model:** [Linear Regression with Standard Scaling]  \n",
    "**Model Performance:** [Validation score]  \n",
    "**Improvement over Baseline:** [Percentage or absolute difference]\n",
    "\n",
    "**Assumptions:**\n",
    "- Features are available at prediction time\n",
    "- Relationship is approximately linear\n",
    "- No major data quality issues\n",
    "\n",
    "**Risks:**\n",
    "- Model may not generalize to different time periods\n",
    "- Assumes no distribution shift\n",
    "- Sensitive to outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Wrap-Up: Key Takeaways\n\n### What We Learned Today:\n\n1. **Metrics Matter**: Choose MAE/RMSE/RÂ² based on business loss function\n2. **Baselines First**: Always establish a simple baseline for comparison\n3. **Holdout Discipline**: Train on train, evaluate on validation, lock test away\n4. **Residuals Tell Stories**: Use diagnostic plots to spot issues\n5. **Document Everything**: Clear evaluation plans prevent confusion later\n\n### Critical Rules:\n\n> **\"If your model can't beat the mean, debug before proceeding\"**\n\n> **\"The test set is a lockbox - open it once\"**\n\n### Next Steps:\n\n- Next notebook: Linear regression with features and diagnostics\n- We'll build on today's evaluation framework\n- Start thinking about which metrics matter for your project\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Participation Assignment Submission Instructions\n",
    "\n",
    "### To Submit This Notebook:\n",
    "\n",
    "1. **Complete all exercises**: Fill in both PAUSE-AND-DO exercise cells with your findings\n",
    "2. **Run All Cells**: Execute `Runtime â†’ Run all` to ensure everything works\n",
    "3. **Save a Copy**: `File â†’ Save a copy in Drive or Download the .ipynb extension`\n",
    "4. **Submit**: Upload your `.ipynb` file in the participation assignment you find in the course Brightspace page.\n",
    "\n",
    "### Before Submitting, Check:\n",
    "\n",
    "- [ ] All cells execute without errors\n",
    "- [ ] All outputs are visible\n",
    "- [ ] Both exercise responses are complete\n",
    "- [ ] Notebook is shared with correct permissions\n",
    "- [ ] You can explain every line of code you wrote\n",
    "\n",
    "### Next Step:\n",
    "\n",
    "Complete the **Quiz** in Brightspace (auto-graded)\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Submission Instructions\n\n### To Submit:\n\n1. Run all cells\n2. Complete both exercises\n3. Write a 3-sentence evaluation note answering:\n   - Which metric is most appropriate for this problem and why?\n   - How much better is linear regression than the baseline?\n   - What does the residual analysis suggest?\n4. Submit Colab link + evaluation note in LMS\n\n---\n\n## Bibliography\n\n- James, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). *An Introduction to Statistical Learning with Python* - Chapter on Model Assessment and Selection\n- Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning* - Test error, training error, bias-variance\n- scikit-learn User Guide: [Regression metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics)\n- scikit-learn User Guide: [Common pitfalls](https://scikit-learn.org/stable/common_pitfalls.html)\n\n---\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "Thank you!\n",
    "\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}