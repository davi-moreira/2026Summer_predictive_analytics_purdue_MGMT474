{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis to Decisions - Thresholds, Calibration, and KPI Alignment\n",
    "\n",
    "<hr>\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/notebooks/figures/mgmt_474_ai_logo_02-modified.png\" width=\"200\"/>\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "# <center><a class=\"tocSkip\"></center>\n",
    "# <center>MGMT47400 Predictive Analytics</center>\n",
    "# <center>Professor: Davi Moreira </center>\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/blob/main/notebooks/16_decision_thresholds_calibration.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Translate model outputs into business decisions (thresholds, costs, constraints)\n",
    "2. Evaluate calibration and when to calibrate probabilities\n",
    "3. Compare models by decision impact, not only by AUC/accuracy\n",
    "4. Produce a threshold/decision recommendation\n",
    "5. Document risks and assumptions explicitly\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Before we can translate model scores into business decisions, we need the right\n",
    "toolkit. This cell imports scikit-learn's calibration utilities alongside the\n",
    "usual data-wrangling and plotting libraries. We also fix `RANDOM_SEED = 474`\n",
    "so that every threshold sweep and calibration curve you see is fully\n",
    "reproducible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.calibration import calibration_curve, CalibratedClassifierCV\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\nimport warnings\n\nwarnings.filterwarnings('ignore')\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (12, 6)\n\nRANDOM_SEED = 474\nnp.random.seed(RANDOM_SEED)\n\nprint(\"\u2713 Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The `Setup complete!` confirmation tells you every import resolved and the\n",
    "global random seed is locked to **474**. If any import had failed, Colab would\n",
    "show a `ModuleNotFoundError` here. Notice that we import both\n",
    "`calibration_curve` (for diagnosis) and `CalibratedClassifierCV` (for\n",
    "correction) -- the two halves of the calibration workflow.\n",
    "\n",
    "**Why this matters:** A clean setup cell is the first reproducibility\n",
    "checkpoint. If someone else opens this notebook in a fresh Colab runtime,\n",
    "this cell must succeed unchanged.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data and Train Model\n",
    "\n",
    "We generate a synthetic binary-classification dataset with `make_classification`\n",
    "(5,000 samples, 20 features, 70/30 class imbalance, 5 % label noise). The data\n",
    "is then split 60/20/20 into train, validation, and test sets. A Random Forest\n",
    "is trained on the training set and its predicted probabilities on the validation\n",
    "set become the raw material for every threshold and calibration analysis that\n",
    "follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=5000, n_features=20, n_informative=15,\n",
    "    n_redundant=5, weights=[0.7, 0.3], flip_y=0.05,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Split\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.20, random_state=RANDOM_SEED)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=RANDOM_SEED)\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "print(f\"Class distribution (validation): {np.bincount(y_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The printout shows the **Train / Val / Test** sizes (roughly **3,000 / 1,000 /\n",
    "1,000**) confirming the 60/20/20 split. The class distribution in the\n",
    "validation set should reflect the 70/30 imbalance we specified in\n",
    "`make_classification`. If the minority class were extremely rare (say < 5 %),\n",
    "we would need stratified splitting -- here the imbalance is moderate enough\n",
    "that a standard split works well.\n",
    "\n",
    "**Key takeaway:** Always print split sizes and class counts right after\n",
    "splitting so you can spot unexpected imbalances early.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=RANDOM_SEED, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Get predicted probabilities\n",
    "y_val_proba = rf_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "print(f\"\\nROC-AUC Score: {roc_auc_score(y_val, y_val_proba):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The **ROC-AUC** score on the validation set tells you how well the Random\n",
    "Forest separates the two classes *across all possible thresholds*. A value\n",
    "close to **1.0** means the model produces well-separated probability\n",
    "distributions for positives and negatives. However, a high AUC does **not**\n",
    "guarantee that the default 0.50 threshold is the best business decision --\n",
    "that is exactly what the cost-based sweep below will reveal.\n",
    "\n",
    "**Why this matters:** AUC is a useful summary, but the optimal operating point\n",
    "depends on the cost matrix, not on AUC alone.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. From Prediction to Action: Thresholds and Costs\n",
    "\n",
    "### 3.1 Define Cost Matrix\n",
    "\n",
    "**Business Context:**\n",
    "- True Positive (TP): Correctly identify positive case \u2192 gain $100\n",
    "- False Positive (FP): Incorrectly classify negative as positive \u2192 cost $30\n",
    "- False Negative (FN): Miss a positive case \u2192 cost $150\n",
    "- True Negative (TN): Correctly identify negative case \u2192 gain $0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost matrix\n",
    "COST_MATRIX = {\n",
    "    'TP': 100,   # Benefit\n",
    "    'FP': -30,   # Cost\n",
    "    'FN': -150,  # Cost\n",
    "    'TN': 0      # No action\n",
    "}\n",
    "\n",
    "def compute_expected_value(y_true, y_pred, cost_matrix):\n",
    "    \"\"\"Compute expected value given cost matrix\"\"\"\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    total_value = (\n",
    "        tp * cost_matrix['TP'] +\n",
    "        fp * cost_matrix['FP'] +\n",
    "        fn * cost_matrix['FN'] +\n",
    "        tn * cost_matrix['TN']\n",
    "    )\n",
    "    \n",
    "    return total_value, {'TP': tp, 'FP': fp, 'FN': fn, 'TN': tn}\n",
    "\n",
    "print(\"Cost Matrix Defined:\")\n",
    "for key, value in COST_MATRIX.items():\n",
    "    print(f\"  {key}: ${value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The printed cost matrix shows **TP = +$100**, **FP = -$30**, **FN = -$150**,\n",
    "and **TN = $0**. Two things jump out: missing a positive case (FN) costs\n",
    "**five times** more than a false alarm (FP), and correctly identifying a\n",
    "negative adds no monetary value. This asymmetry will push the optimal\n",
    "threshold *below* 0.50 because we want to avoid the expensive false negatives\n",
    "even at the price of more false positives.\n",
    "\n",
    "**Key takeaway:** The ratio of FN cost to FP cost is the single most\n",
    "important driver of threshold selection. Always map it before tuning.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Threshold Sweep with Expected Cost\n",
    "\n",
    "Instead of accepting the default 0.50 decision boundary, we sweep thresholds\n",
    "from 0.10 to 0.85 and compute the total expected value under our cost matrix\n",
    "at each point. The threshold that maximises total value is the one the\n",
    "business should adopt. This is where ML stops being about accuracy and starts\n",
    "being about dollars.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sweep thresholds\n",
    "thresholds = np.arange(0.1, 0.9, 0.05)\n",
    "results = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_val_proba >= threshold).astype(int)\n",
    "    total_value, counts = compute_expected_value(y_val, y_pred, COST_MATRIX)\n",
    "    \n",
    "    results.append({\n",
    "        'threshold': threshold,\n",
    "        'total_value': total_value,\n",
    "        'avg_value_per_case': total_value / len(y_val),\n",
    "        'TP': counts['TP'],\n",
    "        'FP': counts['FP'],\n",
    "        'FN': counts['FN'],\n",
    "        'TN': counts['TN']\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "best_threshold = results_df.loc[results_df['total_value'].idxmax(), 'threshold']\n",
    "\n",
    "print(\"\\n=== THRESHOLD SWEEP RESULTS ===\")\n",
    "print(results_df.head(10))\n",
    "print(f\"\\nOptimal Threshold (by total value): {best_threshold:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The results table lists each threshold alongside the **total expected value**\n",
    "and the four confusion-matrix counts. The optimal threshold (printed at the\n",
    "bottom) is the one that maximises total value. Because our FN cost dominates,\n",
    "the optimal point is typically below the default 0.50 -- the model accepts\n",
    "more false positives in exchange for catching almost every true positive.\n",
    "The `avg_value_per_case` column gives a quick per-record profitability\n",
    "estimate you can quote to stakeholders.\n",
    "\n",
    "**Why this matters:** Picking a threshold by accuracy alone would miss the\n",
    "cost-optimal operating point. Always let the cost matrix, not a default, drive\n",
    "the threshold.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize threshold impact\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Total value vs threshold\n",
    "axes[0].plot(results_df['threshold'], results_df['total_value'], marker='o')\n",
    "axes[0].axvline(best_threshold, color='r', linestyle='--', label=f'Optimal: {best_threshold:.2f}')\n",
    "axes[0].set_xlabel('Threshold')\n",
    "axes[0].set_ylabel('Total Expected Value ($)')\n",
    "axes[0].set_title('Expected Value vs Threshold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Confusion matrix components\n",
    "axes[1].plot(results_df['threshold'], results_df['TP'], marker='o', label='True Positives')\n",
    "axes[1].plot(results_df['threshold'], results_df['FP'], marker='s', label='False Positives')\n",
    "axes[1].plot(results_df['threshold'], results_df['FN'], marker='^', label='False Negatives')\n",
    "axes[1].axvline(best_threshold, color='r', linestyle='--', alpha=0.5)\n",
    "axes[1].set_xlabel('Threshold')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Error Counts vs Threshold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The **left panel** shows the total expected value curve: it rises, peaks at\n",
    "the optimal threshold (red dashed line), and then falls as we become too\n",
    "conservative and start missing true positives. The **right panel** decomposes\n",
    "the confusion-matrix counts: as the threshold increases, true positives and\n",
    "false positives both drop, while false negatives climb. The optimal threshold\n",
    "sits where the monetary gain from avoiding FPs is exactly offset by the\n",
    "mounting cost of new FNs.\n",
    "\n",
    "**Key takeaway:** Visualising value *and* error counts together lets you\n",
    "explain the recommendation to non-technical stakeholders: 'We chose this\n",
    "threshold because it balances alarm fatigue against missed detections.'\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd PAUSE-AND-DO Exercise 1 (5 minutes)\n",
    "\n",
    "**Task:** Select a threshold that minimizes expected cost and justify it.\n",
    "\n",
    "**Instructions:**\n",
    "1. Review the threshold sweep results above\n",
    "2. Identify the threshold that maximizes expected value\n",
    "3. Explain why this threshold makes business sense\n",
    "4. Discuss what tradeoffs are being made\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR THRESHOLD RECOMMENDATION HERE:\n",
    "\n",
    "**Recommended Threshold:**  \n",
    "[Value and reasoning]\n",
    "\n",
    "**Business Justification:**  \n",
    "[Why this threshold makes sense for the business]\n",
    "\n",
    "**Tradeoffs:**  \n",
    "[What are we gaining vs losing at this threshold?]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calibration: Are Probabilities Trustworthy?\n",
    "\n",
    "### 4.1 Calibration Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute calibration curve\n",
    "prob_true, prob_pred = calibration_curve(y_val, y_val_proba, n_bins=10, strategy='uniform')\n",
    "\n",
    "# Plot calibration\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Calibration plot\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "axes[0].plot(prob_pred, prob_true, marker='o', label='Random Forest')\n",
    "axes[0].set_xlabel('Mean Predicted Probability')\n",
    "axes[0].set_ylabel('Fraction of Positives')\n",
    "axes[0].set_title('Calibration Plot')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Probability histogram\n",
    "axes[1].hist(y_val_proba[y_val == 0], bins=30, alpha=0.5, label='Negative Class', edgecolor='black')\n",
    "axes[1].hist(y_val_proba[y_val == 1], bins=30, alpha=0.5, label='Positive Class', edgecolor='black')\n",
    "axes[1].set_xlabel('Predicted Probability')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Predicted Probability Distribution')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\u26a0\ufe0f Calibration Assessment:\")\n",
    "print(\"  - Points close to diagonal = well-calibrated\")\n",
    "print(\"  - Points below diagonal = overconfident\")\n",
    "print(\"  - Points above diagonal = underconfident\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The **left panel** (reliability diagram) plots the actual fraction of\n",
    "positives against the model's predicted probability in each bin. Points on\n",
    "the diagonal mean perfect calibration; points *below* the diagonal indicate\n",
    "the model is **overconfident** (it says 0.80 but the true rate is only 0.65).\n",
    "The **right panel** shows how predicted probabilities are distributed for each\n",
    "class -- well-separated histograms confirm good discrimination.\n",
    "\n",
    "**Why this matters:** If you plan to use predicted probabilities to set\n",
    "dollar-denominated decision policies (as we did above), those probabilities\n",
    "must be trustworthy. Miscalibrated scores lead to miscalculated expected\n",
    "values.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Apply Calibration\n",
    "\n",
    "When a reliability diagram reveals systematic miscalibration, we can wrap the\n",
    "trained classifier in `CalibratedClassifierCV` with isotonic regression. This\n",
    "post-hoc adjustment maps raw scores to better-calibrated probabilities without\n",
    "retraining the base model. We fit on the validation set and evaluate on the\n",
    "held-out test set to avoid double-dipping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrate using isotonic regression\n",
    "calibrated_model = CalibratedClassifierCV(rf_model, method='isotonic', cv='prefit')\n",
    "calibrated_model.fit(X_val, y_val)\n",
    "\n",
    "# Get calibrated probabilities on a fresh split (simulating test set)\n",
    "y_cal_proba = calibrated_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compare calibration\n",
    "prob_true_cal, prob_pred_cal = calibration_curve(y_test, y_cal_proba, n_bins=10, strategy='uniform')\n",
    "\n",
    "# Plot comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "ax.plot(prob_pred, prob_true, marker='o', label='Original RF', alpha=0.7)\n",
    "ax.plot(prob_pred_cal, prob_true_cal, marker='s', label='Calibrated RF', alpha=0.7)\n",
    "ax.set_xlabel('Mean Predicted Probability')\n",
    "ax.set_ylabel('Fraction of Positives')\n",
    "ax.set_title('Calibration: Before vs After')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\u2713 Calibration applied using isotonic regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The overlay plot compares the **original** Random Forest calibration curve\n",
    "(circles) with the **isotonic-calibrated** curve (squares). After calibration,\n",
    "the points should hug the diagonal more tightly, meaning that a predicted\n",
    "probability of, say, 0.70 now corresponds much closer to a 70 % true-positive\n",
    "rate. Isotonic regression is non-parametric and flexible, but it can overfit\n",
    "with very small validation sets -- always check the curve visually.\n",
    "\n",
    "**Key takeaway:** Calibration does not change the model's ranking of\n",
    "instances (AUC stays the same); it only re-maps the scores so they can be\n",
    "interpreted as genuine probabilities.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd PAUSE-AND-DO Exercise 2 (5 minutes)\n",
    "\n",
    "**Task:** Check calibration and decide whether calibration is needed.\n",
    "\n",
    "**Instructions:**\n",
    "1. Review the calibration plots above\n",
    "2. Assess whether the model is well-calibrated\n",
    "3. Decide if calibration would improve decision-making\n",
    "4. Justify your recommendation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR CALIBRATION ASSESSMENT HERE:\n",
    "\n",
    "**Calibration Quality:**  \n",
    "[Is the model well-calibrated? What patterns do you see?]\n",
    "\n",
    "**Recommendation:**  \n",
    "[Should we use calibrated probabilities?]\n",
    "\n",
    "**Justification:**  \n",
    "[Why or why not? What's the impact on decision-making?]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Decision Policy Summary\n",
    "\n",
    "### 5.1 Final Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply optimal threshold\n",
    "y_val_pred_optimal = (y_val_proba >= best_threshold).astype(int)\n",
    "total_value, counts = compute_expected_value(y_val, y_val_pred_optimal, COST_MATRIX)\n",
    "\n",
    "print(\"=== DECISION POLICY RECOMMENDATION ===\")\n",
    "print(f\"\\nOptimal Threshold: {best_threshold:.2f}\")\n",
    "print(f\"Expected Total Value: ${total_value:,.2f}\")\n",
    "print(f\"Expected Value per Case: ${total_value/len(y_val):,.2f}\")\n",
    "print(f\"\\nConfusion Matrix at Optimal Threshold:\")\n",
    "print(f\"  True Positives: {counts['TP']}\")\n",
    "print(f\"  False Positives: {counts['FP']}\")\n",
    "print(f\"  False Negatives: {counts['FN']}\")\n",
    "print(f\"  True Negatives: {counts['TN']}\")\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_val_pred_optimal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The summary prints the **optimal threshold**, the **total expected value** in\n",
    "dollars, and the per-case average. The confusion-matrix breakdown shows\n",
    "exactly how many true positives, false positives, false negatives, and true\n",
    "negatives result from that threshold. The classification report adds\n",
    "precision, recall, and F1 for both classes, giving a complete picture.\n",
    "Quoting all three views (dollars, counts, rates) makes the recommendation\n",
    "accessible to finance, operations, and data-science audiences alike.\n",
    "\n",
    "**Why this matters:** A decision policy must be communicated in units each\n",
    "stakeholder cares about -- dollars for the CFO, rates for the analyst,\n",
    "counts for the operations team.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Sensitivity Analysis\n",
    "\n",
    "Cost assumptions are never perfectly known. A sensitivity analysis varies the\n",
    "false-negative cost over a plausible range and records how the optimal\n",
    "threshold and expected value respond. If the threshold barely moves, the\n",
    "decision policy is robust; if it swings dramatically, stakeholders need to\n",
    "invest in tighter cost estimates before deploying.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sensitivity to cost assumptions\n",
    "fn_costs = [100, 150, 200, 250]\n",
    "sensitivity_results = []\n",
    "\n",
    "for fn_cost in fn_costs:\n",
    "    temp_cost_matrix = COST_MATRIX.copy()\n",
    "    temp_cost_matrix['FN'] = -fn_cost\n",
    "    \n",
    "    # Find optimal threshold for this cost\n",
    "    best_value = -np.inf\n",
    "    best_thresh = 0.5\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_val_proba >= threshold).astype(int)\n",
    "        value, _ = compute_expected_value(y_val, y_pred, temp_cost_matrix)\n",
    "        if value > best_value:\n",
    "            best_value = value\n",
    "            best_thresh = threshold\n",
    "    \n",
    "    sensitivity_results.append({\n",
    "        'FN_Cost': fn_cost,\n",
    "        'Optimal_Threshold': best_thresh,\n",
    "        'Expected_Value': best_value\n",
    "    })\n",
    "\n",
    "sensitivity_df = pd.DataFrame(sensitivity_results)\n",
    "\n",
    "print(\"\\n=== SENSITIVITY ANALYSIS ===\")\n",
    "print(\"How does optimal threshold change with FN cost?\")\n",
    "print(sensitivity_df)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(sensitivity_df['FN_Cost'], sensitivity_df['Optimal_Threshold'], marker='o')\n",
    "axes[0].set_xlabel('False Negative Cost ($)')\n",
    "axes[0].set_ylabel('Optimal Threshold')\n",
    "axes[0].set_title('Threshold Sensitivity to FN Cost')\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].plot(sensitivity_df['FN_Cost'], sensitivity_df['Expected_Value'], marker='o')\n",
    "axes[1].set_xlabel('False Negative Cost ($)')\n",
    "axes[1].set_ylabel('Expected Value ($)')\n",
    "axes[1].set_title('Expected Value vs FN Cost')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The sensitivity table shows how the optimal threshold and expected value\n",
    "change as the **false-negative cost** varies from $100 to $250. If the\n",
    "optimal threshold barely shifts across this range, the policy is **robust**\n",
    "to cost-estimation error. The two plots reinforce this: a flat threshold\n",
    "line means stability, while a steep slope means the business must invest in\n",
    "more precise cost estimates before committing to a threshold.\n",
    "\n",
    "**Key takeaway:** Never present a single optimal threshold without showing\n",
    "how sensitive it is to the assumptions. Sensitivity analysis is what turns a\n",
    "model recommendation into a trustworthy business policy.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Wrap-Up: Key Takeaways\n",
    "\n",
    "### What We Learned Today:\n",
    "\n",
    "1. **Cost-Sensitive Decisions**: How to translate predictions into actions using cost matrices\n",
    "2. **Threshold Optimization**: Finding thresholds that maximize business value\n",
    "3. **Calibration Assessment**: Evaluating whether probabilities are trustworthy\n",
    "4. **Calibration Techniques**: Using isotonic regression to improve probability estimates\n",
    "5. **Sensitivity Analysis**: Understanding how decisions change with cost assumptions\n",
    "\n",
    "### Decision-Making Best Practices:\n",
    "\n",
    "- \u2713 Define cost matrix based on business reality, not convenience\n",
    "- \u2713 Optimize thresholds on validation set, not training set\n",
    "- \u2713 Check calibration before using probabilities for decisions\n",
    "- \u2713 Perform sensitivity analysis on cost assumptions\n",
    "- \u2713 Document decision policy clearly for stakeholders\n",
    "- \u2713 Plan for monitoring and updating thresholds over time\n",
    "\n",
    "### Remember:\n",
    "\n",
    "> **\"Model performance metrics don't pay the bills - business value does.\"**  \n",
    "> Always optimize for business outcomes, not just AUC or accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Participation Assignment Submission Instructions\n",
    "\n",
    "### To Submit This Notebook:\n",
    "\n",
    "1. **Complete all exercises**: Fill in both PAUSE-AND-DO exercise cells with your findings\n",
    "2. **Run All Cells**: Execute `Runtime \u2192 Run all` to ensure everything works\n",
    "3. **Save a Copy**: `File \u2192 Save a copy in Drive`\n",
    "4. **Submit**: Upload your `.ipynb` file in the participation assignment you find in the course Brightspace page.\n",
    "\n",
    "### Before Submitting, Check:\n",
    "\n",
    "- [ ] All cells execute without errors\n",
    "- [ ] All outputs are visible\n",
    "- [ ] Both exercise responses are complete\n",
    "- [ ] Notebook is shared with correct permissions\n",
    "- [ ] You can explain every line of code you wrote\n",
    "\n",
    "### Next Step:\n",
    "\n",
    "Complete the **Quiz** in Brightspace (auto-graded)\n",
    "\n",
    "---\n",
    "\n",
    "## Bibliography\n\n- Provost, F., & Fawcett, T. (2013). *Data Science for Business*. O'Reilly Media.\n- scikit-learn User Guide: [Probability Calibration](https://scikit-learn.org/stable/modules/calibration.html)\n- Niculescu-Mizil, A., & Caruana, R. (2005). \"Predicting good probabilities with supervised learning.\" *ICML*.\n- Zadrozny, B., & Elkan, C. (2001). \"Obtaining calibrated probability estimates from decision trees and naive Bayesian classifiers.\" *ICML*.\n\n---\n\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "Thank you!\n",
    "\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}