{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis to Decisions - Thresholds, Calibration, and KPI Alignment\n",
    "\n",
    "<hr>\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/notebooks/figures/mgmt_474_ai_logo_02-modified.png\" width=\"200\"/>\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "# <center><a class=\"tocSkip\"></center>\n",
    "# <center>MGMT47400 Predictive Analytics</center>\n",
    "# <center>Professor: Davi Moreira </center>\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/blob/main/notebooks/16_decision_thresholds_calibration.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Translate model outputs into business decisions (thresholds, costs, constraints)\n",
    "2. Evaluate calibration and when to calibrate probabilities\n",
    "3. Compare models by decision impact, not only by AUC/accuracy\n",
    "4. Produce a threshold/decision recommendation\n",
    "5. Document risks and assumptions explicitly\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.calibration import calibration_curve, CalibratedClassifierCV\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\nimport warnings\n\nwarnings.filterwarnings('ignore')\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (12, 6)\n\nRANDOM_SEED = 474\nnp.random.seed(RANDOM_SEED)\n\nprint(\"\u2713 Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=5000, n_features=20, n_informative=15,\n",
    "    n_redundant=5, weights=[0.7, 0.3], flip_y=0.05,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Split\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.20, random_state=RANDOM_SEED)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=RANDOM_SEED)\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "print(f\"Class distribution (validation): {np.bincount(y_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=RANDOM_SEED, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Get predicted probabilities\n",
    "y_val_proba = rf_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "print(f\"\\nROC-AUC Score: {roc_auc_score(y_val, y_val_proba):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. From Prediction to Action: Thresholds and Costs\n",
    "\n",
    "### 3.1 Define Cost Matrix\n",
    "\n",
    "**Business Context:**\n",
    "- True Positive (TP): Correctly identify positive case \u2192 gain $100\n",
    "- False Positive (FP): Incorrectly classify negative as positive \u2192 cost $30\n",
    "- False Negative (FN): Miss a positive case \u2192 cost $150\n",
    "- True Negative (TN): Correctly identify negative case \u2192 gain $0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost matrix\n",
    "COST_MATRIX = {\n",
    "    'TP': 100,   # Benefit\n",
    "    'FP': -30,   # Cost\n",
    "    'FN': -150,  # Cost\n",
    "    'TN': 0      # No action\n",
    "}\n",
    "\n",
    "def compute_expected_value(y_true, y_pred, cost_matrix):\n",
    "    \"\"\"Compute expected value given cost matrix\"\"\"\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    total_value = (\n",
    "        tp * cost_matrix['TP'] +\n",
    "        fp * cost_matrix['FP'] +\n",
    "        fn * cost_matrix['FN'] +\n",
    "        tn * cost_matrix['TN']\n",
    "    )\n",
    "    \n",
    "    return total_value, {'TP': tp, 'FP': fp, 'FN': fn, 'TN': tn}\n",
    "\n",
    "print(\"Cost Matrix Defined:\")\n",
    "for key, value in COST_MATRIX.items():\n",
    "    print(f\"  {key}: ${value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Threshold Sweep with Expected Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sweep thresholds\n",
    "thresholds = np.arange(0.1, 0.9, 0.05)\n",
    "results = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_val_proba >= threshold).astype(int)\n",
    "    total_value, counts = compute_expected_value(y_val, y_pred, COST_MATRIX)\n",
    "    \n",
    "    results.append({\n",
    "        'threshold': threshold,\n",
    "        'total_value': total_value,\n",
    "        'avg_value_per_case': total_value / len(y_val),\n",
    "        'TP': counts['TP'],\n",
    "        'FP': counts['FP'],\n",
    "        'FN': counts['FN'],\n",
    "        'TN': counts['TN']\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "best_threshold = results_df.loc[results_df['total_value'].idxmax(), 'threshold']\n",
    "\n",
    "print(\"\\n=== THRESHOLD SWEEP RESULTS ===\")\n",
    "print(results_df.head(10))\n",
    "print(f\"\\nOptimal Threshold (by total value): {best_threshold:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize threshold impact\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Total value vs threshold\n",
    "axes[0].plot(results_df['threshold'], results_df['total_value'], marker='o')\n",
    "axes[0].axvline(best_threshold, color='r', linestyle='--', label=f'Optimal: {best_threshold:.2f}')\n",
    "axes[0].set_xlabel('Threshold')\n",
    "axes[0].set_ylabel('Total Expected Value ($)')\n",
    "axes[0].set_title('Expected Value vs Threshold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Confusion matrix components\n",
    "axes[1].plot(results_df['threshold'], results_df['TP'], marker='o', label='True Positives')\n",
    "axes[1].plot(results_df['threshold'], results_df['FP'], marker='s', label='False Positives')\n",
    "axes[1].plot(results_df['threshold'], results_df['FN'], marker='^', label='False Negatives')\n",
    "axes[1].axvline(best_threshold, color='r', linestyle='--', alpha=0.5)\n",
    "axes[1].set_xlabel('Threshold')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Error Counts vs Threshold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd PAUSE-AND-DO Exercise 1 (10 minutes)\n",
    "\n",
    "**Task:** Select a threshold that minimizes expected cost and justify it.\n",
    "\n",
    "**Instructions:**\n",
    "1. Review the threshold sweep results above\n",
    "2. Identify the threshold that maximizes expected value\n",
    "3. Explain why this threshold makes business sense\n",
    "4. Discuss what tradeoffs are being made\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR THRESHOLD RECOMMENDATION HERE:\n",
    "\n",
    "**Recommended Threshold:**  \n",
    "[Value and reasoning]\n",
    "\n",
    "**Business Justification:**  \n",
    "[Why this threshold makes sense for the business]\n",
    "\n",
    "**Tradeoffs:**  \n",
    "[What are we gaining vs losing at this threshold?]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calibration: Are Probabilities Trustworthy?\n",
    "\n",
    "### 4.1 Calibration Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute calibration curve\n",
    "prob_true, prob_pred = calibration_curve(y_val, y_val_proba, n_bins=10, strategy='uniform')\n",
    "\n",
    "# Plot calibration\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Calibration plot\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "axes[0].plot(prob_pred, prob_true, marker='o', label='Random Forest')\n",
    "axes[0].set_xlabel('Mean Predicted Probability')\n",
    "axes[0].set_ylabel('Fraction of Positives')\n",
    "axes[0].set_title('Calibration Plot')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Probability histogram\n",
    "axes[1].hist(y_val_proba[y_val == 0], bins=30, alpha=0.5, label='Negative Class', edgecolor='black')\n",
    "axes[1].hist(y_val_proba[y_val == 1], bins=30, alpha=0.5, label='Positive Class', edgecolor='black')\n",
    "axes[1].set_xlabel('Predicted Probability')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Predicted Probability Distribution')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\u26a0\ufe0f Calibration Assessment:\")\n",
    "print(\"  - Points close to diagonal = well-calibrated\")\n",
    "print(\"  - Points below diagonal = overconfident\")\n",
    "print(\"  - Points above diagonal = underconfident\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Apply Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrate using isotonic regression\n",
    "calibrated_model = CalibratedClassifierCV(rf_model, method='isotonic', cv='prefit')\n",
    "calibrated_model.fit(X_val, y_val)\n",
    "\n",
    "# Get calibrated probabilities on a fresh split (simulating test set)\n",
    "y_cal_proba = calibrated_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compare calibration\n",
    "prob_true_cal, prob_pred_cal = calibration_curve(y_test, y_cal_proba, n_bins=10, strategy='uniform')\n",
    "\n",
    "# Plot comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "ax.plot(prob_pred, prob_true, marker='o', label='Original RF', alpha=0.7)\n",
    "ax.plot(prob_pred_cal, prob_true_cal, marker='s', label='Calibrated RF', alpha=0.7)\n",
    "ax.set_xlabel('Mean Predicted Probability')\n",
    "ax.set_ylabel('Fraction of Positives')\n",
    "ax.set_title('Calibration: Before vs After')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\u2713 Calibration applied using isotonic regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd PAUSE-AND-DO Exercise 2 (10 minutes)\n",
    "\n",
    "**Task:** Check calibration and decide whether calibration is needed.\n",
    "\n",
    "**Instructions:**\n",
    "1. Review the calibration plots above\n",
    "2. Assess whether the model is well-calibrated\n",
    "3. Decide if calibration would improve decision-making\n",
    "4. Justify your recommendation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR CALIBRATION ASSESSMENT HERE:\n",
    "\n",
    "**Calibration Quality:**  \n",
    "[Is the model well-calibrated? What patterns do you see?]\n",
    "\n",
    "**Recommendation:**  \n",
    "[Should we use calibrated probabilities?]\n",
    "\n",
    "**Justification:**  \n",
    "[Why or why not? What's the impact on decision-making?]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Decision Policy Summary\n",
    "\n",
    "### 5.1 Final Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply optimal threshold\n",
    "y_val_pred_optimal = (y_val_proba >= best_threshold).astype(int)\n",
    "total_value, counts = compute_expected_value(y_val, y_val_pred_optimal, COST_MATRIX)\n",
    "\n",
    "print(\"=== DECISION POLICY RECOMMENDATION ===\")\n",
    "print(f\"\\nOptimal Threshold: {best_threshold:.2f}\")\n",
    "print(f\"Expected Total Value: ${total_value:,.2f}\")\n",
    "print(f\"Expected Value per Case: ${total_value/len(y_val):,.2f}\")\n",
    "print(f\"\\nConfusion Matrix at Optimal Threshold:\")\n",
    "print(f\"  True Positives: {counts['TP']}\")\n",
    "print(f\"  False Positives: {counts['FP']}\")\n",
    "print(f\"  False Negatives: {counts['FN']}\")\n",
    "print(f\"  True Negatives: {counts['TN']}\")\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_val_pred_optimal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sensitivity to cost assumptions\n",
    "fn_costs = [100, 150, 200, 250]\n",
    "sensitivity_results = []\n",
    "\n",
    "for fn_cost in fn_costs:\n",
    "    temp_cost_matrix = COST_MATRIX.copy()\n",
    "    temp_cost_matrix['FN'] = -fn_cost\n",
    "    \n",
    "    # Find optimal threshold for this cost\n",
    "    best_value = -np.inf\n",
    "    best_thresh = 0.5\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_val_proba >= threshold).astype(int)\n",
    "        value, _ = compute_expected_value(y_val, y_pred, temp_cost_matrix)\n",
    "        if value > best_value:\n",
    "            best_value = value\n",
    "            best_thresh = threshold\n",
    "    \n",
    "    sensitivity_results.append({\n",
    "        'FN_Cost': fn_cost,\n",
    "        'Optimal_Threshold': best_thresh,\n",
    "        'Expected_Value': best_value\n",
    "    })\n",
    "\n",
    "sensitivity_df = pd.DataFrame(sensitivity_results)\n",
    "\n",
    "print(\"\\n=== SENSITIVITY ANALYSIS ===\")\n",
    "print(\"How does optimal threshold change with FN cost?\")\n",
    "print(sensitivity_df)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(sensitivity_df['FN_Cost'], sensitivity_df['Optimal_Threshold'], marker='o')\n",
    "axes[0].set_xlabel('False Negative Cost ($)')\n",
    "axes[0].set_ylabel('Optimal Threshold')\n",
    "axes[0].set_title('Threshold Sensitivity to FN Cost')\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].plot(sensitivity_df['FN_Cost'], sensitivity_df['Expected_Value'], marker='o')\n",
    "axes[1].set_xlabel('False Negative Cost ($)')\n",
    "axes[1].set_ylabel('Expected Value ($)')\n",
    "axes[1].set_title('Expected Value vs FN Cost')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Wrap-Up: Key Takeaways\n",
    "\n",
    "### What We Learned Today:\n",
    "\n",
    "1. **Cost-Sensitive Decisions**: How to translate predictions into actions using cost matrices\n",
    "2. **Threshold Optimization**: Finding thresholds that maximize business value\n",
    "3. **Calibration Assessment**: Evaluating whether probabilities are trustworthy\n",
    "4. **Calibration Techniques**: Using isotonic regression to improve probability estimates\n",
    "5. **Sensitivity Analysis**: Understanding how decisions change with cost assumptions\n",
    "\n",
    "### Decision-Making Best Practices:\n",
    "\n",
    "- \u2713 Define cost matrix based on business reality, not convenience\n",
    "- \u2713 Optimize thresholds on validation set, not training set\n",
    "- \u2713 Check calibration before using probabilities for decisions\n",
    "- \u2713 Perform sensitivity analysis on cost assumptions\n",
    "- \u2713 Document decision policy clearly for stakeholders\n",
    "- \u2713 Plan for monitoring and updating thresholds over time\n",
    "\n",
    "### Remember:\n",
    "\n",
    "> **\"Model performance metrics don't pay the bills - business value does.\"**  \n",
    "> Always optimize for business outcomes, not just AUC or accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n\n- Provost, F., & Fawcett, T. (2013). *Data Science for Business*. O'Reilly Media.\n- scikit-learn User Guide: [Probability Calibration](https://scikit-learn.org/stable/modules/calibration.html)\n- Niculescu-Mizil, A., & Caruana, R. (2005). \"Predicting good probabilities with supervised learning.\" *ICML*.\n- Zadrozny, B., & Elkan, C. (2001). \"Obtaining calibrated probability estimates from decision trees and naive Bayesian classifiers.\" *ICML*.\n\n---\n\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "Thank you!\n",
    "\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}