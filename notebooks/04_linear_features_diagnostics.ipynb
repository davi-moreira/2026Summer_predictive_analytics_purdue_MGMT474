{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression That Actually Works - Features, Interactions, Diagnostics\n",
    "\n",
    "<hr>\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/notebooks/figures/mgmt_474_ai_logo_02-modified.png\" width=\"200\"/>\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "# <center><a class=\"tocSkip\"></center>\n",
    "# <center>MGMT47400 Predictive Analytics</center>\n",
    "# <center>Professor: Davi Moreira </center>\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/blob/main/notebooks/04_linear_features_diagnostics.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Fit and interpret linear regression in a pipeline\n",
    "2. Create interaction/polynomial features responsibly\n",
    "3. Diagnose underfit/overfit using validation results\n",
    "4. Use residual analysis to spot nonlinearity and heteroskedasticity\n",
    "5. Translate coefficients into business meaning (with caveats)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport warnings\n\nwarnings.filterwarnings('ignore')\npd.set_option('display.precision', 4)\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (10, 6)\nRANDOM_SEED = 474\nnp.random.seed(RANDOM_SEED)\nprint(\"\u2713 Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Create Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "california = fetch_california_housing(as_frame=True)\n",
    "df = california.frame\n",
    "X = df.drop(columns=['MedHouseVal'])\n",
    "y = df['MedHouseVal']\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.20, random_state=RANDOM_SEED)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=RANDOM_SEED)\n",
    "\n",
    "print(f\"Train: {len(X_train)} | Val: {len(X_val)} | Test: {len(X_test)} (locked)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline Linear Model with Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple linear regression pipeline\n",
    "baseline_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "baseline_pipeline.fit(X_train, y_train)\n",
    "\n",
    "train_score = baseline_pipeline.score(X_train, y_train)\n",
    "val_score = baseline_pipeline.score(X_val, y_val)\n",
    "\n",
    "print(\"=== BASELINE LINEAR MODEL ===\")\n",
    "print(f\"Train R\u00b2: {train_score:.4f}\")\n",
    "print(f\"Val R\u00b2: {val_score:.4f}\")\n",
    "print(f\"Overfit gap: {train_score - val_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Coefficient Interpretation\n",
    "\n",
    "### Understanding Linear Regression Coefficients\n",
    "\n",
    "**What coefficients tell you:**\n",
    "- Direction of relationship (positive/negative)\n",
    "- Relative importance (after scaling)\n",
    "- Magnitude of effect (with caveats)\n",
    "\n",
    "**Interpretation caveats:**\n",
    "- Only valid if features are scaled consistently\n",
    "- Assumes linear relationship\n",
    "- Affected by multicollinearity\n",
    "- Correlation \u2260 causation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coefficients\n",
    "coefficients = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Coefficient': baseline_pipeline.named_steps['regressor'].coef_,\n",
    "    'Abs_Coefficient': np.abs(baseline_pipeline.named_steps['regressor'].coef_)\n",
    "}).sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(\"=== LINEAR REGRESSION COEFFICIENTS ===\")\n",
    "print(coefficients)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(coefficients['Feature'], coefficients['Coefficient'])\n",
    "plt.xlabel('Coefficient Value (after scaling)')\n",
    "plt.title('Feature Importance via Linear Regression Coefficients')\n",
    "plt.axvline(x=0, color='red', linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nIntercept: {baseline_pipeline.named_steps['regressor'].intercept_:.4f}\")\n",
    "print(f\"\\n\ud83d\udca1 Positive coefficients increase the prediction\")\n",
    "print(f\"\ud83d\udca1 Negative coefficients decrease the prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering: Interactions\n",
    "\n",
    "### When to Add Interactions\n",
    "\n",
    "Interactions capture **combined effects** of features:\n",
    "- Income \u00d7 Location might matter more than either alone\n",
    "- Age \u00d7 Size might have nonlinear effects\n",
    "\n",
    "**Warning:** Interactions increase features exponentially!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a simple interaction manually first\n",
    "# Example: interaction between MedInc and AveRooms\n",
    "X_train_interact = X_train.copy()\n",
    "X_val_interact = X_val.copy()\n",
    "\n",
    "X_train_interact['MedInc_x_AveRooms'] = X_train['MedInc'] * X_train['AveRooms']\n",
    "X_val_interact['MedInc_x_AveRooms'] = X_val['MedInc'] * X_val['AveRooms']\n",
    "\n",
    "# Fit model with interaction\n",
    "interact_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "interact_pipeline.fit(X_train_interact, y_train)\n",
    "\n",
    "train_score_interact = interact_pipeline.score(X_train_interact, y_train)\n",
    "val_score_interact = interact_pipeline.score(X_val_interact, y_val)\n",
    "\n",
    "print(\"=== WITH INTERACTION FEATURE ===\")\n",
    "print(f\"Train R\u00b2: {train_score_interact:.4f}\")\n",
    "print(f\"Val R\u00b2: {val_score_interact:.4f}\")\n",
    "print(f\"\\nImprovement over baseline: {val_score_interact - val_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd PAUSE-AND-DO Exercise 1 (10 minutes)\n",
    "\n",
    "**Task:** Add an interaction or polynomial block and measure validation change.\n",
    "\n",
    "**Instructions:**\n",
    "1. Use `PolynomialFeatures` to create degree=2 features\n",
    "2. Fit a new pipeline\n",
    "3. Compare validation scores\n",
    "4. Watch for overfitting!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Add polynomial features\n",
    "\n",
    "poly_pipeline = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "poly_pipeline.fit(X_train, y_train)\n",
    "\n",
    "train_score_poly = poly_pipeline.score(X_train, y_train)\n",
    "val_score_poly = poly_pipeline.score(X_val, y_val)\n",
    "\n",
    "print(\"=== POLYNOMIAL FEATURES (degree=2) ===\")\n",
    "print(f\"Train R\u00b2: {train_score_poly:.4f}\")\n",
    "print(f\"Val R\u00b2: {val_score_poly:.4f}\")\n",
    "print(f\"Overfit gap: {train_score_poly - val_score_poly:.4f}\")\n",
    "\n",
    "# Count features\n",
    "n_features_original = X_train.shape[1]\n",
    "n_features_poly = poly_pipeline.named_steps['poly'].transform(X_train[:1]).shape[1]\n",
    "print(f\"\\nOriginal features: {n_features_original}\")\n",
    "print(f\"After polynomial (degree=2): {n_features_poly}\")\n",
    "print(f\"Feature explosion: {n_features_poly / n_features_original:.1f}x increase\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANALYSIS:\n",
    "\n",
    "**Observation 1: Performance**  \n",
    "[Did polynomial features improve validation score?]\n",
    "\n",
    "**Observation 2: Overfitting**  \n",
    "[Is the train-val gap larger now?]\n",
    "\n",
    "**Observation 3: Complexity**  \n",
    "[Is the added complexity worth the improvement?]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Residual Diagnostics\n",
    "\n",
    "### What to Look For:\n",
    "\n",
    "1. **Patterns in residual plots** \u2192 Model is missing something (nonlinearity, interactions)\n",
    "2. **Funnel shape** \u2192 Heteroskedasticity (variance changes with prediction)\n",
    "3. **Non-normal residuals** \u2192 Outliers or wrong model family\n",
    "4. **Systematic errors in ranges** \u2192 Model struggles in certain regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "y_pred_baseline = baseline_pipeline.predict(X_val)\n",
    "y_pred_poly = poly_pipeline.predict(X_val)\n",
    "\n",
    "residuals_baseline = y_val - y_pred_baseline\n",
    "residuals_poly = y_val - y_pred_poly\n",
    "\n",
    "# Residual comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Baseline residuals\n",
    "axes[0, 0].scatter(y_pred_baseline, residuals_baseline, alpha=0.5)\n",
    "axes[0, 0].axhline(y=0, color='r', linestyle='--')\n",
    "axes[0, 0].set_xlabel('Predicted')\n",
    "axes[0, 0].set_ylabel('Residuals')\n",
    "axes[0, 0].set_title('Baseline Model Residuals')\n",
    "\n",
    "# Polynomial residuals\n",
    "axes[0, 1].scatter(y_pred_poly, residuals_poly, alpha=0.5, color='orange')\n",
    "axes[0, 1].axhline(y=0, color='r', linestyle='--')\n",
    "axes[0, 1].set_xlabel('Predicted')\n",
    "axes[0, 1].set_ylabel('Residuals')\n",
    "axes[0, 1].set_title('Polynomial Model Residuals')\n",
    "\n",
    "# Baseline distribution\n",
    "axes[1, 0].hist(residuals_baseline, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].axvline(x=0, color='r', linestyle='--')\n",
    "axes[1, 0].set_xlabel('Residual')\n",
    "axes[1, 0].set_title('Baseline Residual Distribution')\n",
    "\n",
    "# Polynomial distribution\n",
    "axes[1, 1].hist(residuals_poly, bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[1, 1].axvline(x=0, color='r', linestyle='--')\n",
    "axes[1, 1].set_xlabel('Residual')\n",
    "axes[1, 1].set_title('Polynomial Residual Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=== RESIDUAL STATISTICS ===\")\n",
    "print(f\"Baseline MAE: {mean_absolute_error(y_val, y_pred_baseline):.4f}\")\n",
    "print(f\"Polynomial MAE: {mean_absolute_error(y_val, y_pred_poly):.4f}\")\n",
    "print(f\"\\nBaseline RMSE: {np.sqrt(mean_squared_error(y_val, y_pred_baseline)):.4f}\")\n",
    "print(f\"Polynomial RMSE: {np.sqrt(mean_squared_error(y_val, y_pred_poly)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd PAUSE-AND-DO Exercise 2 (10 minutes)\n",
    "\n",
    "**Task:** Write a short diagnostic conclusion (what error patterns suggest).\n",
    "\n",
    "Look at the residual plots above and answer:\n",
    "1. Do you see any patterns (non-random scatter)?\n",
    "2. Is there a funnel shape (heteroskedasticity)?\n",
    "3. Does the polynomial model fix any issues?\n",
    "4. What would you try next?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR DIAGNOSTIC CONCLUSION:\n",
    "\n",
    "**Pattern Analysis:**  \n",
    "[Describe any patterns you see in residuals]\n",
    "\n",
    "**Heteroskedasticity:**  \n",
    "[Is variance constant across predictions?]\n",
    "\n",
    "**Model Comparison:**  \n",
    "[Which model has better residual behavior?]\n",
    "\n",
    "**Next Steps:**  \n",
    "[What would you try to improve the model?]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison = pd.DataFrame([\n",
    "    {\n",
    "        'Model': 'Baseline Linear',\n",
    "        'Features': X_train.shape[1],\n",
    "        'Train_R2': train_score,\n",
    "        'Val_R2': val_score,\n",
    "        'Overfit_Gap': train_score - val_score,\n",
    "        'Val_MAE': mean_absolute_error(y_val, y_pred_baseline)\n",
    "    },\n",
    "    {\n",
    "        'Model': 'With Interaction',\n",
    "        'Features': X_train_interact.shape[1],\n",
    "        'Train_R2': train_score_interact,\n",
    "        'Val_R2': val_score_interact,\n",
    "        'Overfit_Gap': train_score_interact - val_score_interact,\n",
    "        'Val_MAE': mean_absolute_error(y_val, interact_pipeline.predict(X_val_interact))\n",
    "    },\n",
    "    {\n",
    "        'Model': 'Polynomial (deg=2)',\n",
    "        'Features': n_features_poly,\n",
    "        'Train_R2': train_score_poly,\n",
    "        'Val_R2': val_score_poly,\n",
    "        'Overfit_Gap': train_score_poly - val_score_poly,\n",
    "        'Val_MAE': mean_absolute_error(y_val, y_pred_poly)\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"=== MODEL COMPARISON ===\")\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "# Best model\n",
    "best_idx = comparison['Val_R2'].idxmax()\n",
    "print(f\"\\n\u2713 Best validation R\u00b2: {comparison.loc[best_idx, 'Model']} ({comparison.loc[best_idx, 'Val_R2']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Wrap-Up: Key Takeaways\n\n### What We Learned Today:\n\n1. **Coefficient Interpretation**: Requires scaling, caution about multicollinearity\n2. **Feature Engineering**: Interactions and polynomials can help, but watch overfitting\n3. **Residual Diagnostics**: Visual patterns reveal model inadequacies\n4. **Complexity Tradeoff**: More features \u2260 better performance\n5. **Validation Discipline**: Always check validation scores, not just training\n\n### Critical Rules:\n\n> **\"Scale features before interpreting coefficients\"**\n\n> **\"Polynomial features explode exponentially - use sparingly\"**\n\n> **\"Residual plots never lie\"**\n\n### Next Steps:\n\n- Next notebook: Regularization (Ridge/Lasso) + Project proposal\n- Regularization will help control overfitting from complex features\n- Start thinking about your project dataset and target\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n\n- James, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). *An Introduction to Statistical Learning with Python* - Linear Regression chapter\n- Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning* - Linear methods\n- scikit-learn User Guide: [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n- scikit-learn User Guide: [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)\n\n---\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "Thank you!\n",
    "\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}