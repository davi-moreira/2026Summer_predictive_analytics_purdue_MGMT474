{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 7: Classification Metrics - Confusion Matrix, ROC/PR, Calibration, and Business Costs\n",
    "\n",
    "**MGMT 47400 - Predictive Analytics**  \n",
    "**4-Week Online Course**  \n",
    "**Day 7 - Wednesday May 26, 2027**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/blob/main/notebooks/07_classification_metrics_thresholding.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Compute and interpret precision, recall, F1, ROC-AUC, PR-AUC\n",
    "2. Select thresholds based on business cost tradeoffs\n",
    "3. Understand calibration and when it matters\n",
    "4. Handle class imbalance at the evaluation level (metrics first)\n",
    "5. Produce a metrics dashboard table for model comparison\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report,\n",
    "    precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve,\n",
    "    precision_recall_curve, average_precision_score,\n",
    "    ConfusionMatrixDisplay, RocCurveDisplay, PrecisionRecallDisplay\n",
    ")\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.precision', 4)\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "print(\"âœ“ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data = load_breast_cancer(as_frame=True)\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.20, random_state=RANDOM_SEED, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=RANDOM_SEED, stratify=y_temp)\n",
    "\n",
    "# Train model\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(random_state=RANDOM_SEED, max_iter=1000))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred_val = pipeline.predict(X_val)\n",
    "y_proba_val = pipeline.predict_proba(X_val)[:, 1]\n",
    "\n",
    "print(f\"Train: {len(X_train)} | Val: {len(X_val)} | Test: {len(X_test)}\")\n",
    "print(f\"Validation Accuracy: {pipeline.score(X_val, y_val):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Confusion Matrix Deep Dive\n",
    "\n",
    "### Understanding the 2Ã—2 Matrix\n",
    "\n",
    "```\n",
    "                    Predicted Negative    Predicted Positive\n",
    "Actual Negative          TN                     FP\n",
    "Actual Positive          FN                     TP\n",
    "```\n",
    "\n",
    "**Definitions:**\n",
    "- **True Positive (TP)**: Correctly predicted positive\n",
    "- **True Negative (TN)**: Correctly predicted negative\n",
    "- **False Positive (FP)**: Incorrectly predicted positive (Type I error)\n",
    "- **False Negative (FN)**: Incorrectly predicted negative (Type II error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_val, y_pred_val)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Malignant (0)', 'Benign (1)'])\n",
    "disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "plt.title('Confusion Matrix - Validation Set')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "print(\"=== CONFUSION MATRIX VALUES ===\")\n",
    "print(f\"True Negatives (TN):  {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")\n",
    "print(f\"True Positives (TP):  {TP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Core Classification Metrics\n",
    "\n",
    "### Precision\n",
    "**Precision = TP / (TP + FP)**\n",
    "- \"Of all positive predictions, how many were correct?\"\n",
    "- High precision = few false alarms\n",
    "\n",
    "### Recall (Sensitivity, True Positive Rate)\n",
    "**Recall = TP / (TP + FN)**\n",
    "- \"Of all actual positives, how many did we find?\"\n",
    "- High recall = few missed positives\n",
    "\n",
    "### F1 Score\n",
    "**F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)**\n",
    "- Harmonic mean of precision and recall\n",
    "- Useful when you need balance\n",
    "\n",
    "### Accuracy\n",
    "**Accuracy = (TP + TN) / (TP + TN + FP + FN)**\n",
    "- Overall correctness\n",
    "- âš ï¸ Misleading with class imbalance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate all metrics\n",
    "metrics = {\n",
    "    'Accuracy': (TP + TN) / (TP + TN + FP + FN),\n",
    "    'Precision': TP / (TP + FP) if (TP + FP) > 0 else 0,\n",
    "    'Recall': TP / (TP + FN) if (TP + FN) > 0 else 0,\n",
    "    'F1': f1_score(y_val, y_pred_val),\n",
    "    'Specificity': TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "}\n",
    "\n",
    "print(\"=== CLASSIFICATION METRICS ===\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric:15s}: {value:.4f}\")\n",
    "\n",
    "# Using sklearn functions\n",
    "print(\"\\n=== SKLEARN CLASSIFICATION REPORT ===\")\n",
    "print(classification_report(y_val, y_pred_val, target_names=['Malignant', 'Benign']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ROC Curve and AUC\n",
    "\n",
    "### ROC (Receiver Operating Characteristic)\n",
    "- Plots True Positive Rate vs False Positive Rate\n",
    "- Shows performance across all thresholds\n",
    "- AUC (Area Under Curve) summarizes performance\n",
    "\n",
    "**AUC Interpretation:**\n",
    "- AUC = 1.0: Perfect classifier\n",
    "- AUC = 0.5: Random guessing\n",
    "- AUC < 0.5: Worse than random (predictions inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "fpr, tpr, thresholds_roc = roc_curve(y_val, y_proba_val)\n",
    "roc_auc = roc_auc_score(y_val, y_proba_val)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate (Recall)')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "print(\"\\nðŸ’¡ Higher AUC = better separation between classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Precision-Recall Curve\n",
    "\n",
    "### When to Use PR Curve vs ROC\n",
    "\n",
    "**Use ROC when:**\n",
    "- Classes are balanced\n",
    "- You care about both TPR and FPR\n",
    "\n",
    "**Use PR Curve when:**\n",
    "- Classes are imbalanced\n",
    "- Positive class is more important\n",
    "- You want to focus on precision/recall tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curve\n",
    "precision, recall, thresholds_pr = precision_recall_curve(y_val, y_proba_val)\n",
    "pr_auc = average_precision_score(y_val, y_proba_val)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, linewidth=2, label=f'PR Curve (AP = {pr_auc:.3f})')\n",
    "baseline = y_val.sum() / len(y_val)\n",
    "plt.axhline(y=baseline, color='k', linestyle='--', linewidth=1, label=f'Baseline (class ratio = {baseline:.3f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average Precision (PR-AUC): {pr_auc:.4f}\")\n",
    "print(\"\\nðŸ’¡ Closer to top-right = better model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ PAUSE-AND-DO Exercise 1 (10 minutes)\n",
    "\n",
    "**Task:** Build a threshold sweep and pick a threshold by business cost.\n",
    "\n",
    "**Scenario:** Medical diagnosis\n",
    "- False Negative (missed cancer): Cost = $50,000 (late treatment)\n",
    "- False Positive (false alarm): Cost = $1,000 (unnecessary biopsy)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold sweep with cost analysis\n",
    "cost_FN = 50000  # Cost of missing a malignant case\n",
    "cost_FP = 1000   # Cost of false alarm\n",
    "\n",
    "thresholds_to_test = np.linspace(0.1, 0.9, 50)\n",
    "threshold_results = []\n",
    "\n",
    "for thresh in thresholds_to_test:\n",
    "    y_pred_thresh = (y_proba_val >= thresh).astype(int)\n",
    "    cm = confusion_matrix(y_val, y_pred_thresh)\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    \n",
    "    # Calculate expected cost\n",
    "    total_cost = (FN * cost_FN) + (FP * cost_FP)\n",
    "    \n",
    "    threshold_results.append({\n",
    "        'Threshold': thresh,\n",
    "        'TP': TP,\n",
    "        'FP': FP,\n",
    "        'FN': FN,\n",
    "        'TN': TN,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'Total_Cost': total_cost\n",
    "    })\n",
    "\n",
    "threshold_df = pd.DataFrame(threshold_results)\n",
    "\n",
    "# Find optimal threshold (minimum cost)\n",
    "optimal_idx = threshold_df['Total_Cost'].idxmin()\n",
    "optimal_thresh = threshold_df.loc[optimal_idx, 'Threshold']\n",
    "optimal_cost = threshold_df.loc[optimal_idx, 'Total_Cost']\n",
    "\n",
    "print(\"=== OPTIMAL THRESHOLD BY COST ===\")\n",
    "print(f\"Optimal Threshold: {optimal_thresh:.3f}\")\n",
    "print(f\"Expected Cost: ${optimal_cost:,.0f}\")\n",
    "print(f\"\\nAt this threshold:\")\n",
    "print(f\"  Precision: {threshold_df.loc[optimal_idx, 'Precision']:.4f}\")\n",
    "print(f\"  Recall: {threshold_df.loc[optimal_idx, 'Recall']:.4f}\")\n",
    "print(f\"  FP: {threshold_df.loc[optimal_idx, 'FP']:.0f}\")\n",
    "print(f\"  FN: {threshold_df.loc[optimal_idx, 'FN']:.0f}\")\n",
    "\n",
    "# Visualize cost vs threshold\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(threshold_df['Threshold'], threshold_df['Total_Cost'], linewidth=2)\n",
    "axes[0].axvline(x=optimal_thresh, color='r', linestyle='--', label=f'Optimal = {optimal_thresh:.3f}')\n",
    "axes[0].set_xlabel('Threshold')\n",
    "axes[0].set_ylabel('Expected Cost ($)')\n",
    "axes[0].set_title('Total Cost vs Threshold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(threshold_df['Threshold'], threshold_df['Precision'], label='Precision', linewidth=2)\n",
    "axes[1].plot(threshold_df['Threshold'], threshold_df['Recall'], label='Recall', linewidth=2)\n",
    "axes[1].axvline(x=optimal_thresh, color='r', linestyle='--', alpha=0.5)\n",
    "axes[1].set_xlabel('Threshold')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_title('Precision-Recall vs Threshold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANALYSIS:\n",
    "\n",
    "**Question 1: Why is the optimal threshold different from 0.5?**  \n",
    "[Your answer - think about asymmetric costs]\n",
    "\n",
    "**Question 2: What happens if you change the cost ratio?**  \n",
    "[Your answer - try FN cost = $100,000]\n",
    "\n",
    "**Question 3: In production, how would you monitor this?**  \n",
    "[Your answer - what could go wrong over time?]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ PAUSE-AND-DO Exercise 2 (10 minutes)\n",
    "\n",
    "**Task:** Explain why accuracy fails under imbalance (with evidence).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create imbalanced scenario\n",
    "# Simulate: 95% benign, 5% malignant\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X_imb, y_imb = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    weights=[0.95, 0.05],  # 95% class 0, 5% class 1\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "X_train_imb, X_val_imb, y_train_imb, y_val_imb = train_test_split(\n",
    "    X_imb, y_imb, test_size=0.3, random_state=RANDOM_SEED, stratify=y_imb\n",
    ")\n",
    "\n",
    "# Train model\n",
    "pipe_imb = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(random_state=RANDOM_SEED, max_iter=1000))\n",
    "])\n",
    "pipe_imb.fit(X_train_imb, y_train_imb)\n",
    "\n",
    "y_pred_imb = pipe_imb.predict(X_val_imb)\n",
    "\n",
    "# Compare metrics\n",
    "print(\"=== IMBALANCED DATASET (95% class 0, 5% class 1) ===\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(pd.Series(y_val_imb).value_counts(normalize=True))\n",
    "\n",
    "print(f\"\\n=== MODEL PERFORMANCE ===\")\n",
    "print(classification_report(y_val_imb, y_pred_imb, target_names=['Class 0 (95%)', 'Class 1 (5%)']))\n",
    "\n",
    "# Naive baseline: always predict majority class\n",
    "y_naive = np.zeros_like(y_val_imb)\n",
    "naive_acc = (y_naive == y_val_imb).mean()\n",
    "\n",
    "print(f\"\\n=== NAIVE BASELINE (always predict 0) ===\")\n",
    "print(f\"Accuracy: {naive_acc:.4f}\")\n",
    "print(f\"Recall for class 1: 0.0000 (missed all positives!)\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ Key Insight: {naive_acc*100:.1f}% accuracy by predicting everything as class 0!\")\n",
    "print(f\"ðŸ’¡ This is why accuracy alone is dangerous with imbalance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR EXPLANATION:\n",
    "\n",
    "**Why accuracy is misleading:**  \n",
    "[Your explanation with evidence from above]\n",
    "\n",
    "**Better metrics for imbalance:**  \n",
    "[Which metrics would you use instead?]\n",
    "\n",
    "**Real-world example:**  \n",
    "[Give an example where this matters]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Metrics Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete metrics dashboard\n",
    "def create_metrics_dashboard(y_true, y_pred, y_proba):\n",
    "    \"\"\"Generate comprehensive classification metrics\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    \n",
    "    metrics = {\n",
    "        'Accuracy': (TP + TN) / (TP + TN + FP + FN),\n",
    "        'Precision': TP / (TP + FP) if (TP + FP) > 0 else 0,\n",
    "        'Recall': TP / (TP + FN) if (TP + FN) > 0 else 0,\n",
    "        'F1': f1_score(y_true, y_pred),\n",
    "        'Specificity': TN / (TN + FP) if (TN + FP) > 0 else 0,\n",
    "        'ROC_AUC': roc_auc_score(y_true, y_proba),\n",
    "        'PR_AUC': average_precision_score(y_true, y_proba),\n",
    "        'TP': TP,\n",
    "        'FP': FP,\n",
    "        'FN': FN,\n",
    "        'TN': TN\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "dashboard = create_metrics_dashboard(y_val, y_pred_val, y_proba_val)\n",
    "\n",
    "print(\"=== COMPREHENSIVE METRICS DASHBOARD ===\")\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"  Accuracy:    {dashboard['Accuracy']:.4f}\")\n",
    "print(f\"  Precision:   {dashboard['Precision']:.4f}\")\n",
    "print(f\"  Recall:      {dashboard['Recall']:.4f}\")\n",
    "print(f\"  F1 Score:    {dashboard['F1']:.4f}\")\n",
    "print(f\"  Specificity: {dashboard['Specificity']:.4f}\")\n",
    "print(f\"  ROC-AUC:     {dashboard['ROC_AUC']:.4f}\")\n",
    "print(f\"  PR-AUC:      {dashboard['PR_AUC']:.4f}\")\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  TP: {dashboard['TP']:4d}    FP: {dashboard['FP']:4d}\")\n",
    "print(f\"  FN: {dashboard['FN']:4d}    TN: {dashboard['TN']:4d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Wrap-Up: Key Takeaways\n",
    "\n",
    "### What We Learned Today:\n",
    "\n",
    "1. **Confusion Matrix**: Foundation for understanding classification errors\n",
    "2. **Precision vs Recall**: Tradeoff between false positives and false negatives\n",
    "3. **ROC and PR Curves**: Visualize performance across thresholds\n",
    "4. **Cost-Based Thresholding**: Align decisions to business objectives\n",
    "5. **Imbalance Handling**: Accuracy is dangerous - use precision, recall, AUC\n",
    "\n",
    "### Critical Rules:\n",
    "\n",
    "> **\"Never trust accuracy alone\"**\n",
    "\n",
    "> **\"Choose thresholds based on business costs, not defaults\"**\n",
    "\n",
    "> **\"With imbalance, use PR curves over ROC curves\"**\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Day 8: Cross-validation for robust model comparison\n",
    "- We'll use today's metrics with proper CV evaluation\n",
    "- Apply to your project dataset\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "- Fawcett, T. (2006). \"An introduction to ROC analysis.\" *Pattern Recognition Letters*, 27(8), 861-874.\n",
    "- Saito, T., & Rehmsmeier, M. (2015). \"The precision-recall plot is more informative than the ROC plot when evaluating binary classifiers on imbalanced datasets.\" *PLOS ONE*.\n",
    "- scikit-learn User Guide: [Classification metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics)\n",
    "- Provost, F., & Fawcett, T. (2013). *Data Science for Business* - Chapter on evaluation and costs\n",
    "\n",
    "---\n",
    "\n",
    "**End of Day 7 Notebook**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
