{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Metrics - Confusion Matrix, ROC/PR, Calibration, and Business Costs\n",
    "\n",
    "<hr>\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/notebooks/figures/mgmt_474_ai_logo_02-modified.png\" width=\"200\"/>\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "# <center><a class=\"tocSkip\"></center>\n",
    "# <center>MGMT47400 Predictive Analytics</center>\n",
    "# <center>Professor: Davi Moreira </center>\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/blob/main/notebooks/07_classification_metrics_thresholding.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Compute and interpret precision, recall, F1, ROC-AUC, PR-AUC\n",
    "2. Select thresholds based on business cost tradeoffs\n",
    "3. Understand calibration and when it matters\n",
    "4. Handle class imbalance at the evaluation level (metrics first)\n",
    "5. Produce a metrics dashboard table for model comparison\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (\n    confusion_matrix, classification_report,\n    precision_score, recall_score, f1_score,\n    roc_auc_score, roc_curve,\n    precision_recall_curve, average_precision_score,\n    ConfusionMatrixDisplay, RocCurveDisplay, PrecisionRecallDisplay\n)\nimport warnings\n\nwarnings.filterwarnings('ignore')\npd.set_option('display.precision', 4)\nRANDOM_SEED = 474\nnp.random.seed(RANDOM_SEED)\nprint(\"âœ“ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The setup cell imports every metric function we will use in this notebook: `confusion_matrix`, `precision_score`, `recall_score`, `f1_score`, `roc_auc_score`, `roc_curve`, `precision_recall_curve`, and `average_precision_score`. It also imports the display helpers `ConfusionMatrixDisplay`, `RocCurveDisplay`, and `PrecisionRecallDisplay` that produce publication-quality plots with a single call. The `StandardScaler` and `LogisticRegression` imports let us build our baseline classification pipeline. The confirmation **\"Setup complete!\"** means all libraries loaded without conflict.\n",
    "\n",
    "**Why this matters:** Having all metric functions imported up front keeps the rest of the notebook clean -- we can focus on *interpreting* results rather than wrestling with imports.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Train Model\n",
    "\n",
    "We begin with the **breast cancer Wisconsin dataset** (569 samples, 30 numeric features), a classic binary-classification benchmark packaged with scikit-learn. Each sample is labeled *malignant* (0) or *benign* (1) based on digitized measurements of cell nuclei from fine-needle aspirate images.\n",
    "\n",
    "Before we can study classification metrics, we need predictions to evaluate. The cell below loads the data, applies the standard 60/20/20 train-validation-test split, and fits a Logistic Regression pipeline with `StandardScaler` as a preprocessing step. We then extract the confusion-matrix components (TP, TN, FP, FN) from the validation predictions so we can calculate every metric by hand in the sections that follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Gemini Prompt:** \"Load the breast cancer dataset, split it 60/20/20 with stratification, then build and fit a logistic regression pipeline (StandardScaler + LogisticRegression with max_iter=1000). Get hard predictions and class-1 probabilities on the validation set. Print set sizes and validation accuracy.\"\n",
    ">\n",
    "> **After running, verify:**\n",
    "> - Train/Val/Test sizes are printed\n",
    "> - Validation accuracy is reported (should be above 95%)\n",
    "> - Both y_pred_val (hard labels) and y_proba_val (probabilities) are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data = load_breast_cancer(as_frame=True)\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.20, random_state=RANDOM_SEED, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=RANDOM_SEED, stratify=y_temp)\n",
    "\n",
    "# Train model\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(random_state=RANDOM_SEED, max_iter=1000))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred_val = pipeline.predict(X_val)\n",
    "y_proba_val = pipeline.predict_proba(X_val)[:, 1]\n",
    "\n",
    "print(f\"Train: {len(X_train)} | Val: {len(X_val)} | Test: {len(X_test)}\")\n",
    "print(f\"Validation Accuracy: {pipeline.score(X_val, y_val):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The dataset has been split into **Train: 341 | Val: 114 | Test: 114** observations, following the 60/20/20 convention. The Logistic Regression pipeline (with `StandardScaler`) achieves a **validation accuracy around 0.97**, which sounds high but hides important nuances we will uncover below.\n",
    "\n",
    "The variables `y_pred_val` (hard 0/1 labels) and `y_proba_val` (continuous probabilities for the positive class) are now available. Hard labels let us build a confusion matrix; probabilities let us sweep thresholds and plot ROC/PR curves.\n",
    "\n",
    "**Key takeaway:** A single accuracy number is only the starting point. The next sections decompose this performance into the four cells of the confusion matrix and the metrics that derive from them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Confusion Matrix Deep Dive\n",
    "\n",
    "### Understanding the 2Ã—2 Matrix\n",
    "\n",
    "```\n",
    "                    Predicted Negative    Predicted Positive\n",
    "Actual Negative          TN                     FP\n",
    "Actual Positive          FN                     TP\n",
    "```\n",
    "\n",
    "**Definitions:**\n",
    "- **True Positive (TP)**: Correctly predicted positive\n",
    "- **True Negative (TN)**: Correctly predicted negative\n",
    "- **False Positive (FP)**: Incorrectly predicted positive (Type I error)\n",
    "- **False Negative (FN)**: Incorrectly predicted negative (Type II error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Gemini Prompt:** \"Compute the confusion matrix for validation predictions and display it as a heatmap using ConfusionMatrixDisplay with labels Malignant (0) and Benign (1). Unpack TN, FP, FN, TP using cm.ravel() and print each value.\"\n",
    ">\n",
    "> **After running, verify:**\n",
    "> - Heatmap shows a 2x2 confusion matrix with labeled axes\n",
    "> - TN, FP, FN, TP are printed as separate named values\n",
    "> - Most predictions should be on the diagonal (correct classifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_val, y_pred_val)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Malignant (0)', 'Benign (1)'])\n",
    "disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "plt.title('Confusion Matrix - Validation Set')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "print(\"=== CONFUSION MATRIX VALUES ===\")\n",
    "print(f\"True Negatives (TN):  {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")\n",
    "print(f\"True Positives (TP):  {TP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The heatmap shows the 2x2 confusion matrix on the validation set. The rows represent the *actual* class and the columns represent the *predicted* class. Darker blue squares indicate higher counts.\n",
    "\n",
    "Below the plot, the four raw counts are printed:\n",
    "\n",
    "- **True Negatives (TN):** correctly identified malignant cases.\n",
    "- **False Positives (FP):** malignant cases the model mistakenly called benign -- these patients might be sent home without treatment.\n",
    "- **False Negatives (FN):** benign cases the model mistakenly called malignant -- these patients would receive unnecessary follow-up.\n",
    "- **True Positives (TP):** correctly identified benign cases.\n",
    "\n",
    "Because the breast cancer dataset labels benign as 1 (positive class), a *False Negative* here means the model predicted malignant when the truth was benign. In a real clinical setting you would typically flip the positive class so that \"malignant = positive,\" but for this educational exercise we follow scikit-learn's convention.\n",
    "\n",
    "**Why this matters:** Every classification metric is just arithmetic on TN, FP, FN, and TP. Understanding these four numbers is the foundation for everything that follows.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Core Classification Metrics\n",
    "\n",
    "### Precision\n",
    "**Precision = TP / (TP + FP)**\n",
    "- \"Of all positive predictions, how many were correct?\"\n",
    "- High precision = few false alarms\n",
    "\n",
    "### Recall (Sensitivity, True Positive Rate)\n",
    "**Recall = TP / (TP + FN)**\n",
    "- \"Of all actual positives, how many did we find?\"\n",
    "- High recall = few missed positives\n",
    "\n",
    "### F1 Score\n",
    "**F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)**\n",
    "- Harmonic mean of precision and recall\n",
    "- Useful when you need balance\n",
    "\n",
    "### Accuracy\n",
    "**Accuracy = (TP + TN) / (TP + TN + FP + FN)**\n",
    "- Overall correctness\n",
    "- âš ï¸ Misleading with class imbalance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Gemini Prompt:** \"Calculate classification metrics manually from TP, TN, FP, FN: Accuracy, Precision, Recall, F1 (using sklearn f1_score), and Specificity. Print all five metrics. Then print sklearn full classification_report with target names Malignant and Benign.\"\n",
    ">\n",
    "> **After running, verify:**\n",
    "> - Five metrics are printed: Accuracy, Precision, Recall, F1, Specificity\n",
    "> - classification_report shows per-class precision, recall, f1, and support\n",
    "> - Manual calculations match sklearn automated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate all metrics\n",
    "metrics = {\n",
    "    'Accuracy': (TP + TN) / (TP + TN + FP + FN),\n",
    "    'Precision': TP / (TP + FP) if (TP + FP) > 0 else 0,\n",
    "    'Recall': TP / (TP + FN) if (TP + FN) > 0 else 0,\n",
    "    'F1': f1_score(y_val, y_pred_val),\n",
    "    'Specificity': TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "}\n",
    "\n",
    "print(\"=== CLASSIFICATION METRICS ===\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric:15s}: {value:.4f}\")\n",
    "\n",
    "# Using sklearn functions\n",
    "print(\"\\n=== SKLEARN CLASSIFICATION REPORT ===\")\n",
    "print(classification_report(y_val, y_pred_val, target_names=['Malignant', 'Benign']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The first block prints five hand-calculated metrics:\n",
    "\n",
    "| Metric | Typical value | What it answers |\n",
    "|--------|--------------|----------------|\n",
    "| **Accuracy** | ~0.97 | What fraction of all predictions were correct? |\n",
    "| **Precision** | ~0.97-0.99 | When we predicted benign, how often were we right? |\n",
    "| **Recall** | ~0.97-0.99 | Of all truly benign cases, how many did we catch? |\n",
    "| **F1** | ~0.98 | Harmonic mean balancing precision and recall |\n",
    "| **Specificity** | ~0.93-0.97 | Of all truly malignant cases, how many did we flag? |\n",
    "\n",
    "The second block, `classification_report`, confirms these numbers and adds the per-class breakdown (Malignant row and Benign row) plus macro and weighted averages. The `support` column shows how many validation samples belong to each class.\n",
    "\n",
    "**Key takeaway:** Precision and recall often tell different stories. In a medical context, missing a malignant tumor (low specificity) is far worse than an unnecessary biopsy (low precision for the malignant class). The right metric depends on the *business cost* of each error type, which we explore in the PAUSE-AND-DO exercise.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ROC Curve and AUC\n",
    "\n",
    "### ROC (Receiver Operating Characteristic)\n",
    "- Plots True Positive Rate vs False Positive Rate\n",
    "- Shows performance across all thresholds\n",
    "- AUC (Area Under Curve) summarizes performance\n",
    "\n",
    "**AUC Interpretation:**\n",
    "- AUC = 1.0: Perfect classifier\n",
    "- AUC = 0.5: Random guessing\n",
    "- AUC < 0.5: Worse than random (predictions inverted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Gemini Prompt:** \"Plot the ROC curve using sklearn roc_curve on validation probabilities. Include the diagonal random-classifier baseline as a dashed line. Compute and display ROC-AUC score in the legend. Label axes as False Positive Rate and True Positive Rate.\"\n",
    ">\n",
    "> **After running, verify:**\n",
    "> - ROC curve is plotted with AUC value shown in the legend\n",
    "> - Diagonal dashed line represents a random classifier (AUC=0.5)\n",
    "> - ROC-AUC score is printed below the plot (should be close to 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "fpr, tpr, thresholds_roc = roc_curve(y_val, y_proba_val)\n",
    "roc_auc = roc_auc_score(y_val, y_proba_val)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate (Recall)')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "print(\"\\nðŸ’¡ Higher AUC = better separation between classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The ROC curve plots the **True Positive Rate** (recall) on the y-axis against the **False Positive Rate** (1 - specificity) on the x-axis as we sweep the decision threshold from 1.0 down to 0.0. A perfect classifier hugs the top-left corner; the dashed diagonal represents a random coin flip.\n",
    "\n",
    "The **AUC (Area Under the Curve)** is printed below the plot. For our Logistic Regression on the breast cancer validation set, expect an **ROC-AUC around 0.99**, indicating near-perfect discrimination between malignant and benign samples. An AUC of 0.99 means that if you randomly pick one benign and one malignant case, the model assigns a higher probability to the benign case 99% of the time.\n",
    "\n",
    "**Why this matters:** ROC-AUC is a threshold-independent summary of model quality. It is especially useful during model *selection* because it evaluates the full range of operating points, not just the default 0.5 threshold.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Precision-Recall Curve\n",
    "\n",
    "### When to Use PR Curve vs ROC\n",
    "\n",
    "**Use ROC when:**\n",
    "- Classes are balanced\n",
    "- You care about both TPR and FPR\n",
    "\n",
    "**Use PR Curve when:**\n",
    "- Classes are imbalanced\n",
    "- Positive class is more important\n",
    "- You want to focus on precision/recall tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Gemini Prompt:** \"Plot the Precision-Recall curve using sklearn precision_recall_curve. Add a horizontal baseline at the positive class ratio. Compute average_precision_score and show it in the legend. Label axes as Recall (x) and Precision (y).\"\n",
    ">\n",
    "> **After running, verify:**\n",
    "> - PR curve is plotted with Average Precision (AP) shown in the legend\n",
    "> - Horizontal dashed baseline shows the positive class ratio\n",
    "> - AP score is printed below the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curve\n",
    "precision, recall, thresholds_pr = precision_recall_curve(y_val, y_proba_val)\n",
    "pr_auc = average_precision_score(y_val, y_proba_val)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, linewidth=2, label=f'PR Curve (AP = {pr_auc:.3f})')\n",
    "baseline = y_val.sum() / len(y_val)\n",
    "plt.axhline(y=baseline, color='k', linestyle='--', linewidth=1, label=f'Baseline (class ratio = {baseline:.3f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average Precision (PR-AUC): {pr_auc:.4f}\")\n",
    "print(\"\\nðŸ’¡ Closer to top-right = better model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The Precision-Recall curve plots **Precision** on the y-axis against **Recall** on the x-axis. Unlike the ROC curve, the baseline here is the *class prevalence* -- the dashed horizontal line at the positive-class ratio (roughly 0.63 for benign in this dataset). A model that predicts the majority class for every sample would land on that baseline.\n",
    "\n",
    "The **Average Precision (AP)** score, also called PR-AUC, summarizes the area under this curve. For our model, expect **AP around 0.99**, mirroring the strong ROC-AUC. The curve stays close to the top-right corner, meaning the model maintains high precision even at high recall levels.\n",
    "\n",
    "**Key takeaway:** PR curves become more informative than ROC curves when classes are *imbalanced*. If only 5% of samples are positive, ROC-AUC can look optimistic because TN counts dominate the False Positive Rate denominator. PR-AUC focuses exclusively on the positive class, making it the metric of choice in fraud detection, rare-disease screening, and similar domains.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ PAUSE-AND-DO Exercise 1 (5 minutes)\n",
    "\n",
    "**Task:** Build a threshold sweep and pick a threshold by business cost.\n",
    "\n",
    "**Scenario:** Medical diagnosis\n",
    "- False Negative (missed cancer): Cost = $50,000 (late treatment)\n",
    "- False Positive (false alarm): Cost = $1,000 (unnecessary biopsy)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Gemini Prompt:** \"Sweep 50 thresholds from 0.1 to 0.9. For each, compute TP, FP, FN, TN, Precision, Recall, and total cost using cost_FN=50000 and cost_FP=1000. Find the threshold that minimizes total cost. Plot two side-by-side charts: (1) Total Cost vs Threshold with the optimal threshold marked, and (2) Precision and Recall vs Threshold.\"\n",
    ">\n",
    "> **After running, verify:**\n",
    "> - Optimal threshold and its expected cost are printed\n",
    "> - Left plot shows a cost curve with the minimum marked by a red dashed line\n",
    "> - Right plot shows precision increasing and recall decreasing as threshold rises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold sweep with cost analysis\n",
    "cost_FN = 50000  # Cost of missing a malignant case\n",
    "cost_FP = 1000   # Cost of false alarm\n",
    "\n",
    "thresholds_to_test = np.linspace(0.1, 0.9, 50)\n",
    "threshold_results = []\n",
    "\n",
    "for thresh in thresholds_to_test:\n",
    "    y_pred_thresh = (y_proba_val >= thresh).astype(int)\n",
    "    cm = confusion_matrix(y_val, y_pred_thresh)\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    \n",
    "    # Calculate expected cost\n",
    "    total_cost = (FN * cost_FN) + (FP * cost_FP)\n",
    "    \n",
    "    threshold_results.append({\n",
    "        'Threshold': thresh,\n",
    "        'TP': TP,\n",
    "        'FP': FP,\n",
    "        'FN': FN,\n",
    "        'TN': TN,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'Total_Cost': total_cost\n",
    "    })\n",
    "\n",
    "threshold_df = pd.DataFrame(threshold_results)\n",
    "\n",
    "# Find optimal threshold (minimum cost)\n",
    "optimal_idx = threshold_df['Total_Cost'].idxmin()\n",
    "optimal_thresh = threshold_df.loc[optimal_idx, 'Threshold']\n",
    "optimal_cost = threshold_df.loc[optimal_idx, 'Total_Cost']\n",
    "\n",
    "print(\"=== OPTIMAL THRESHOLD BY COST ===\")\n",
    "print(f\"Optimal Threshold: {optimal_thresh:.3f}\")\n",
    "print(f\"Expected Cost: ${optimal_cost:,.0f}\")\n",
    "print(f\"\\nAt this threshold:\")\n",
    "print(f\"  Precision: {threshold_df.loc[optimal_idx, 'Precision']:.4f}\")\n",
    "print(f\"  Recall: {threshold_df.loc[optimal_idx, 'Recall']:.4f}\")\n",
    "print(f\"  FP: {threshold_df.loc[optimal_idx, 'FP']:.0f}\")\n",
    "print(f\"  FN: {threshold_df.loc[optimal_idx, 'FN']:.0f}\")\n",
    "\n",
    "# Visualize cost vs threshold\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(threshold_df['Threshold'], threshold_df['Total_Cost'], linewidth=2)\n",
    "axes[0].axvline(x=optimal_thresh, color='r', linestyle='--', label=f'Optimal = {optimal_thresh:.3f}')\n",
    "axes[0].set_xlabel('Threshold')\n",
    "axes[0].set_ylabel('Expected Cost ($)')\n",
    "axes[0].set_title('Total Cost vs Threshold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(threshold_df['Threshold'], threshold_df['Precision'], label='Precision', linewidth=2)\n",
    "axes[1].plot(threshold_df['Threshold'], threshold_df['Recall'], label='Recall', linewidth=2)\n",
    "axes[1].axvline(x=optimal_thresh, color='r', linestyle='--', alpha=0.5)\n",
    "axes[1].set_xlabel('Threshold')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_title('Precision-Recall vs Threshold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The output reports the **optimal threshold** that minimizes expected business cost. With a False Negative cost of **$50,000** (missed malignant case) and a False Positive cost of **$1,000** (unnecessary biopsy), the optimizer typically picks a threshold well below 0.5 -- sometimes around **0.1-0.3** -- because the penalty for missing cancer far outweighs the cost of a false alarm.\n",
    "\n",
    "At the optimal threshold you will see:\n",
    "- **Recall near 1.0:** The model catches nearly every malignant case.\n",
    "- **Precision drops:** More false positives are tolerated.\n",
    "- **Total cost minimized:** The combined dollar cost across all validation samples is as low as possible.\n",
    "\n",
    "The left plot shows the U-shaped cost curve -- cost is high when the threshold is too strict (many FN) and high again when it is too lenient (many FP), with a sweet spot in between. The right plot shows precision and recall as functions of threshold, illustrating the classic tradeoff: as threshold decreases, recall rises but precision falls.\n",
    "\n",
    "**Why this matters:** The default 0.5 threshold is arbitrary. Real-world deployment decisions should be driven by the *business cost structure*, not by a mathematical convenience. This cost-based threshold selection is a technique you will use in your course project.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANALYSIS:\n",
    "\n",
    "**Question 1: Why is the optimal threshold different from 0.5?**  \n",
    "[Your answer - think about asymmetric costs]\n",
    "\n",
    "**Question 2: What happens if you change the cost ratio?**  \n",
    "[Your answer - try FN cost = $100,000]\n",
    "\n",
    "**Question 3: In production, how would you monitor this?**  \n",
    "[Your answer - what could go wrong over time?]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ PAUSE-AND-DO Exercise 2 (5 minutes)\n",
    "\n",
    "**Task:** Explain why accuracy fails under imbalance (with evidence).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Gemini Prompt:** \"Create a synthetic imbalanced dataset with make_classification: 1000 samples, 20 features, 95%/5% class split. Split 70/30 with stratification, fit a logistic regression pipeline, and print the classification_report. Compare against a naive all-zeros baseline to show that high accuracy can be misleading with class imbalance.\"\n",
    ">\n",
    "> **After running, verify:**\n",
    "> - Class distribution confirms ~95% class 0 and ~5% class 1\n",
    "> - classification_report shows per-class metrics (class 1 recall may be low)\n",
    "> - Naive baseline accuracy is ~95% despite predicting all zeros (zero recall for class 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create imbalanced scenario\n",
    "# Simulate: 95% benign, 5% malignant\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X_imb, y_imb = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    weights=[0.95, 0.05],  # 95% class 0, 5% class 1\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "X_train_imb, X_val_imb, y_train_imb, y_val_imb = train_test_split(\n",
    "    X_imb, y_imb, test_size=0.3, random_state=RANDOM_SEED, stratify=y_imb\n",
    ")\n",
    "\n",
    "# Train model\n",
    "pipe_imb = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(random_state=RANDOM_SEED, max_iter=1000))\n",
    "])\n",
    "pipe_imb.fit(X_train_imb, y_train_imb)\n",
    "\n",
    "y_pred_imb = pipe_imb.predict(X_val_imb)\n",
    "\n",
    "# Compare metrics\n",
    "print(\"=== IMBALANCED DATASET (95% class 0, 5% class 1) ===\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(pd.Series(y_val_imb).value_counts(normalize=True))\n",
    "\n",
    "print(f\"\\n=== MODEL PERFORMANCE ===\")\n",
    "print(classification_report(y_val_imb, y_pred_imb, target_names=['Class 0 (95%)', 'Class 1 (5%)']))\n",
    "\n",
    "# Naive baseline: always predict majority class\n",
    "y_naive = np.zeros_like(y_val_imb)\n",
    "naive_acc = (y_naive == y_val_imb).mean()\n",
    "\n",
    "print(f\"\\n=== NAIVE BASELINE (always predict 0) ===\")\n",
    "print(f\"Accuracy: {naive_acc:.4f}\")\n",
    "print(f\"Recall for class 1: 0.0000 (missed all positives!)\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ Key Insight: {naive_acc*100:.1f}% accuracy by predicting everything as class 0!\")\n",
    "print(f\"ðŸ’¡ This is why accuracy alone is dangerous with imbalance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "This cell creates a synthetic dataset with **95% class 0 and 5% class 1** using `make_classification`, then trains a Logistic Regression and compares it against a naive baseline that *always predicts class 0*.\n",
    "\n",
    "Key findings:\n",
    "- The **naive baseline achieves ~95% accuracy** by always predicting the majority class -- despite being completely useless for identifying the minority class.\n",
    "- The `classification_report` for the trained model shows that while overall accuracy looks respectable, **recall for class 1 may be significantly lower** than for class 0, because the model has very few positive examples to learn from.\n",
    "- The naive baseline has **recall = 0.0** for class 1, meaning it misses every single positive case.\n",
    "\n",
    "This is the **accuracy paradox**: a metric that appears excellent on the surface but hides total failure on the class you actually care about.\n",
    "\n",
    "**Key takeaway:** When classes are imbalanced, always supplement accuracy with precision, recall, F1, and AUC. In extreme cases (fraud, rare disease), accuracy should be *ignored entirely* in favor of PR-AUC and recall.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR EXPLANATION:\n",
    "\n",
    "**Why accuracy is misleading:**  \n",
    "[Your explanation with evidence from above]\n",
    "\n",
    "**Better metrics for imbalance:**  \n",
    "[Which metrics would you use instead?]\n",
    "\n",
    "**Real-world example:**  \n",
    "[Give an example where this matters]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Metrics Dashboard\n",
    "\n",
    "Individual metrics tell part of the story; a **dashboard** tells the whole story at a glance. The function below wraps every metric we have discussed -- accuracy, precision, recall, F1, specificity, ROC-AUC, and PR-AUC -- into a single call that returns a dictionary you can log, compare across models, or embed in a report.\n",
    "\n",
    "Building a reusable dashboard function is a best practice you will carry into your course project. It guarantees that every model you evaluate is measured on the same set of metrics, making apples-to-apples comparison straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Gemini Prompt:** \"Define a function create_metrics_dashboard(y_true, y_pred, y_proba) that computes Accuracy, Precision, Recall, F1, Specificity, ROC-AUC, PR-AUC, and the four confusion matrix values (TP, FP, FN, TN), returning them as a dictionary. Call it on the validation set and print all metrics in a formatted dashboard.\"\n",
    ">\n",
    "> **After running, verify:**\n",
    "> - Function returns a dictionary with 11 keys (7 metrics + 4 confusion matrix counts)\n",
    "> - All performance metrics are printed in a clean formatted layout\n",
    "> - ROC-AUC and PR-AUC are included alongside threshold-dependent metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete metrics dashboard\n",
    "def create_metrics_dashboard(y_true, y_pred, y_proba):\n",
    "    \"\"\"Generate comprehensive classification metrics\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    \n",
    "    metrics = {\n",
    "        'Accuracy': (TP + TN) / (TP + TN + FP + FN),\n",
    "        'Precision': TP / (TP + FP) if (TP + FP) > 0 else 0,\n",
    "        'Recall': TP / (TP + FN) if (TP + FN) > 0 else 0,\n",
    "        'F1': f1_score(y_true, y_pred),\n",
    "        'Specificity': TN / (TN + FP) if (TN + FP) > 0 else 0,\n",
    "        'ROC_AUC': roc_auc_score(y_true, y_proba),\n",
    "        'PR_AUC': average_precision_score(y_true, y_proba),\n",
    "        'TP': TP,\n",
    "        'FP': FP,\n",
    "        'FN': FN,\n",
    "        'TN': TN\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "dashboard = create_metrics_dashboard(y_val, y_pred_val, y_proba_val)\n",
    "\n",
    "print(\"=== COMPREHENSIVE METRICS DASHBOARD ===\")\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"  Accuracy:    {dashboard['Accuracy']:.4f}\")\n",
    "print(f\"  Precision:   {dashboard['Precision']:.4f}\")\n",
    "print(f\"  Recall:      {dashboard['Recall']:.4f}\")\n",
    "print(f\"  F1 Score:    {dashboard['F1']:.4f}\")\n",
    "print(f\"  Specificity: {dashboard['Specificity']:.4f}\")\n",
    "print(f\"  ROC-AUC:     {dashboard['ROC_AUC']:.4f}\")\n",
    "print(f\"  PR-AUC:      {dashboard['PR_AUC']:.4f}\")\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  TP: {dashboard['TP']:4d}    FP: {dashboard['FP']:4d}\")\n",
    "print(f\"  FN: {dashboard['FN']:4d}    TN: {dashboard['TN']:4d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The dashboard function `create_metrics_dashboard` returns a dictionary with every metric in one place. The printed output is organized into two sections:\n",
    "\n",
    "1. **Performance Metrics:** Accuracy, Precision, Recall, F1, Specificity, ROC-AUC, and PR-AUC -- all computed on the original breast cancer validation set. Expect values in the **0.95-0.99** range for this well-separated dataset.\n",
    "2. **Confusion Matrix counts:** TP, FP, FN, TN as raw integers so you can verify any metric by hand.\n",
    "\n",
    "This function is designed to be *reusable*: you can call `create_metrics_dashboard(y_true, y_pred, y_proba)` for any model and get a standardized report. In your course project, you will call this function for each candidate model and collect the results into a comparison DataFrame.\n",
    "\n",
    "**Why this matters:** A metrics dashboard eliminates the risk of forgetting a metric or computing it inconsistently. It is the final step before presenting results to stakeholders, and it ensures that every model is evaluated on exactly the same terms.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Wrap-Up: Key Takeaways\n\n### What We Learned Today:\n\n1. **Confusion Matrix**: Foundation for understanding classification errors\n2. **Precision vs Recall**: Tradeoff between false positives and false negatives\n3. **ROC and PR Curves**: Visualize performance across thresholds\n4. **Cost-Based Thresholding**: Align decisions to business objectives\n5. **Imbalance Handling**: Accuracy is dangerous - use precision, recall, AUC\n\n### Critical Rules:\n\n> **\"Never trust accuracy alone\"**\n\n> **\"Choose thresholds based on business costs, not defaults\"**\n\n> **\"With imbalance, use PR curves over ROC curves\"**\n\n### Next Steps:\n\n- Next notebook: Cross-validation for robust model comparison\n- We'll use today's metrics with proper CV evaluation\n- Apply to your project dataset\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Participation Assignment Submission Instructions\n",
    "\n",
    "### To Submit This Notebook:\n",
    "\n",
    "1. **Complete all exercises**: Fill in both PAUSE-AND-DO exercise cells with your findings\n",
    "2. **Run All Cells**: Execute `Runtime â†’ Run all` to ensure everything works\n",
    "3. **Save a Copy**: `File â†’ Save a copy in Drive or Download the .ipynb extension`\n",
    "4. **Submit**: Upload your `.ipynb` file in the participation assignment you find in the course Brightspace page.\n",
    "\n",
    "### Before Submitting, Check:\n",
    "\n",
    "- [ ] All cells execute without errors\n",
    "- [ ] All outputs are visible\n",
    "- [ ] Both exercise responses are complete\n",
    "- [ ] Notebook is shared with correct permissions\n",
    "- [ ] You can explain every line of code you wrote\n",
    "\n",
    "### Next Step:\n",
    "\n",
    "Complete the **Quiz** in Brightspace (auto-graded)\n",
    "\n",
    "---\n",
    "\n",
    "## Bibliography\n\n- Fawcett, T. (2006). \"An introduction to ROC analysis.\" *Pattern Recognition Letters*, 27(8), 861-874.\n- Saito, T., & Rehmsmeier, M. (2015). \"The precision-recall plot is more informative than the ROC plot when evaluating binary classifiers on imbalanced datasets.\" *PLOS ONE*.\n- scikit-learn User Guide: [Classification metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics)\n- Provost, F., & Fawcett, T. (2013). *Data Science for Business* - Chapter on evaluation and costs\n\n---\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "Thank you!\n",
    "\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
