{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - Probabilities, Decision Boundaries, and Pipelines\n",
    "\n",
    "<hr>\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/notebooks/figures/mgmt_474_ai_logo_02-modified.png\" width=\"200\"/>\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "# <center><a class=\"tocSkip\"></center>\n",
    "# <center>MGMT47400 Predictive Analytics</center>\n",
    "# <center>Professor: Davi Moreira </center>\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/blob/main/notebooks/06_logistic_pipelines.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Fit logistic regression with preprocessing in a pipeline\n",
    "2. Interpret probabilities vs classes (and why thresholds matter)\n",
    "3. Use regularization in logistic regression for stability\n",
    "4. Choose an appropriate baseline for classification\n",
    "5. Document the classification objective and error costs\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import make_classification, load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.metrics import accuracy_score, log_loss, confusion_matrix, classification_report\nimport warnings\n\nwarnings.filterwarnings('ignore')\npd.set_option('display.precision', 4)\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (10, 6)\nRANDOM_SEED = 474\nnp.random.seed(RANDOM_SEED)\nprint(\"\u2713 Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The `Setup complete!` message confirms that all imports loaded. This notebook introduces several classification-specific tools: `LogisticRegression` for the model, `DummyClassifier` for baselines, `accuracy_score` and `log_loss` for evaluation, and `confusion_matrix` plus `classification_report` for detailed error analysis. We also import `load_breast_cancer` and `make_classification` from `sklearn.datasets`. The usual display settings and **RANDOM_SEED = 474** remain in effect.\n",
    "\n",
    "**Why this matters:** Classification problems require a different toolkit than regression. Metrics like accuracy, log loss, and confusion matrices replace MAE, RMSE, and R\u00b2. Recognizing which tools belong to which problem type is a foundational skill.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Classification Dataset\n",
    "\n",
    "We switch from regression to classification using the **Breast Cancer Wisconsin** dataset, a classic binary-classification benchmark included in scikit-learn. Each of the **569 samples** represents a digitized image of a fine-needle aspirate of a breast mass, described by **30 numeric features** (mean, standard error, and worst of 10 measurements like radius, texture, and symmetry). The target is **0 = malignant** or **1 = benign**.\n",
    "Splits are stratified to preserve the original class balance in every partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load breast cancer dataset (binary classification)\n",
    "data = load_breast_cancer(as_frame=True)\n",
    "df = data.frame\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "print(f\"Dataset: {data.DESCR.split('===')[0].strip()}\")\n",
    "print(f\"\\nShape: {X.shape}\")\n",
    "print(f\"Target classes: {data.target_names}\")\n",
    "print(f\"Class distribution:\")\n",
    "print(y.value_counts())\n",
    "print(f\"\\nClass balance: {y.value_counts(normalize=True).round(3).to_dict()}\")\n",
    "\n",
    "# Split data\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.20, random_state=RANDOM_SEED, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=RANDOM_SEED, stratify=y_temp)\n",
    "\n",
    "print(f\"\\nTrain: {len(X_train)} | Val: {len(X_val)} | Test: {len(X_test)} (locked)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The output shows the dataset has **569 samples** and **30 features**. The target classes are `malignant` (0) and `benign` (1), with a class distribution of roughly **212 malignant** and **357 benign** samples -- approximately a **37%/63%** split. This mild imbalance is important: a model that always predicts \"benign\" would already achieve ~63% accuracy, so raw accuracy alone is not a reliable performance indicator. The stratified splits produce approximately **341 training**, **114 validation**, and **114 test** samples, each preserving the 37/63 class ratio.\n",
    "\n",
    "**Key takeaway:** Always inspect class balance before modeling. When one class dominates, accuracy inflates and you need additional metrics (precision, recall, confusion matrix) to assess real performance.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification Baselines\n",
    "\n",
    "### Why Baselines Matter for Classification\n",
    "\n",
    "**Common baselines:**\n",
    "- **Most frequent class**: Always predict the majority class\n",
    "- **Stratified random**: Predict classes proportional to training distribution\n",
    "- **Domain heuristic**: Simple rule based on domain knowledge\n",
    "\n",
    "**Key insight**: With imbalanced classes, even naive baselines can have high accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most frequent class baseline\n",
    "baseline_mf = DummyClassifier(strategy='most_frequent')\n",
    "baseline_mf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_baseline = baseline_mf.predict(X_val)\n",
    "baseline_acc = accuracy_score(y_val, y_pred_baseline)\n",
    "\n",
    "print(\"=== BASELINE: MOST FREQUENT CLASS ===\")\n",
    "print(f\"Validation Accuracy: {baseline_acc:.4f}\")\n",
    "print(f\"\\nThis baseline always predicts: {data.target_names[int(baseline_mf.predict([X_train.iloc[0]])[0])]}\")\n",
    "print(f\"\\n\u26a0\ufe0f Accuracy can be misleading! We need better metrics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The `DummyClassifier(strategy='most_frequent')` always predicts the majority class (benign). Its validation accuracy is approximately **0.63**, which is simply the proportion of benign samples in the validation set. The warning below the score emphasizes that this number can be misleadingly high: the model has learned *nothing* about the data and would miss every single malignant case.\n",
    "\n",
    "**Why this matters:** This baseline accuracy sets the floor. Any real classifier must beat **~63%** to demonstrate that it has learned something useful. More importantly, we need metrics beyond accuracy to ensure the model actually detects malignant tumors, not just defaults to the majority class.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Logistic Regression: From Log-Odds to Probabilities\n",
    "\n",
    "### The Math (Simplified)\n",
    "\n",
    "**Linear combination:**\n",
    "```\n",
    "z = \u03b2\u2080 + \u03b2\u2081x\u2081 + \u03b2\u2082x\u2082 + ... + \u03b2\u209ax\u209a\n",
    "```\n",
    "\n",
    "**Logistic function (sigmoid):**\n",
    "```\n",
    "P(y=1|X) = 1 / (1 + e^(-z))\n",
    "```\n",
    "\n",
    "**Properties:**\n",
    "- Output is always between 0 and 1 (valid probability)\n",
    "- z = 0 \u2192 P = 0.5 (decision boundary)\n",
    "- Large positive z \u2192 P \u2248 1\n",
    "- Large negative z \u2192 P \u2248 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sigmoid function\n",
    "z = np.linspace(-10, 10, 200)\n",
    "sigmoid = 1 / (1 + np.exp(-z))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(z, sigmoid, linewidth=2)\n",
    "plt.axhline(y=0.5, color='r', linestyle='--', label='Default threshold (0.5)')\n",
    "plt.axvline(x=0, color='g', linestyle='--', alpha=0.5, label='Decision boundary (z=0)')\n",
    "plt.xlabel('Linear Combination (z)')\n",
    "plt.ylabel('Probability P(y=1|X)')\n",
    "plt.title('Logistic (Sigmoid) Function')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"\ud83d\udca1 The sigmoid squashes any real number into [0, 1]\")\n",
    "print(\"\ud83d\udca1 Default: if P > 0.5, predict class 1; else predict class 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The plot shows the classic **S-shaped sigmoid curve** mapping any real-valued linear combination z to a probability between 0 and 1. The red dashed line at **P = 0.5** marks the default decision threshold, and the green dashed line at **z = 0** marks the corresponding input value. When z is large and positive, the sigmoid saturates near 1 (high confidence in class 1); when z is large and negative, it saturates near 0 (high confidence in class 0). The transition region around z = 0 is where the model is most uncertain.\n",
    "\n",
    "**Key takeaway:** The sigmoid function is what makes logistic regression a *probability* model rather than just a classifier. Understanding this curve helps you reason about confidence: a prediction of P = 0.99 is qualitatively different from P = 0.51, even though both produce the same class label.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Logistic Regression Pipeline\n",
    "\n",
    "We now build a proper scikit-learn `Pipeline` that chains `StandardScaler` (zero-mean, unit-variance normalization) with `LogisticRegression`. Scaling is especially important for logistic regression because the model's convergence and regularization behavior depend on feature magnitudes.\n",
    "The pipeline reports both **accuracy** (fraction of correct predictions) and **log loss** (a probability-quality metric that penalizes confident wrong predictions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic logistic regression pipeline\n",
    "log_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(random_state=RANDOM_SEED, max_iter=1000))\n",
    "])\n",
    "\n",
    "log_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = log_pipeline.predict(X_train)\n",
    "y_pred_val = log_pipeline.predict(X_val)\n",
    "\n",
    "# Probabilities\n",
    "y_proba_train = log_pipeline.predict_proba(X_train)\n",
    "y_proba_val = log_pipeline.predict_proba(X_val)\n",
    "\n",
    "print(\"=== LOGISTIC REGRESSION ===\")\n",
    "print(f\"Train Accuracy: {accuracy_score(y_train, y_pred_train):.4f}\")\n",
    "print(f\"Val Accuracy: {accuracy_score(y_val, y_pred_val):.4f}\")\n",
    "print(f\"\\nTrain Log Loss: {log_loss(y_train, y_proba_train):.4f}\")\n",
    "print(f\"Val Log Loss: {log_loss(y_val, y_proba_val):.4f}\")\n",
    "\n",
    "print(f\"\\n\u2713 Improvement over baseline: {accuracy_score(y_val, y_pred_val) - baseline_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The logistic regression pipeline reports **Train Accuracy** and **Val Accuracy**, both likely above **0.96**, representing a massive jump from the ~63% baseline. **Log loss** is also printed for both sets; lower is better, and values around **0.08--0.12** indicate that the model's probability estimates are well-calibrated. The \"Improvement over baseline\" line quantifies the accuracy gain -- typically **+0.33** or more. The small gap between train and validation metrics suggests minimal overfitting.\n",
    "\n",
    "**Why this matters:** Accuracy alone does not tell you *where* the model makes mistakes. Two models can have 97% accuracy but differ dramatically in which errors they make (false positives vs. false negatives). Log loss rewards well-calibrated probabilities, making it a more informative optimization target than raw accuracy.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd PAUSE-AND-DO Exercise 1 (5 minutes)\n",
    "\n",
    "**Task:** Build logistic pipeline and compute validation accuracy + log loss.\n",
    "\n",
    "The pipeline is already built above. Now:\n",
    "1. Look at the probabilities for a few samples\n",
    "2. Understand the difference between `.predict()` and `.predict_proba()`\n",
    "3. Explain why log loss might be better than accuracy\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine predictions vs probabilities\n",
    "sample_df = pd.DataFrame({\n",
    "    'True_Label': y_val.iloc[:10].values,\n",
    "    'Predicted_Class': y_pred_val[:10],\n",
    "    'Prob_Class_0': y_proba_val[:10, 0],\n",
    "    'Prob_Class_1': y_proba_val[:10, 1],\n",
    "    'Correct': y_val.iloc[:10].values == y_pred_val[:10]\n",
    "})\n",
    "\n",
    "print(\"=== SAMPLE PREDICTIONS ===\")\n",
    "print(sample_df)\n",
    "\n",
    "print(\"\\n\ud83d\udca1 Notice:\")\n",
    "print(\"  - Probabilities sum to 1.0 for each sample\")\n",
    "print(\"  - Predicted class = argmax(probabilities)\")\n",
    "print(\"  - Some predictions are confident (prob close to 0 or 1)\")\n",
    "print(\"  - Some predictions are uncertain (prob close to 0.5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The 10-row table shows, for each validation sample: the **True_Label**, the **Predicted_Class**, the probability assigned to **class 0** (malignant) and **class 1** (benign), and whether the prediction was **Correct**. Notice that the two probability columns always sum to **1.0** for each row. Some predictions are highly confident (e.g., Prob_Class_1 > 0.99), while others are closer to 0.5, indicating uncertainty. The `Predicted_Class` column is simply the argmax of the two probabilities at the default 0.5 threshold.\n",
    "\n",
    "**Key takeaway:** Probabilities carry more information than hard labels. A prediction of P(malignant) = 0.48 (just below the 0.5 threshold) is treated the same as P(malignant) = 0.01 in terms of the predicted class, but a clinician would want to know about the 0.48 case. This is why examining `predict_proba()` output is essential.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANALYSIS:\n",
    "\n",
    "**Question 1: What's the difference between `.predict()` and `.predict_proba()`?**  \n",
    "[Your answer]\n",
    "\n",
    "**Question 2: Why might log loss be better than accuracy?**  \n",
    "[Your answer - hint: think about probability quality]\n",
    "\n",
    "**Question 3: What does a probability of 0.51 vs 0.99 tell you?**  \n",
    "[Your answer - both predict class 1, but...]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Thresholding Matters!\n",
    "\n",
    "Logistic regression outputs a probability for each class, but the final prediction depends on a **threshold**: if P(class 1) >= threshold, predict class 1. The default is 0.5, but this is not always optimal. Lowering the threshold makes the model more eager to predict the positive class (higher recall, more false positives); raising it makes the model more conservative (higher precision, more false negatives).\n",
    "The sweep below tests four thresholds to illustrate how this single number reshapes the entire prediction profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different thresholds\nthresholds = [0.3, 0.5, 0.7, 0.9]\nthreshold_results = []\n\nfor thresh in thresholds:\n    y_pred_thresh = (y_proba_val[:, 1] >= thresh).astype(int)\n    acc = accuracy_score(y_val, y_pred_thresh)\n    cm = confusion_matrix(y_val, y_pred_thresh)\n    \n    threshold_results.append({\n        'Threshold': thresh,\n        'Accuracy': acc,\n        'Predicted_Positive': y_pred_thresh.sum(),\n        'Predicted_Negative': len(y_pred_thresh) - y_pred_thresh.sum()\n    })\n\nresults_df = pd.DataFrame(threshold_results)\nprint(\"=== THRESHOLD SENSITIVITY ===\")\nprint(results_df)\n\nprint(\"\\n\ud83d\udca1 Key insight: Changing the threshold changes predictions!\")\nprint(\"\ud83d\udca1 Default 0.5 is not always optimal\")\nprint(\"\ud83d\udca1 We'll explore this more in upcoming notebooks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The threshold sensitivity table shows four rows corresponding to thresholds **0.3, 0.5, 0.7, and 0.9**. As the threshold increases, the model becomes more conservative about predicting the positive class (benign): the **Predicted_Positive** count drops and **Predicted_Negative** rises. Accuracy may actually decrease at extreme thresholds because the model starts misclassifying clear benign cases. At a very low threshold (0.3), nearly everything is predicted positive, boosting recall for the benign class but potentially missing malignant cases.\n",
    "\n",
    "**Why this matters:** In medical diagnosis, the *cost* of each error type drives threshold choice. If missing a malignant tumor is catastrophic (false negative), you would lower the threshold to catch more positives, accepting more false alarms. If unnecessary biopsies are costly (false positive), you would raise the threshold. There is no universally correct threshold -- it depends on the business or clinical context.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd PAUSE-AND-DO Exercise 2 (5 minutes)\n",
    "\n",
    "**Task:** Change threshold from 0.5 and observe metric shifts.\n",
    "\n",
    "Already done above. Now answer:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR OBSERVATIONS:\n",
    "\n",
    "**Observation 1: What happens when you lower the threshold?**  \n",
    "[Hint: more/fewer positive predictions?]\n",
    "\n",
    "**Observation 2: What happens when you raise the threshold?**  \n",
    "[Hint: how does it affect prediction distribution?]\n",
    "\n",
    "**Observation 3: When might you want a threshold other than 0.5?**  \n",
    "[Think about business costs]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Regularized Logistic Regression\n",
    "\n",
    "Scikit-learn's `LogisticRegression` applies L2 regularization by default, controlled by the inverse-regularization parameter **C** (lower C = stronger penalty). This is the classification analogue of Ridge regression.\n",
    "We sweep C across five orders of magnitude (0.01 to 100) to observe how regularization strength affects training accuracy, validation accuracy, and the gap between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularized logistic regression (lower C = stronger regularization)\n",
    "log_reg_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(C=1.0, random_state=RANDOM_SEED, max_iter=1000))\n",
    "])\n",
    "\n",
    "log_reg_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Compare different C values\n",
    "C_values = [0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "reg_results = []\n",
    "\n",
    "for C in C_values:\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', LogisticRegression(C=C, random_state=RANDOM_SEED, max_iter=1000))\n",
    "    ])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    \n",
    "    train_acc = pipe.score(X_train, y_train)\n",
    "    val_acc = pipe.score(X_val, y_val)\n",
    "    \n",
    "    reg_results.append({\n",
    "        'C': C,\n",
    "        'Train_Acc': train_acc,\n",
    "        'Val_Acc': val_acc,\n",
    "        'Gap': train_acc - val_acc\n",
    "    })\n",
    "\n",
    "reg_df = pd.DataFrame(reg_results)\n",
    "print(\"=== REGULARIZATION STRENGTH (C parameter) ===\")\n",
    "print(reg_df)\n",
    "print(\"\\n\ud83d\udca1 Lower C = stronger regularization\")\n",
    "print(\"\ud83d\udca1 Look for good validation performance without huge train-val gap\")\n",
    "\n",
    "best_C = reg_df.loc[reg_df['Val_Acc'].idxmax(), 'C']\n",
    "print(f\"\\n\u2713 Best validation accuracy at C = {best_C}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The table shows five rows for **C = 0.01, 0.1, 1.0, 10.0, and 100.0**. At very low C (strong regularization), both train and validation accuracy may dip slightly because the model is overly constrained. As C increases (weaker regularization), accuracy improves and then plateaus. The **Gap** column (train minus validation accuracy) should remain small across all C values, indicating that logistic regression is not highly prone to overfitting on this 30-feature dataset. The \"Best validation accuracy at C = ...\" line identifies the sweet spot.\n",
    "\n",
    "**Key takeaway:** The C parameter in logistic regression is the inverse of alpha in Ridge/Lasso: **small C = strong penalty**. In practice, you would use cross-validation (e.g., `LogisticRegressionCV`) to select C automatically, just as we used `RidgeCV` and `LassoCV` in the previous notebook.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Confusion Matrix\n",
    "\n",
    "A confusion matrix tabulates all four outcomes of a binary classifier: true positives, true negatives, false positives, and false negatives. In a medical context like breast cancer diagnosis, the cost of a **false negative** (missing a malignant tumor) is far higher than a **false positive** (an unnecessary biopsy).\n",
    "The heatmap below makes these counts immediately visible, and the printed interpretation maps each cell to its clinical meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_val, y_pred_val)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=data.target_names)\n",
    "disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "plt.title('Confusion Matrix - Validation Set')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== CONFUSION MATRIX INTERPRETATION ===\")\n",
    "print(f\"True Negatives (TN): {cm[0, 0]}\")\n",
    "print(f\"False Positives (FP): {cm[0, 1]}\")\n",
    "print(f\"False Negatives (FN): {cm[1, 0]}\")\n",
    "print(f\"True Positives (TP): {cm[1, 1]}\")\n",
    "\n",
    "print(\"\\n\ud83d\udca1 In medical diagnosis:\")\n",
    "print(\"   FP = False alarm (predicted malignant, actually benign)\")\n",
    "print(\"   FN = Missed diagnosis (predicted benign, actually malignant)\")\n",
    "print(\"\\n\u26a0\ufe0f Which error is more costly? This drives threshold choice!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The confusion matrix heatmap is a 2x2 grid with true labels on the y-axis and predicted labels on the x-axis. The diagonal cells (top-left and bottom-right) show correct predictions: **true negatives** (correctly identified malignant) and **true positives** (correctly identified benign). The off-diagonal cells show errors: **false positives** (top-right, predicted benign but actually malignant) and **false negatives** (bottom-left, predicted malignant but actually benign). The printed counts below the chart give exact numbers for each cell. In this medical context, false negatives (missed malignant cases) are the most dangerous error.\n",
    "\n",
    "**Why this matters:** The confusion matrix is the foundation for precision, recall, F1-score, and all other classification metrics you will encounter in the next notebook. Learning to read it fluently -- and to identify which cell represents the most costly error for your specific problem -- is a core skill in applied machine learning.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Wrap-Up: Key Takeaways\n\n### What We Learned Today:\n\n1. **Logistic Regression**: Maps linear combinations to probabilities via sigmoid\n2. **Probabilities vs Classes**: `.predict_proba()` gives you more information than `.predict()`\n3. **Thresholds Matter**: Default 0.5 is not always optimal\n4. **Baselines**: Even naive strategies can have decent accuracy with imbalance\n5. **Regularization**: Control C to prevent overfitting\n\n### Critical Rules:\n\n> **\"Always look at probabilities, not just classes\"**\n\n> **\"Accuracy is not enough - confusion matrix reveals errors\"**\n\n> **\"Thresholds should be tuned to business costs\"**\n\n### Next Steps:\n\n- Next notebook: Classification metrics (precision, recall, ROC, PR curves)\n- We'll learn how to systematically choose thresholds\n- Class imbalance handling strategies\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Participation Assignment Submission Instructions\n",
    "\n",
    "### To Submit This Notebook:\n",
    "\n",
    "1. **Complete all exercises**: Fill in both PAUSE-AND-DO exercise cells with your findings\n",
    "2. **Run All Cells**: Execute `Runtime \u2192 Run all` to ensure everything works\n",
    "3. **Save a Copy**: `File \u2192 Save a copy in Drive`\n",
    "4. **Submit**: Upload your `.ipynb` file in the participation assignment you find in the course Brightspace page.\n",
    "\n",
    "### Before Submitting, Check:\n",
    "\n",
    "- [ ] All cells execute without errors\n",
    "- [ ] All outputs are visible\n",
    "- [ ] Both exercise responses are complete\n",
    "- [ ] Notebook is shared with correct permissions\n",
    "- [ ] You can explain every line of code you wrote\n",
    "\n",
    "### Next Step:\n",
    "\n",
    "Complete the **Quiz** in Brightspace (auto-graded)\n",
    "\n",
    "---\n",
    "\n",
    "## Bibliography\n\n- James, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). *An Introduction to Statistical Learning with Python* - Classification chapter\n- Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning* - Logistic regression foundations\n- scikit-learn User Guide: [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n- scikit-learn User Guide: [Probability calibration](https://scikit-learn.org/stable/modules/calibration.html)\n\n---\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "Thank you!\n",
    "\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}