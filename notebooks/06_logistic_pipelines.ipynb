{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - Probabilities, Decision Boundaries, and Pipelines\n",
    "\n",
    "<hr>\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/notebooks/figures/mgmt_474_ai_logo_02-modified.png\" width=\"200\"/>\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "# <center><a class=\"tocSkip\"></center>\n",
    "# <center>MGMT47400 Predictive Analytics</center>\n",
    "# <center>Professor: Davi Moreira </center>\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/blob/main/notebooks/06_logistic_pipelines.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Fit logistic regression with preprocessing in a pipeline\n",
    "2. Interpret probabilities vs classes (and why thresholds matter)\n",
    "3. Use regularization in logistic regression for stability\n",
    "4. Choose an appropriate baseline for classification\n",
    "5. Document the classification objective and error costs\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import make_classification, load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.metrics import accuracy_score, log_loss, confusion_matrix, classification_report\nimport warnings\n\nwarnings.filterwarnings('ignore')\npd.set_option('display.precision', 4)\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (10, 6)\nRANDOM_SEED = 474\nnp.random.seed(RANDOM_SEED)\nprint(\"\u2713 Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Classification Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load breast cancer dataset (binary classification)\n",
    "data = load_breast_cancer(as_frame=True)\n",
    "df = data.frame\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "print(f\"Dataset: {data.DESCR.split('===')[0].strip()}\")\n",
    "print(f\"\\nShape: {X.shape}\")\n",
    "print(f\"Target classes: {data.target_names}\")\n",
    "print(f\"Class distribution:\")\n",
    "print(y.value_counts())\n",
    "print(f\"\\nClass balance: {y.value_counts(normalize=True).round(3).to_dict()}\")\n",
    "\n",
    "# Split data\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.20, random_state=RANDOM_SEED, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=RANDOM_SEED, stratify=y_temp)\n",
    "\n",
    "print(f\"\\nTrain: {len(X_train)} | Val: {len(X_val)} | Test: {len(X_test)} (locked)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification Baselines\n",
    "\n",
    "### Why Baselines Matter for Classification\n",
    "\n",
    "**Common baselines:**\n",
    "- **Most frequent class**: Always predict the majority class\n",
    "- **Stratified random**: Predict classes proportional to training distribution\n",
    "- **Domain heuristic**: Simple rule based on domain knowledge\n",
    "\n",
    "**Key insight**: With imbalanced classes, even naive baselines can have high accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most frequent class baseline\n",
    "baseline_mf = DummyClassifier(strategy='most_frequent')\n",
    "baseline_mf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_baseline = baseline_mf.predict(X_val)\n",
    "baseline_acc = accuracy_score(y_val, y_pred_baseline)\n",
    "\n",
    "print(\"=== BASELINE: MOST FREQUENT CLASS ===\")\n",
    "print(f\"Validation Accuracy: {baseline_acc:.4f}\")\n",
    "print(f\"\\nThis baseline always predicts: {data.target_names[int(baseline_mf.predict([X_train.iloc[0]])[0])]}\")\n",
    "print(f\"\\n\u26a0\ufe0f Accuracy can be misleading! We need better metrics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Logistic Regression: From Log-Odds to Probabilities\n",
    "\n",
    "### The Math (Simplified)\n",
    "\n",
    "**Linear combination:**\n",
    "```\n",
    "z = \u03b2\u2080 + \u03b2\u2081x\u2081 + \u03b2\u2082x\u2082 + ... + \u03b2\u209ax\u209a\n",
    "```\n",
    "\n",
    "**Logistic function (sigmoid):**\n",
    "```\n",
    "P(y=1|X) = 1 / (1 + e^(-z))\n",
    "```\n",
    "\n",
    "**Properties:**\n",
    "- Output is always between 0 and 1 (valid probability)\n",
    "- z = 0 \u2192 P = 0.5 (decision boundary)\n",
    "- Large positive z \u2192 P \u2248 1\n",
    "- Large negative z \u2192 P \u2248 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sigmoid function\n",
    "z = np.linspace(-10, 10, 200)\n",
    "sigmoid = 1 / (1 + np.exp(-z))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(z, sigmoid, linewidth=2)\n",
    "plt.axhline(y=0.5, color='r', linestyle='--', label='Default threshold (0.5)')\n",
    "plt.axvline(x=0, color='g', linestyle='--', alpha=0.5, label='Decision boundary (z=0)')\n",
    "plt.xlabel('Linear Combination (z)')\n",
    "plt.ylabel('Probability P(y=1|X)')\n",
    "plt.title('Logistic (Sigmoid) Function')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"\ud83d\udca1 The sigmoid squashes any real number into [0, 1]\")\n",
    "print(\"\ud83d\udca1 Default: if P > 0.5, predict class 1; else predict class 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Logistic Regression Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic logistic regression pipeline\n",
    "log_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(random_state=RANDOM_SEED, max_iter=1000))\n",
    "])\n",
    "\n",
    "log_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = log_pipeline.predict(X_train)\n",
    "y_pred_val = log_pipeline.predict(X_val)\n",
    "\n",
    "# Probabilities\n",
    "y_proba_train = log_pipeline.predict_proba(X_train)\n",
    "y_proba_val = log_pipeline.predict_proba(X_val)\n",
    "\n",
    "print(\"=== LOGISTIC REGRESSION ===\")\n",
    "print(f\"Train Accuracy: {accuracy_score(y_train, y_pred_train):.4f}\")\n",
    "print(f\"Val Accuracy: {accuracy_score(y_val, y_pred_val):.4f}\")\n",
    "print(f\"\\nTrain Log Loss: {log_loss(y_train, y_proba_train):.4f}\")\n",
    "print(f\"Val Log Loss: {log_loss(y_val, y_proba_val):.4f}\")\n",
    "\n",
    "print(f\"\\n\u2713 Improvement over baseline: {accuracy_score(y_val, y_pred_val) - baseline_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd PAUSE-AND-DO Exercise 1 (10 minutes)\n",
    "\n",
    "**Task:** Build logistic pipeline and compute validation accuracy + log loss.\n",
    "\n",
    "The pipeline is already built above. Now:\n",
    "1. Look at the probabilities for a few samples\n",
    "2. Understand the difference between `.predict()` and `.predict_proba()`\n",
    "3. Explain why log loss might be better than accuracy\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine predictions vs probabilities\n",
    "sample_df = pd.DataFrame({\n",
    "    'True_Label': y_val.iloc[:10].values,\n",
    "    'Predicted_Class': y_pred_val[:10],\n",
    "    'Prob_Class_0': y_proba_val[:10, 0],\n",
    "    'Prob_Class_1': y_proba_val[:10, 1],\n",
    "    'Correct': y_val.iloc[:10].values == y_pred_val[:10]\n",
    "})\n",
    "\n",
    "print(\"=== SAMPLE PREDICTIONS ===\")\n",
    "print(sample_df)\n",
    "\n",
    "print(\"\\n\ud83d\udca1 Notice:\")\n",
    "print(\"  - Probabilities sum to 1.0 for each sample\")\n",
    "print(\"  - Predicted class = argmax(probabilities)\")\n",
    "print(\"  - Some predictions are confident (prob close to 0 or 1)\")\n",
    "print(\"  - Some predictions are uncertain (prob close to 0.5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANALYSIS:\n",
    "\n",
    "**Question 1: What's the difference between `.predict()` and `.predict_proba()`?**  \n",
    "[Your answer]\n",
    "\n",
    "**Question 2: Why might log loss be better than accuracy?**  \n",
    "[Your answer - hint: think about probability quality]\n",
    "\n",
    "**Question 3: What does a probability of 0.51 vs 0.99 tell you?**  \n",
    "[Your answer - both predict class 1, but...]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Thresholding Matters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different thresholds\nthresholds = [0.3, 0.5, 0.7, 0.9]\nthreshold_results = []\n\nfor thresh in thresholds:\n    y_pred_thresh = (y_proba_val[:, 1] >= thresh).astype(int)\n    acc = accuracy_score(y_val, y_pred_thresh)\n    cm = confusion_matrix(y_val, y_pred_thresh)\n    \n    threshold_results.append({\n        'Threshold': thresh,\n        'Accuracy': acc,\n        'Predicted_Positive': y_pred_thresh.sum(),\n        'Predicted_Negative': len(y_pred_thresh) - y_pred_thresh.sum()\n    })\n\nresults_df = pd.DataFrame(threshold_results)\nprint(\"=== THRESHOLD SENSITIVITY ===\")\nprint(results_df)\n\nprint(\"\\n\ud83d\udca1 Key insight: Changing the threshold changes predictions!\")\nprint(\"\ud83d\udca1 Default 0.5 is not always optimal\")\nprint(\"\ud83d\udca1 We'll explore this more in upcoming notebooks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd PAUSE-AND-DO Exercise 2 (10 minutes)\n",
    "\n",
    "**Task:** Change threshold from 0.5 and observe metric shifts.\n",
    "\n",
    "Already done above. Now answer:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR OBSERVATIONS:\n",
    "\n",
    "**Observation 1: What happens when you lower the threshold?**  \n",
    "[Hint: more/fewer positive predictions?]\n",
    "\n",
    "**Observation 2: What happens when you raise the threshold?**  \n",
    "[Hint: how does it affect prediction distribution?]\n",
    "\n",
    "**Observation 3: When might you want a threshold other than 0.5?**  \n",
    "[Think about business costs]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularized logistic regression (lower C = stronger regularization)\n",
    "log_reg_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(C=1.0, random_state=RANDOM_SEED, max_iter=1000))\n",
    "])\n",
    "\n",
    "log_reg_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Compare different C values\n",
    "C_values = [0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "reg_results = []\n",
    "\n",
    "for C in C_values:\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', LogisticRegression(C=C, random_state=RANDOM_SEED, max_iter=1000))\n",
    "    ])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    \n",
    "    train_acc = pipe.score(X_train, y_train)\n",
    "    val_acc = pipe.score(X_val, y_val)\n",
    "    \n",
    "    reg_results.append({\n",
    "        'C': C,\n",
    "        'Train_Acc': train_acc,\n",
    "        'Val_Acc': val_acc,\n",
    "        'Gap': train_acc - val_acc\n",
    "    })\n",
    "\n",
    "reg_df = pd.DataFrame(reg_results)\n",
    "print(\"=== REGULARIZATION STRENGTH (C parameter) ===\")\n",
    "print(reg_df)\n",
    "print(\"\\n\ud83d\udca1 Lower C = stronger regularization\")\n",
    "print(\"\ud83d\udca1 Look for good validation performance without huge train-val gap\")\n",
    "\n",
    "best_C = reg_df.loc[reg_df['Val_Acc'].idxmax(), 'C']\n",
    "print(f\"\\n\u2713 Best validation accuracy at C = {best_C}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_val, y_pred_val)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=data.target_names)\n",
    "disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "plt.title('Confusion Matrix - Validation Set')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== CONFUSION MATRIX INTERPRETATION ===\")\n",
    "print(f\"True Negatives (TN): {cm[0, 0]}\")\n",
    "print(f\"False Positives (FP): {cm[0, 1]}\")\n",
    "print(f\"False Negatives (FN): {cm[1, 0]}\")\n",
    "print(f\"True Positives (TP): {cm[1, 1]}\")\n",
    "\n",
    "print(\"\\n\ud83d\udca1 In medical diagnosis:\")\n",
    "print(\"   FP = False alarm (predicted malignant, actually benign)\")\n",
    "print(\"   FN = Missed diagnosis (predicted benign, actually malignant)\")\n",
    "print(\"\\n\u26a0\ufe0f Which error is more costly? This drives threshold choice!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Wrap-Up: Key Takeaways\n\n### What We Learned Today:\n\n1. **Logistic Regression**: Maps linear combinations to probabilities via sigmoid\n2. **Probabilities vs Classes**: `.predict_proba()` gives you more information than `.predict()`\n3. **Thresholds Matter**: Default 0.5 is not always optimal\n4. **Baselines**: Even naive strategies can have decent accuracy with imbalance\n5. **Regularization**: Control C to prevent overfitting\n\n### Critical Rules:\n\n> **\"Always look at probabilities, not just classes\"**\n\n> **\"Accuracy is not enough - confusion matrix reveals errors\"**\n\n> **\"Thresholds should be tuned to business costs\"**\n\n### Next Steps:\n\n- Next notebook: Classification metrics (precision, recall, ROC, PR curves)\n- We'll learn how to systematically choose thresholds\n- Class imbalance handling strategies\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n\n- James, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). *An Introduction to Statistical Learning with Python* - Classification chapter\n- Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning* - Logistic regression foundations\n- scikit-learn User Guide: [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n- scikit-learn User Guide: [Probability calibration](https://scikit-learn.org/stable/modules/calibration.html)\n\n---\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "Thank you!\n",
    "\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}