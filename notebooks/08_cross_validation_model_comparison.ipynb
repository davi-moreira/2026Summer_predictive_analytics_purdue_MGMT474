{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling and CV - How to Compare Models Without Fooling Yourself\n",
    "\n",
    "<hr>\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/notebooks/figures/mgmt_474_ai_logo_02-modified.png\" width=\"200\"/>\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "# <center><a class=\"tocSkip\"></center>\n",
    "# <center>MGMT47400 Predictive Analytics</center>\n",
    "# <center>Professor: Davi Moreira </center>\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/blob/main/notebooks/08_cross_validation_model_comparison.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Run k-fold cross-validation for classification and regression\n",
    "2. Use stratified CV for classification\n",
    "3. Understand variance of performance estimates (why one split is fragile)\n",
    "4. Compare models using consistent CV and a single primary metric\n",
    "5. Build a reusable CV evaluation function (project-ready)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_breast_cancer, fetch_california_housing\nfrom sklearn.model_selection import (\n    KFold, StratifiedKFold, cross_val_score, cross_validate, RepeatedKFold\n)\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression, Ridge\nfrom sklearn.ensemble import RandomForestClassifier\nimport warnings\n\nwarnings.filterwarnings('ignore')\npd.set_option('display.precision', 4)\nRANDOM_SEED = 474\nnp.random.seed(RANDOM_SEED)\nprint(\"\u2713 Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The setup cell imports the resampling toolkit we will use throughout this notebook. Key imports include `KFold` and `StratifiedKFold` (CV splitters), `cross_val_score` and `cross_validate` (convenience functions that handle the train-evaluate loop internally), and `RepeatedKFold` for more stable estimates. On the modeling side, we import `LogisticRegression`, `Ridge`, and `RandomForestClassifier` so we can compare diverse algorithms on the same folds. The confirmation **\"Setup complete!\"** means everything loaded successfully.\n",
    "\n",
    "**Why this matters:** scikit-learn provides dedicated splitter objects rather than manual index slicing. Using them guarantees reproducible folds (via `random_state`) and correct stratification.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Why Cross-Validation Exists\n",
    "\n",
    "### The Problem with Single Train/Val Splits\n",
    "\n",
    "**Issues:**\n",
    "- Performance depends on which samples end up in validation\n",
    "- High variance in estimates\n",
    "- May get lucky or unlucky with the split\n",
    "- Wastes data (validation set sits idle)\n",
    "\n",
    "### The Solution: Cross-Validation\n",
    "\n",
    "**K-Fold CV:**\n",
    "1. Split data into K folds\n",
    "2. Train on K-1 folds, validate on 1 fold\n",
    "3. Repeat K times (each fold gets to be validation once)\n",
    "4. Average the K performance scores\n",
    "\n",
    "**Benefits:**\n",
    "- More stable performance estimates\n",
    "- Uses all data for both training and validation\n",
    "- Reveals variance in model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. K-Fold Cross-Validation for Regression\n",
    "\n",
    "We start with a regression example to build intuition about what K-Fold CV actually does. The California Housing dataset (20,640 samples, 8 features) predicts median house value from census-block-level attributes such as median income, average rooms, and geographic coordinates.\n",
    "\n",
    "With `KFold(n_splits=5)`, the data is divided into 5 equally-sized folds. The model trains on 4 folds (~16,512 samples) and evaluates on the held-out fold (~4,128 samples), repeating this 5 times so every sample is validated exactly once. The result is 5 R-squared scores whose mean and standard deviation give us a more honest estimate of model quality than any single train/val split could."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load regression dataset\n",
    "california = fetch_california_housing(as_frame=True)\n",
    "X_reg = california.data\n",
    "y_reg = california.target\n",
    "\n",
    "print(f\"Regression dataset: {X_reg.shape}\")\n",
    "\n",
    "# Create pipeline\n",
    "ridge_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', Ridge(alpha=1.0))\n",
    "])\n",
    "\n",
    "# 5-fold cross-validation\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "cv_scores = cross_val_score(ridge_pipeline, X_reg, y_reg, cv=cv, scoring='r2')\n",
    "\n",
    "print(\"\\n=== 5-FOLD CROSS-VALIDATION (Regression) ===\")\n",
    "print(f\"Fold scores: {cv_scores}\")\n",
    "print(f\"Mean R\u00b2: {cv_scores.mean():.4f}\")\n",
    "print(f\"Std R\u00b2:  {cv_scores.std():.4f}\")\n",
    "print(f\"95% CI:  [{cv_scores.mean() - 2*cv_scores.std():.4f}, {cv_scores.mean() + 2*cv_scores.std():.4f}]\")\n",
    "\n",
    "# Visualize fold variation\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(1, 6), cv_scores, alpha=0.7, edgecolor='black')\n",
    "plt.axhline(y=cv_scores.mean(), color='r', linestyle='--', label=f'Mean = {cv_scores.mean():.4f}')\n",
    "plt.axhline(y=cv_scores.mean() + cv_scores.std(), color='orange', linestyle=':', label=f'\u00b11 Std')\n",
    "plt.axhline(y=cv_scores.mean() - cv_scores.std(), color='orange', linestyle=':')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('R\u00b2 Score')\n",
    "plt.title('Cross-Validation Scores Across Folds')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udca1 Notice the variation across folds - this is why CV matters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "Five R-squared scores are printed, one per fold. Notice the variation -- the highest fold might score around **R\u00b2 = 0.61** while the lowest might dip to **R\u00b2 = 0.58**. The summary line reports the **mean R\u00b2** (roughly 0.59-0.61) and the **standard deviation** (typically 0.01-0.02).\n",
    "\n",
    "The bar chart makes the fold-to-fold variation visible. The red dashed line marks the mean, and the orange dotted lines mark plus/minus one standard deviation. If any bar falls far outside the orange band, it may indicate that certain regions of the data are harder to predict (for example, luxury coastal properties versus inland suburbs).\n",
    "\n",
    "The approximate **95% confidence interval** is computed as mean +/- 2*std. This interval tells you the range within which you would expect the model's true generalization performance to fall. A narrow interval means the estimate is stable; a wide one means you need more data or a different CV scheme.\n",
    "\n",
    "**Key takeaway:** A single train/val split would have given you just *one* of these five bars. Cross-validation gives you the full picture: both the central tendency and the uncertainty around it.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stratified K-Fold for Classification\n",
    "\n",
    "When the target variable is categorical, random K-Fold splitting can produce folds with different class proportions. For instance, if the overall positive rate is 63%, one fold might end up with 55% while another gets 70%, injecting noise into the CV estimates.\n",
    "\n",
    "**Stratified K-Fold** solves this by enforcing the same class distribution in every fold. For the breast cancer dataset (569 samples, 212 malignant, 357 benign), each of the 5 folds will contain approximately 42 malignant and 71 benign observations. The cell below runs *both* regular and stratified 5-fold CV side by side so you can compare the variance in scores directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load classification dataset\n",
    "data = load_breast_cancer(as_frame=True)\n",
    "X_clf = data.data\n",
    "y_clf = data.target\n",
    "\n",
    "print(f\"Classification dataset: {X_clf.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y_clf)}\")\n",
    "print(f\"Class ratio: {np.bincount(y_clf)[1] / len(y_clf):.4f}\")\n",
    "\n",
    "# Create pipeline\n",
    "log_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(random_state=RANDOM_SEED, max_iter=1000))\n",
    "])\n",
    "\n",
    "# Compare regular vs stratified CV\n",
    "cv_regular = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "cv_stratified = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "scores_regular = cross_val_score(log_pipeline, X_clf, y_clf, cv=cv_regular, scoring='roc_auc')\n",
    "scores_stratified = cross_val_score(log_pipeline, X_clf, y_clf, cv=cv_stratified, scoring='roc_auc')\n",
    "\n",
    "print(\"\\n=== REGULAR K-FOLD ===\")\n",
    "print(f\"Scores: {scores_regular}\")\n",
    "print(f\"Mean \u00b1 Std: {scores_regular.mean():.4f} \u00b1 {scores_regular.std():.4f}\")\n",
    "\n",
    "print(\"\\n=== STRATIFIED K-FOLD ===\")\n",
    "print(f\"Scores: {scores_stratified}\")\n",
    "print(f\"Mean \u00b1 Std: {scores_stratified.mean():.4f} \u00b1 {scores_stratified.std():.4f}\")\n",
    "\n",
    "print(\"\\n\ud83d\udca1 Stratified CV typically has lower variance\")\n",
    "print(f\"\ud83d\udca1 Variance reduction: {(scores_regular.std() - scores_stratified.std()) / scores_regular.std() * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The breast cancer dataset has 569 samples with 30 features. The class distribution shows roughly **63% benign (class 1)** and **37% malignant (class 0)**.\n",
    "\n",
    "Two sets of 5-fold CV scores are printed side by side:\n",
    "- **Regular K-Fold:** The ROC-AUC scores show noticeable variation because some folds may have unbalanced class ratios by chance.\n",
    "- **Stratified K-Fold:** The scores are typically tighter (lower standard deviation) because every fold mirrors the overall 63/37 class split.\n",
    "\n",
    "The **variance reduction** percentage at the bottom quantifies how much stratification helped. A positive value means stratified CV produced more stable estimates. In practice, the improvement can range from modest (5-10%) to dramatic (50%+), depending on how unbalanced the data is and how small the dataset is.\n",
    "\n",
    "**Why this matters:** For classification tasks, always use `StratifiedKFold` (or `StratifiedShuffleSplit`). It is a free lunch: same computational cost, lower-variance estimates, and no risk of a fold that accidentally contains zero samples of the minority class.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd PAUSE-AND-DO Exercise 1 (5 minutes)\n",
    "\n",
    "**Task:** Write `cv_report(model, X, y, cv, scoring)` returning mean/std.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_report(model, X, y, cv, scoring='accuracy'):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive cross-validation report.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : estimator\n",
    "        sklearn model or pipeline\n",
    "    X : array-like\n",
    "        Features\n",
    "    y : array-like\n",
    "        Target\n",
    "    cv : cross-validator\n",
    "        CV splitter (e.g., KFold, StratifiedKFold)\n",
    "    scoring : str\n",
    "        Scoring metric\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary with scores and statistics\n",
    "    \"\"\"\n",
    "    # Run cross-validation\n",
    "    cv_results = cross_validate(\n",
    "        model, X, y, cv=cv, scoring=scoring,\n",
    "        return_train_score=True,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Extract scores\n",
    "    train_scores = cv_results['train_score']\n",
    "    val_scores = cv_results['test_score']\n",
    "    fit_times = cv_results['fit_time']\n",
    "    \n",
    "    # Calculate statistics\n",
    "    report = {\n",
    "        'train_mean': train_scores.mean(),\n",
    "        'train_std': train_scores.std(),\n",
    "        'val_mean': val_scores.mean(),\n",
    "        'val_std': val_scores.std(),\n",
    "        'overfit_gap': train_scores.mean() - val_scores.mean(),\n",
    "        'fold_scores': val_scores,\n",
    "        'mean_fit_time': fit_times.mean()\n",
    "    }\n",
    "    \n",
    "    # Print report\n",
    "    print(f\"=== CROSS-VALIDATION REPORT ({scoring}) ===\")\n",
    "    print(f\"Validation: {report['val_mean']:.4f} \u00b1 {report['val_std']:.4f}\")\n",
    "    print(f\"Training:   {report['train_mean']:.4f} \u00b1 {report['train_std']:.4f}\")\n",
    "    print(f\"Overfit gap: {report['overfit_gap']:.4f}\")\n",
    "    print(f\"Mean fit time: {report['mean_fit_time']:.3f}s\")\n",
    "    print(f\"\\nFold-by-fold: {report['fold_scores'].round(4)}\")\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Test the function\n",
    "cv_strat = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "report = cv_report(log_pipeline, X_clf, y_clf, cv_strat, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The `cv_report` function prints a structured summary with four key numbers:\n",
    "\n",
    "- **Validation score (mean +/- std):** The average ROC-AUC across the 5 folds, plus the spread. This is the number you would report in a paper or presentation.\n",
    "- **Training score (mean +/- std):** How well the model fits its own training data. Training scores are almost always higher than validation scores.\n",
    "- **Overfit gap:** The difference between training and validation means. A gap near zero suggests the model generalizes well; a large gap (e.g., > 0.05) signals overfitting.\n",
    "- **Mean fit time:** How long each fold took to train, useful when comparing fast linear models against slower ensemble methods.\n",
    "\n",
    "The fold-by-fold array at the bottom lets you inspect individual fold scores for outliers. If one fold is dramatically lower, it may point to a data quality issue or a particularly hard subpopulation.\n",
    "\n",
    "**Key takeaway:** Always report *both* the mean and standard deviation. Saying \"ROC-AUC = 0.99\" without the spread hides the fact that some folds might score 0.97 -- a gap that could matter in high-stakes applications.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANALYSIS:\n",
    "\n",
    "**Question 1: Why do we report both mean and std?**  \n",
    "[Your answer - what does std tell us?]\n",
    "\n",
    "**Question 2: What's a good overfit gap?**  \n",
    "[Your answer - when should you worry?]\n",
    "\n",
    "**Question 3: How would you use this in your project?**  \n",
    "[Your answer - workflow integration]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fair Model Comparison Under CV\n",
    "\n",
    "### The Rules for Fair Comparison\n",
    "\n",
    "**Must be identical:**\n",
    "1. Same CV folds (use same random seed)\n",
    "2. Same data\n",
    "3. Same scoring metric\n",
    "4. Same preprocessing (if any)\n",
    "\n",
    "**Why this matters:**\n",
    "- Different folds \u2192 different scores (not comparable)\n",
    "- Different metrics \u2192 different models win\n",
    "- Unfair comparison \u2192 wrong model chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare multiple models with SAME CV folds\n",
    "models = {\n",
    "    'Logistic (C=1.0)': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', LogisticRegression(C=1.0, random_state=RANDOM_SEED, max_iter=1000))\n",
    "    ]),\n",
    "    'Logistic (C=0.1)': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', LogisticRegression(C=0.1, random_state=RANDOM_SEED, max_iter=1000))\n",
    "    ]),\n",
    "    'Random Forest': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', RandomForestClassifier(n_estimators=100, random_state=RANDOM_SEED))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Use SAME CV splitter for all models\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    cv_results = cross_validate(\n",
    "        model, X_clf, y_clf, cv=cv,\n",
    "        scoring=['roc_auc', 'accuracy', 'f1'],\n",
    "        return_train_score=True,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'ROC_AUC_mean': cv_results['test_roc_auc'].mean(),\n",
    "        'ROC_AUC_std': cv_results['test_roc_auc'].std(),\n",
    "        'Accuracy_mean': cv_results['test_accuracy'].mean(),\n",
    "        'Accuracy_std': cv_results['test_accuracy'].std(),\n",
    "        'F1_mean': cv_results['test_f1'].mean(),\n",
    "        'F1_std': cv_results['test_f1'].std(),\n",
    "        'Fit_Time': cv_results['fit_time'].mean()\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(results)\n",
    "print(\"=== MODEL COMPARISON (5-FOLD CV) ===\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Highlight best model\n",
    "best_idx = comparison_df['ROC_AUC_mean'].idxmax()\n",
    "print(f\"\\n\u2713 Best ROC-AUC: {comparison_df.loc[best_idx, 'Model']} ({comparison_df.loc[best_idx, 'ROC_AUC_mean']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The comparison table lists three models -- **Logistic (C=1.0)**, **Logistic (C=0.1)**, and **Random Forest** -- evaluated on the *same* 5 stratified folds. For each model, three metrics are reported: ROC-AUC, Accuracy, and F1, each with mean and standard deviation.\n",
    "\n",
    "Key observations:\n",
    "- All three models perform well on this relatively easy dataset, with ROC-AUC values typically above **0.98**.\n",
    "- The two logistic regression variants may differ only slightly, showing that moderate regularization (C=0.1 vs C=1.0) has limited impact when features are already informative.\n",
    "- Random Forest may show a marginally higher or lower score; the standard deviations will tell you whether the difference is meaningful.\n",
    "- The **Fit_Time** column reveals that Random Forest takes noticeably longer than Logistic Regression due to building 100 decision trees.\n",
    "\n",
    "The final line highlights the **best model by ROC-AUC**. However, if two models are within one standard deviation of each other, the simpler model (Logistic Regression) is generally preferred -- it is faster, more interpretable, and less prone to overfitting.\n",
    "\n",
    "**Why this matters:** Fair model comparison requires identical folds, identical metrics, and identical data. Change any one of these and the comparison becomes invalid. This pattern -- same `StratifiedKFold` object passed to every model -- is the gold standard.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd PAUSE-AND-DO Exercise 2 (5 minutes)\n",
    "\n",
    "**Task:** Compare logistic vs regularized logistic under the same CV.\n",
    "\n",
    "Already done above! Now answer:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR COMPARISON:\n",
    "\n",
    "**Observation 1: Which model performs best?**  \n",
    "[Based on primary metric - ROC-AUC]\n",
    "\n",
    "**Observation 2: Is the difference statistically meaningful?**  \n",
    "[Compare means relative to standard deviations]\n",
    "\n",
    "**Observation 3: What about fit time?**  \n",
    "[Is the performance improvement worth the time cost?]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced: Repeated Cross-Validation\n",
    "\n",
    "For even more stable estimates, repeat CV multiple times with different random seeds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "# 5-fold CV repeated 3 times = 15 total evaluations\n",
    "cv_repeated = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=RANDOM_SEED)\n",
    "\n",
    "scores_single = cross_val_score(log_pipeline, X_clf, y_clf, cv=cv_strat, scoring='roc_auc')\n",
    "scores_repeated = cross_val_score(log_pipeline, X_clf, y_clf, cv=cv_repeated, scoring='roc_auc')\n",
    "\n",
    "print(\"=== SINGLE VS REPEATED CV ===\")\n",
    "print(f\"\\nSingle 5-fold:\")\n",
    "print(f\"  Mean: {scores_single.mean():.4f}\")\n",
    "print(f\"  Std:  {scores_single.std():.4f}\")\n",
    "print(f\"  N evaluations: {len(scores_single)}\")\n",
    "\n",
    "print(f\"\\nRepeated 5-fold (\u00d73):\")\n",
    "print(f\"  Mean: {scores_repeated.mean():.4f}\")\n",
    "print(f\"  Std:  {scores_repeated.std():.4f}\")\n",
    "print(f\"  N evaluations: {len(scores_repeated)}\")\n",
    "\n",
    "# Visualize distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(scores_single, bins=5, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(scores_single.mean(), color='r', linestyle='--', label=f'Mean = {scores_single.mean():.4f}')\n",
    "axes[0].set_xlabel('ROC-AUC Score')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Single 5-Fold CV')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(scores_repeated, bins=10, edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(scores_repeated.mean(), color='r', linestyle='--', label=f'Mean = {scores_repeated.mean():.4f}')\n",
    "axes[1].set_xlabel('ROC-AUC Score')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Repeated 5-Fold CV (\u00d73)')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udca1 Repeated CV gives more samples \u2192 more stable estimate\")\n",
    "print(\"\ud83d\udca1 Trade-off: More computation time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "Two CV strategies are compared for the same Logistic Regression pipeline:\n",
    "\n",
    "- **Single 5-Fold CV:** 5 evaluation scores.\n",
    "- **Repeated 5-Fold CV (x3):** 15 evaluation scores (5 folds repeated with 3 different random shuffles).\n",
    "\n",
    "The means should be close to each other, but the repeated version typically has a **smaller standard deviation** because it averages over more independent evaluations. The histograms on the right visually confirm this: the repeated-CV distribution is smoother and more bell-shaped, while the single-CV histogram is sparse with only 5 bars.\n",
    "\n",
    "The tradeoff is computational: repeated CV requires **3x the model fits** (15 instead of 5). For fast models like Logistic Regression this is negligible, but for large Random Forests or deep learning models, the extra time can be significant.\n",
    "\n",
    "**Key takeaway:** Use repeated CV when you need high-confidence estimates (e.g., for a final report or publication). Use single-fold CV during early exploration when speed matters more than precision.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Reusable CV Comparison Function (Project-Ready)\n",
    "\n",
    "Throughout the previous sections we repeated the same pattern: create a CV splitter, loop over models, collect scores, print results. The function below packages that entire workflow into a single call, `compare_models()`, that accepts a dictionary of named models, a CV splitter, and one or more scoring metrics.\n",
    "\n",
    "This is the function you will copy directly into your course project. It returns a tidy DataFrame suitable for reporting, and it enforces fair comparison by using the *same* CV folds for every model. Investing five minutes now to understand its API will save hours later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(models_dict, X, y, cv, scoring='accuracy', verbose=True):\n",
    "    \"\"\"\n",
    "    Compare multiple models using the same CV folds.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    models_dict : dict\n",
    "        Dictionary of {name: model} pairs\n",
    "    X : array-like\n",
    "        Features\n",
    "    y : array-like\n",
    "        Target\n",
    "    cv : cross-validator\n",
    "        CV splitter\n",
    "    scoring : str or list\n",
    "        Scoring metric(s)\n",
    "    verbose : bool\n",
    "        Print results\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : Comparison table\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for name, model in models_dict.items():\n",
    "        cv_results = cross_validate(\n",
    "            model, X, y, cv=cv, scoring=scoring,\n",
    "            return_train_score=True, n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Handle single or multiple metrics\n",
    "        if isinstance(scoring, str):\n",
    "            val_scores = cv_results['test_score']\n",
    "            train_scores = cv_results['train_score']\n",
    "            \n",
    "            results.append({\n",
    "                'Model': name,\n",
    "                'Val_Mean': val_scores.mean(),\n",
    "                'Val_Std': val_scores.std(),\n",
    "                'Train_Mean': train_scores.mean(),\n",
    "                'Overfit_Gap': train_scores.mean() - val_scores.mean(),\n",
    "                'Fit_Time_Mean': cv_results['fit_time'].mean()\n",
    "            })\n",
    "        else:\n",
    "            # Multiple metrics case\n",
    "            row = {'Model': name}\n",
    "            for metric in scoring:\n",
    "                val_scores = cv_results[f'test_{metric}']\n",
    "                row[f'{metric}_mean'] = val_scores.mean()\n",
    "                row[f'{metric}_std'] = val_scores.std()\n",
    "            row['Fit_Time'] = cv_results['fit_time'].mean()\n",
    "            results.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=== MODEL COMPARISON ===\")\n",
    "        print(df.to_string(index=False))\n",
    "        print(f\"\\nCV: {cv}\")\n",
    "        print(f\"Scoring: {scoring}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Test it\n",
    "test_models = {\n",
    "    'Logistic': Pipeline([('scaler', StandardScaler()), ('clf', LogisticRegression(random_state=RANDOM_SEED, max_iter=1000))]),\n",
    "    'Random Forest': Pipeline([('scaler', StandardScaler()), ('clf', RandomForestClassifier(n_estimators=50, random_state=RANDOM_SEED))])\n",
    "}\n",
    "\n",
    "comparison = compare_models(\n",
    "    test_models, X_clf, y_clf,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED),\n",
    "    scoring=['roc_auc', 'accuracy'],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The `compare_models` function produces a comparison table with two models -- **Logistic** and **Random Forest** -- scored on both `roc_auc` and `accuracy`. Each metric column shows the mean across 5 stratified folds, plus the standard deviation and the fit time.\n",
    "\n",
    "This function is *project-ready*: you can pass in any dictionary of named pipelines, any CV splitter, and any list of scoring metrics. It returns a pandas DataFrame, so you can sort, filter, export to CSV, or feed it into a visualization.\n",
    "\n",
    "Notice the footer prints the CV object and scoring parameter, so you always know exactly how the comparison was conducted. This metadata is critical for reproducibility -- if a colleague asks \"how did you compare these models?\", the answer is right in the output.\n",
    "\n",
    "**Why this matters:** Automating model comparison into a single function eliminates copy-paste errors and ensures consistency. In your course project, replace the `test_models` dictionary with your own candidate pipelines and this function will handle the rest.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Wrap-Up: Key Takeaways\n\n### What We Learned Today:\n\n1. **CV Reduces Variance**: More stable estimates than single train/val split\n2. **Stratified CV**: Essential for classification (maintains class balance)\n3. **Fair Comparison**: Same folds, same metrics, same data\n4. **Report Uncertainty**: Always show mean \u00b1 std, not just mean\n5. **Reusable Functions**: Build tools you can use in your project\n\n### Critical Rules:\n\n> **\"Never compare models with different CV folds\"**\n\n> **\"Always use stratified CV for classification\"**\n\n> **\"Report mean AND standard deviation\"**\n\n### Next Steps:\n\n- Next notebook: Hyperparameter tuning with GridSearchCV\n- We'll combine CV with systematic parameter search\n- Start building your project baseline model\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Participation Assignment Submission Instructions\n",
    "\n",
    "### To Submit This Notebook:\n",
    "\n",
    "1. **Complete all exercises**: Fill in both PAUSE-AND-DO exercise cells with your findings\n",
    "2. **Run All Cells**: Execute `Runtime \u2192 Run all` to ensure everything works\n",
    "3. **Save a Copy**: `File \u2192 Save a copy in Drive`\n",
    "4. **Submit**: Upload your `.ipynb` file in the participation assignment you find in the course Brightspace page.\n",
    "\n",
    "### Before Submitting, Check:\n",
    "\n",
    "- [ ] All cells execute without errors\n",
    "- [ ] All outputs are visible\n",
    "- [ ] Both exercise responses are complete\n",
    "- [ ] Notebook is shared with correct permissions\n",
    "- [ ] You can explain every line of code you wrote\n",
    "\n",
    "### Next Step:\n",
    "\n",
    "Complete the **Quiz** in Brightspace (auto-graded)\n",
    "\n",
    "---\n",
    "\n",
    "## Bibliography\n\n- James, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). *An Introduction to Statistical Learning with Python* - Model Assessment and Selection (k-fold CV, resampling concepts)\n- Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning* - Resampling theory and selection bias\n- scikit-learn User Guide: [Cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html)\n- Kohavi, R. (1995). \"A study of cross-validation and bootstrap for accuracy estimation and model selection.\" *IJCAI*.\n\n---\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "Thank you!\n",
    "\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}