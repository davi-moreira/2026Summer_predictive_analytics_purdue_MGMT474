{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering + Model Selection Workflow (Project Baseline Build)\n",
    "\n",
    "<hr>\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/notebooks/figures/mgmt_474_ai_logo_02-modified.png\" width=\"200\"/>\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "# <center><a class=\"tocSkip\"></center>\n",
    "# <center>MGMT47400 Predictive Analytics</center>\n",
    "# <center>Professor: Davi Moreira </center>\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/blob/main/notebooks/09_tuning_feature_engineering_project_baseline.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Engineer features with pipelines without leakage\n",
    "2. Use `GridSearchCV` / `RandomizedSearchCV` for systematic tuning\n",
    "3. Define a project-grade evaluation plan (metric + split/CV + baseline + reporting)\n",
    "4. Produce a baseline model notebook that can be extended\n",
    "5. Use Gemini to draft search grids and then simplify them\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import (\n    train_test_split, StratifiedKFold,\n    GridSearchCV, RandomizedSearchCV\n)\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom scipy.stats import uniform, randint\nimport warnings\n\nwarnings.filterwarnings('ignore')\npd.set_option('display.precision', 4)\nRANDOM_SEED = 474\nnp.random.seed(RANDOM_SEED)\nprint(\"\u2713 Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The setup cell imports the tuning machinery we will use: `GridSearchCV` for exhaustive parameter search, `RandomizedSearchCV` for sampling-based search, and `StratifiedKFold` for the inner CV loop that evaluates each parameter combination. We also import `PolynomialFeatures` and `SelectKBest` for feature engineering, and `scipy.stats.uniform` and `randint` for defining continuous and integer parameter distributions in randomized search. The **\"Setup complete!\"** message confirms all libraries loaded without error.\n",
    "\n",
    "**Why this matters:** Grid search and randomized search are the two workhorses of hyperparameter tuning in scikit-learn. Understanding when to use each -- and how they interact with pipelines and CV -- is essential for building strong models without overfitting to the validation set.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Baseline\n",
    "\n",
    "Every tuning experiment needs a **baseline** -- a simple model with default hyperparameters that establishes the performance floor. Without a baseline, you cannot tell whether elaborate feature engineering or exhaustive grid searches actually improved anything.\n",
    "\n",
    "We continue with the breast cancer Wisconsin dataset (569 samples, 30 features) and the standard 60/20/20 split. The baseline is a Logistic Regression pipeline with `StandardScaler` and all default hyperparameters (`C=1.0`, `penalty='l2'`). Its validation accuracy will be our reference point for every experiment in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = load_breast_cancer(as_frame=True)\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.20, random_state=RANDOM_SEED, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=RANDOM_SEED, stratify=y_temp)\n",
    "\n",
    "print(f\"Train: {len(X_train)} | Val: {len(X_val)} | Test: {len(X_test)}\")\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "\n",
    "# Simple baseline\n",
    "baseline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', LogisticRegression(random_state=RANDOM_SEED, max_iter=1000))\n",
    "])\n",
    "\n",
    "baseline.fit(X_train, y_train)\n",
    "baseline_score = baseline.score(X_val, y_val)\n",
    "\n",
    "print(f\"\\nBaseline validation accuracy: {baseline_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The split sizes are printed as **Train: 341 | Val: 114 | Test: 114**, matching the 60/20/20 convention. The dataset has **30 features**, all numeric measurements of cell nuclei.\n",
    "\n",
    "The baseline Logistic Regression achieves a **validation accuracy around 0.96-0.97**. This is already high because the breast cancer dataset is relatively well-separated, but there is still room for improvement -- especially if we can identify which features matter most or find non-linear relationships.\n",
    "\n",
    "**Key takeaway:** Record this baseline number. Every subsequent experiment (feature selection, polynomial features, grid search) must be compared against it. If a complex model does not beat the baseline, the extra complexity is not justified.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering Inside Pipelines\n",
    "\n",
    "### Safe Feature Engineering Patterns\n",
    "\n",
    "**Rule:** All feature engineering must happen INSIDE the pipeline\n",
    "\n",
    "**Why?**\n",
    "- Prevents leakage (fit on train, transform on val/test)\n",
    "- Ensures reproducibility\n",
    "- Makes deployment easier\n",
    "\n",
    "**Common feature engineering steps:**\n",
    "1. Polynomial features\n",
    "2. Feature interactions\n",
    "3. Feature selection\n",
    "4. Domain-specific transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline with feature engineering\n",
    "fe_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('feature_selection', SelectKBest(f_classif, k=20)),  # Keep top 20 features\n",
    "    ('clf', LogisticRegression(random_state=RANDOM_SEED, max_iter=1000))\n",
    "])\n",
    "\n",
    "fe_pipeline.fit(X_train, y_train)\n",
    "fe_score = fe_pipeline.score(X_val, y_val)\n",
    "\n",
    "print(\"=== FEATURE ENGINEERING PIPELINE ===\")\n",
    "print(f\"Original features: {X.shape[1]}\")\n",
    "print(f\"Selected features: 20\")\n",
    "print(f\"Validation accuracy: {fe_score:.4f}\")\n",
    "print(f\"Improvement: {(fe_score - baseline_score):.4f}\")\n",
    "\n",
    "# See which features were selected\n",
    "selected_mask = fe_pipeline.named_steps['feature_selection'].get_support()\n",
    "selected_features = X.columns[selected_mask].tolist()\n",
    "print(f\"\\nSelected features: {selected_features[:5]}... (showing first 5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The feature engineering pipeline uses `SelectKBest(f_classif, k=20)` to keep only the 20 features with the highest ANOVA F-statistic, discarding the 10 least informative ones. The output compares:\n",
    "\n",
    "- **Original features:** 30\n",
    "- **Selected features:** 20\n",
    "- **Validation accuracy:** typically close to or slightly above the baseline\n",
    "- **Improvement:** the difference from baseline (may be small, positive, or even negative)\n",
    "\n",
    "The first five selected feature names are printed to give you a sense of which measurements survived the filter. Features like `mean concave points`, `worst radius`, and `worst perimeter` tend to rank highly because they correlate strongly with the malignant/benign distinction.\n",
    "\n",
    "**Why this matters:** Feature selection inside a pipeline prevents data leakage. The `SelectKBest` step is fitted *only* on training data, so the feature rankings do not peek at validation labels. If you performed selection outside the pipeline, you would leak information and get overly optimistic scores.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd PAUSE-AND-DO Exercise 1 (10 minutes)\n",
    "\n",
    "**Task:** Add engineered features and re-run CV.\n",
    "\n",
    "Try adding polynomial features (degree=2) to a subset of features.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE: Add polynomial features\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Warning: Polynomial features can explode the feature space!\n",
    "# Let's use only a subset\n",
    "poly_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('feature_selection_pre', SelectKBest(f_classif, k=10)),  # Reduce to 10 first\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('scaler2', StandardScaler()),  # Re-scale after polynomial\n",
    "    ('clf', LogisticRegression(random_state=RANDOM_SEED, max_iter=2000))\n",
    "])\n",
    "\n",
    "# Evaluate with CV\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "poly_scores = cross_val_score(poly_pipeline, X_train, y_train, cv=cv, scoring='roc_auc')\n",
    "baseline_scores = cross_val_score(baseline, X_train, y_train, cv=cv, scoring='roc_auc')\n",
    "\n",
    "print(\"=== POLYNOMIAL FEATURES COMPARISON ===\")\n",
    "print(f\"Baseline:   {baseline_scores.mean():.4f} \u00b1 {baseline_scores.std():.4f}\")\n",
    "print(f\"Polynomial: {poly_scores.mean():.4f} \u00b1 {poly_scores.std():.4f}\")\n",
    "print(f\"\\nImprovement: {(poly_scores.mean() - baseline_scores.mean()):.4f}\")\n",
    "\n",
    "# Check feature explosion\n",
    "poly_pipeline.fit(X_train.iloc[:10], y_train.iloc[:10])  # Fit on small sample to check\n",
    "n_poly_features = poly_pipeline.named_steps['poly'].n_output_features_\n",
    "print(f\"\\n\u26a0\ufe0f Feature explosion: 10 \u2192 {n_poly_features} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "This cell adds `PolynomialFeatures(degree=2)` *after* reducing to 10 features via `SelectKBest`. The polynomial step generates all pairwise interactions and squared terms, expanding 10 features to **65 features** (10 originals + 10 squares + 45 interactions).\n",
    "\n",
    "The comparison shows two lines of CV scores (5-fold Stratified, scored by ROC-AUC):\n",
    "- **Baseline:** mean +/- std without polynomial features.\n",
    "- **Polynomial:** mean +/- std with the expanded feature set.\n",
    "\n",
    "The improvement line shows the difference. On this dataset, polynomial features may provide a marginal boost or may actually hurt slightly, because Logistic Regression can already capture the linear decision boundary well. The warning about **feature explosion (10 to 65)** reminds you that degree-2 polynomials grow quadratically -- with all 30 original features, you would get 496 columns, dramatically increasing overfitting risk and training time.\n",
    "\n",
    "**Key takeaway:** Polynomial features are a double-edged sword. They can capture non-linear relationships, but they multiply the feature space and require more training data. Always compare against the baseline with CV, and use `SelectKBest` *before* the polynomial step to keep dimensionality manageable.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANALYSIS:\n",
    "\n",
    "**Did polynomial features help?**  \n",
    "[Your analysis]\n",
    "\n",
    "**What's the cost?**  \n",
    "[Feature explosion, complexity, overfitting risk]\n",
    "\n",
    "**Would you use this in production?**  \n",
    "[Justify your decision]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GridSearchCV - Systematic Hyperparameter Tuning\n",
    "\n",
    "### How GridSearchCV Works\n",
    "\n",
    "1. Define parameter grid\n",
    "2. Try every combination\n",
    "3. Use CV to evaluate each\n",
    "4. Return best parameters\n",
    "\n",
    "**Warning:** Grid search can be expensive!\n",
    "- 3 parameters \u00d7 3 values each = 27 combinations\n",
    "- 27 combinations \u00d7 5 folds = 135 model fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', LogisticRegression(random_state=RANDOM_SEED, max_iter=1000))\n",
    "])\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'clf__C': [0.01, 0.1, 1.0, 10.0],\n",
    "    'clf__penalty': ['l2'],  # Just L2 for speed\n",
    "    'clf__solver': ['lbfgs', 'liblinear']\n",
    "}\n",
    "\n",
    "print(\"=== GRID SEARCH CONFIGURATION ===\")\n",
    "print(f\"Parameter grid: {param_grid}\")\n",
    "n_combinations = len(param_grid['clf__C']) * len(param_grid['clf__penalty']) * len(param_grid['clf__solver'])\n",
    "print(f\"Total combinations: {n_combinations}\")\n",
    "print(f\"With 5-fold CV: {n_combinations * 5} model fits\")\n",
    "\n",
    "# Run grid search\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED),\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=0,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(\"\\nRunning grid search...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n=== GRID SEARCH RESULTS ===\")\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
    "print(f\"Validation score: {grid_search.score(X_val, y_val):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The grid search configuration is printed first: 4 values of `C` (0.01, 0.1, 1.0, 10.0) times 1 penalty times 2 solvers = **8 combinations**, each evaluated across 5 folds for a total of **40 model fits**.\n",
    "\n",
    "After fitting, three key results appear:\n",
    "- **Best parameters:** the combination that achieved the highest mean CV ROC-AUC. Expect `C=1.0` or `C=0.1` to win, since the breast cancer data does not require heavy regularization.\n",
    "- **Best CV score:** the mean ROC-AUC of the winning combination, typically around **0.99**.\n",
    "- **Validation score:** performance on the held-out validation set (which the grid search never saw). This should be close to the CV score; a large gap would indicate that the search overfit to the CV folds.\n",
    "\n",
    "**Why this matters:** `GridSearchCV` wraps the entire search-and-evaluate loop into a single estimator. After calling `.fit()`, the object automatically refits the best parameters on the full training set, so `grid_search.predict()` uses the optimal model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine all results\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "results_summary = results_df[[\n",
    "    'param_clf__C', 'param_clf__solver',\n",
    "    'mean_test_score', 'std_test_score',\n",
    "    'mean_train_score', 'rank_test_score'\n",
    "]].sort_values('rank_test_score')\n",
    "\n",
    "print(\"\\n=== TOP 5 PARAMETER COMBINATIONS ===\")\n",
    "print(results_summary.head().to_string(index=False))\n",
    "\n",
    "# Visualize C parameter effect\n",
    "plt.figure(figsize=(10, 6))\n",
    "for solver in param_grid['clf__solver']:\n",
    "    mask = results_df['param_clf__solver'] == solver\n",
    "    plt.plot(\n",
    "        results_df[mask]['param_clf__C'],\n",
    "        results_df[mask]['mean_test_score'],\n",
    "        marker='o', label=f'Solver: {solver}'\n",
    "    )\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C (Regularization Parameter)')\n",
    "plt.ylabel('Mean CV ROC-AUC')\n",
    "plt.title('Grid Search: C Parameter Effect')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The top-5 parameter combinations table shows each combination ranked by `rank_test_score`. Columns include the actual parameter values (`param_clf__C`, `param_clf__solver`), the mean and standard deviation of the test (validation) score, and the mean training score.\n",
    "\n",
    "The line plot below visualizes how **C** (the inverse regularization strength) affects ROC-AUC for each solver. You will typically see:\n",
    "- **Small C (0.01):** Strong regularization, slightly lower scores because the model is too constrained.\n",
    "- **Mid-range C (0.1-1.0):** The sweet spot with highest CV scores.\n",
    "- **Large C (10.0):** Weak regularization; scores may plateau or dip slightly as the model begins to overfit.\n",
    "\n",
    "The two solver lines (lbfgs and liblinear) usually overlap closely, confirming that solver choice matters less than regularization strength for this problem.\n",
    "\n",
    "**Key takeaway:** Examining `cv_results_` is not just good practice -- it is essential. The best parameters alone do not tell you how *sensitive* performance is to each hyperparameter. If performance is nearly flat across all C values, you have little to gain from further tuning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RandomizedSearchCV - Faster Alternative\n",
    "\n",
    "### When to Use Randomized Search\n",
    "\n",
    "**Use RandomizedSearchCV when:**\n",
    "- Parameter space is large\n",
    "- Continuous parameters\n",
    "- Time budget is limited\n",
    "- Initial exploration phase\n",
    "\n",
    "**Advantage:** Sample randomly instead of exhaustive search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest with randomized search\n",
    "rf_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', RandomForestClassifier(random_state=RANDOM_SEED))\n",
    "])\n",
    "\n",
    "# Define parameter distributions\n",
    "param_distributions = {\n",
    "    'clf__n_estimators': randint(50, 200),\n",
    "    'clf__max_depth': randint(3, 20),\n",
    "    'clf__min_samples_split': randint(2, 20),\n",
    "    'clf__min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    rf_pipeline,\n",
    "    param_distributions,\n",
    "    n_iter=20,  # Try 20 random combinations\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED),\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_SEED,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(\"=== RANDOMIZED SEARCH ===\")\n",
    "print(f\"Parameter space: {param_distributions}\")\n",
    "print(f\"Sampling 20 random combinations...\")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters: {random_search.best_params_}\")\n",
    "print(f\"Best CV score: {random_search.best_score_:.4f}\")\n",
    "print(f\"Validation score: {random_search.score(X_val, y_val):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "Randomized search sampled **20 random combinations** from the specified distributions: `n_estimators` between 50 and 200, `max_depth` between 3 and 20, `min_samples_split` between 2 and 20, and `min_samples_leaf` between 1 and 10. Each combination was evaluated with 5-fold stratified CV, totaling **100 model fits**.\n",
    "\n",
    "The output reports:\n",
    "- **Best parameters:** a specific set of Random Forest hyperparameters, such as `n_estimators=150, max_depth=12, min_samples_split=4, min_samples_leaf=2`.\n",
    "- **Best CV score:** the mean ROC-AUC of the winning combination, likely in the **0.98-0.99** range.\n",
    "- **Validation score:** performance on the held-out validation set.\n",
    "\n",
    "Compared to GridSearchCV, randomized search explored a much larger parameter space (continuous distributions instead of fixed lists) while fitting fewer total models. The tradeoff is that it *might* miss the exact optimum, but research by Bergstra & Bengio (2012) shows that random search finds near-optimal configurations with far fewer iterations.\n",
    "\n",
    "**Why this matters:** Use `GridSearchCV` when the parameter space is small and discrete. Use `RandomizedSearchCV` for initial exploration of large or continuous spaces, then optionally refine with a focused grid around the best region.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd PAUSE-AND-DO Exercise 2 (10 minutes)\n",
    "\n",
    "**Task:** Run a small grid (2-3 params) and report best CV score.\n",
    "\n",
    "Already done above! Now create a baseline report table:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive baseline report\n",
    "baseline_report = []\n",
    "\n",
    "# Model 1: Simple baseline\n",
    "baseline_report.append({\n",
    "    'Model': 'Logistic (default)',\n",
    "    'Val_ROC_AUC': baseline_score,\n",
    "    'Parameters': 'C=1.0, penalty=l2',\n",
    "    'Features': X.shape[1],\n",
    "    'Notes': 'Simple baseline'\n",
    "})\n",
    "\n",
    "# Model 2: Grid search best\n",
    "baseline_report.append({\n",
    "    'Model': 'Logistic (tuned)',\n",
    "    'Val_ROC_AUC': grid_search.score(X_val, y_val),\n",
    "    'Parameters': str(grid_search.best_params_),\n",
    "    'Features': X.shape[1],\n",
    "    'Notes': 'Grid search optimized'\n",
    "})\n",
    "\n",
    "# Model 3: Random Forest tuned\n",
    "baseline_report.append({\n",
    "    'Model': 'Random Forest (tuned)',\n",
    "    'Val_ROC_AUC': random_search.score(X_val, y_val),\n",
    "    'Parameters': str(random_search.best_params_),\n",
    "    'Features': X.shape[1],\n",
    "    'Notes': 'Random search optimized'\n",
    "})\n",
    "\n",
    "report_df = pd.DataFrame(baseline_report)\n",
    "print(\"=== PROJECT BASELINE REPORT ===\")\n",
    "print(report_df.to_string(index=False))\n",
    "\n",
    "# Identify champion\n",
    "best_idx = report_df['Val_ROC_AUC'].idxmax()\n",
    "print(f\"\\n\u2713 Champion model: {report_df.loc[best_idx, 'Model']}\")\n",
    "print(f\"\u2713 Validation ROC-AUC: {report_df.loc[best_idx, 'Val_ROC_AUC']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The baseline report table compares three models head-to-head:\n",
    "\n",
    "| Model | What it represents |\n",
    "|-------|-------------------|\n",
    "| **Logistic (default)** | No tuning, C=1.0 -- the performance floor |\n",
    "| **Logistic (tuned)** | Best parameters from GridSearchCV |\n",
    "| **Random Forest (tuned)** | Best parameters from RandomizedSearchCV |\n",
    "\n",
    "Each row shows the validation ROC-AUC, the winning parameters, the feature count, and a short note. The **champion model** is highlighted at the bottom -- the model with the highest validation ROC-AUC.\n",
    "\n",
    "On this dataset, all three models likely perform within a narrow band (0.96-0.99 ROC-AUC), because the breast cancer features are highly informative. In your course project, the gaps between models may be larger, making this comparison table even more valuable.\n",
    "\n",
    "**Key takeaway:** A baseline report table is a *deliverable*, not just a debugging tool. It documents every model you tried, the parameters you used, and the metrics you achieved. Include this table in your project submission so reviewers can see your modeling journey at a glance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Project Baseline Notebook Scaffold\n",
    "\n",
    "### Required Components for Project Baseline\n",
    "\n",
    "1. **Data Loading and Audit**\n",
    "   - Load dataset\n",
    "   - Check for issues\n",
    "   - Document data quality\n",
    "\n",
    "2. **Train/Val/Test Splits**\n",
    "   - Proper splits with stratification\n",
    "   - Lock test set away\n",
    "\n",
    "3. **Baseline Model**\n",
    "   - Simple model (mean/mode/simple classifier)\n",
    "   - Establishes floor performance\n",
    "\n",
    "4. **Improved Model**\n",
    "   - Preprocessing pipeline\n",
    "   - Tuned hyperparameters\n",
    "   - CV evaluation\n",
    "\n",
    "5. **Evaluation Report**\n",
    "   - Multiple metrics\n",
    "   - Comparison table\n",
    "   - Visualizations\n",
    "\n",
    "6. **Documentation**\n",
    "   - Modeling choices explained\n",
    "   - Assumptions documented\n",
    "   - Next steps identified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Gemini Prompts for Tuning\n",
    "\n",
    "### Example Prompts:\n",
    "\n",
    "**Prompt 1: Generate Parameter Grid**\n",
    "```\n",
    "I'm tuning a Random Forest classifier for a binary classification task.\n",
    "Generate a reasonable parameter grid for GridSearchCV including:\n",
    "- n_estimators\n",
    "- max_depth\n",
    "- min_samples_split\n",
    "\n",
    "Keep it small (< 20 combinations) for initial exploration.\n",
    "```\n",
    "\n",
    "**Prompt 2: Optimize Grid**\n",
    "```\n",
    "I ran GridSearchCV and found best params: {results}\n",
    "Help me design a refined grid search around these values\n",
    "to fine-tune performance.\n",
    "```\n",
    "\n",
    "**Prompt 3: Debug Search**\n",
    "```\n",
    "My RandomizedSearchCV is taking too long. Here's my config: {config}\n",
    "Help me reduce search time while maintaining good coverage.\n",
    "```\n",
    "\n",
    "**Remember:**\n",
    "- Verify Gemini's suggestions\n",
    "- Start small, then expand\n",
    "- Always use CV, never single split\n",
    "- Document your search strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Wrap-Up: Key Takeaways\n\n### What We Learned Today:\n\n1. **Pipeline Feature Engineering**: Keep everything inside pipelines\n2. **GridSearchCV**: Exhaustive search for small parameter spaces\n3. **RandomizedSearchCV**: Faster exploration of large spaces\n4. **Baseline Reports**: Document all models systematically\n5. **Project Readiness**: Structure for reproducible modeling\n\n### Critical Rules:\n\n> **\"All feature engineering must be in the pipeline\"**\n\n> **\"Start with small grids, then refine\"**\n\n> **\"Document every modeling choice\"**\n\n### Next Steps:\n\n- Next notebook: Midterm - Business case practicum\n- **Project Milestone 2 checkpoint**: Draft baseline notebook\n- Apply today's patterns to your project dataset\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n\n- James, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). *An Introduction to Statistical Learning with Python* - Python labs on feature engineering\n- scikit-learn User Guide: [Grid search](https://scikit-learn.org/stable/modules/grid_search.html)\n- scikit-learn User Guide: [Pipeline parameter tuning](https://scikit-learn.org/stable/modules/compose.html#pipeline-tuning)\n- Provost, F., & Fawcett, T. (2013). *Data Science for Business* - Evaluation and business framing\n\n---\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "Thank you!\n",
    "\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}