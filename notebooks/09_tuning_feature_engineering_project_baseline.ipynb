{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering + Model Selection Workflow (Project Baseline Build)\n",
    "\n",
    "<hr>\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/notebooks/figures/mgmt_474_ai_logo_02-modified.png\" width=\"200\"/>\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "# <center><a class=\"tocSkip\"></center>\n",
    "# <center>MGMT47400 Predictive Analytics</center>\n",
    "# <center>Professor: Davi Moreira </center>\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/blob/main/notebooks/09_tuning_feature_engineering_project_baseline.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Engineer features with pipelines without leakage\n",
    "2. Use `GridSearchCV` / `RandomizedSearchCV` for systematic tuning\n",
    "3. Define a project-grade evaluation plan (metric + split/CV + baseline + reporting)\n",
    "4. Produce a baseline model notebook that can be extended\n",
    "5. Use Gemini to draft search grids and then simplify them\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import (\n    train_test_split, StratifiedKFold,\n    GridSearchCV, RandomizedSearchCV\n)\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom scipy.stats import uniform, randint\nimport warnings\n\nwarnings.filterwarnings('ignore')\npd.set_option('display.precision', 4)\nRANDOM_SEED = 474\nnp.random.seed(RANDOM_SEED)\nprint(\"\u2713 Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = load_breast_cancer(as_frame=True)\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.20, random_state=RANDOM_SEED, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=RANDOM_SEED, stratify=y_temp)\n",
    "\n",
    "print(f\"Train: {len(X_train)} | Val: {len(X_val)} | Test: {len(X_test)}\")\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "\n",
    "# Simple baseline\n",
    "baseline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', LogisticRegression(random_state=RANDOM_SEED, max_iter=1000))\n",
    "])\n",
    "\n",
    "baseline.fit(X_train, y_train)\n",
    "baseline_score = baseline.score(X_val, y_val)\n",
    "\n",
    "print(f\"\\nBaseline validation accuracy: {baseline_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering Inside Pipelines\n",
    "\n",
    "### Safe Feature Engineering Patterns\n",
    "\n",
    "**Rule:** All feature engineering must happen INSIDE the pipeline\n",
    "\n",
    "**Why?**\n",
    "- Prevents leakage (fit on train, transform on val/test)\n",
    "- Ensures reproducibility\n",
    "- Makes deployment easier\n",
    "\n",
    "**Common feature engineering steps:**\n",
    "1. Polynomial features\n",
    "2. Feature interactions\n",
    "3. Feature selection\n",
    "4. Domain-specific transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline with feature engineering\n",
    "fe_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('feature_selection', SelectKBest(f_classif, k=20)),  # Keep top 20 features\n",
    "    ('clf', LogisticRegression(random_state=RANDOM_SEED, max_iter=1000))\n",
    "])\n",
    "\n",
    "fe_pipeline.fit(X_train, y_train)\n",
    "fe_score = fe_pipeline.score(X_val, y_val)\n",
    "\n",
    "print(\"=== FEATURE ENGINEERING PIPELINE ===\")\n",
    "print(f\"Original features: {X.shape[1]}\")\n",
    "print(f\"Selected features: 20\")\n",
    "print(f\"Validation accuracy: {fe_score:.4f}\")\n",
    "print(f\"Improvement: {(fe_score - baseline_score):.4f}\")\n",
    "\n",
    "# See which features were selected\n",
    "selected_mask = fe_pipeline.named_steps['feature_selection'].get_support()\n",
    "selected_features = X.columns[selected_mask].tolist()\n",
    "print(f\"\\nSelected features: {selected_features[:5]}... (showing first 5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd PAUSE-AND-DO Exercise 1 (10 minutes)\n",
    "\n",
    "**Task:** Add engineered features and re-run CV.\n",
    "\n",
    "Try adding polynomial features (degree=2) to a subset of features.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE: Add polynomial features\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Warning: Polynomial features can explode the feature space!\n",
    "# Let's use only a subset\n",
    "poly_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('feature_selection_pre', SelectKBest(f_classif, k=10)),  # Reduce to 10 first\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('scaler2', StandardScaler()),  # Re-scale after polynomial\n",
    "    ('clf', LogisticRegression(random_state=RANDOM_SEED, max_iter=2000))\n",
    "])\n",
    "\n",
    "# Evaluate with CV\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "poly_scores = cross_val_score(poly_pipeline, X_train, y_train, cv=cv, scoring='roc_auc')\n",
    "baseline_scores = cross_val_score(baseline, X_train, y_train, cv=cv, scoring='roc_auc')\n",
    "\n",
    "print(\"=== POLYNOMIAL FEATURES COMPARISON ===\")\n",
    "print(f\"Baseline:   {baseline_scores.mean():.4f} \u00b1 {baseline_scores.std():.4f}\")\n",
    "print(f\"Polynomial: {poly_scores.mean():.4f} \u00b1 {poly_scores.std():.4f}\")\n",
    "print(f\"\\nImprovement: {(poly_scores.mean() - baseline_scores.mean()):.4f}\")\n",
    "\n",
    "# Check feature explosion\n",
    "poly_pipeline.fit(X_train.iloc[:10], y_train.iloc[:10])  # Fit on small sample to check\n",
    "n_poly_features = poly_pipeline.named_steps['poly'].n_output_features_\n",
    "print(f\"\\n\u26a0\ufe0f Feature explosion: 10 \u2192 {n_poly_features} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANALYSIS:\n",
    "\n",
    "**Did polynomial features help?**  \n",
    "[Your analysis]\n",
    "\n",
    "**What's the cost?**  \n",
    "[Feature explosion, complexity, overfitting risk]\n",
    "\n",
    "**Would you use this in production?**  \n",
    "[Justify your decision]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GridSearchCV - Systematic Hyperparameter Tuning\n",
    "\n",
    "### How GridSearchCV Works\n",
    "\n",
    "1. Define parameter grid\n",
    "2. Try every combination\n",
    "3. Use CV to evaluate each\n",
    "4. Return best parameters\n",
    "\n",
    "**Warning:** Grid search can be expensive!\n",
    "- 3 parameters \u00d7 3 values each = 27 combinations\n",
    "- 27 combinations \u00d7 5 folds = 135 model fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', LogisticRegression(random_state=RANDOM_SEED, max_iter=1000))\n",
    "])\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'clf__C': [0.01, 0.1, 1.0, 10.0],\n",
    "    'clf__penalty': ['l2'],  # Just L2 for speed\n",
    "    'clf__solver': ['lbfgs', 'liblinear']\n",
    "}\n",
    "\n",
    "print(\"=== GRID SEARCH CONFIGURATION ===\")\n",
    "print(f\"Parameter grid: {param_grid}\")\n",
    "n_combinations = len(param_grid['clf__C']) * len(param_grid['clf__penalty']) * len(param_grid['clf__solver'])\n",
    "print(f\"Total combinations: {n_combinations}\")\n",
    "print(f\"With 5-fold CV: {n_combinations * 5} model fits\")\n",
    "\n",
    "# Run grid search\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED),\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=0,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(\"\\nRunning grid search...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n=== GRID SEARCH RESULTS ===\")\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
    "print(f\"Validation score: {grid_search.score(X_val, y_val):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine all results\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "results_summary = results_df[[\n",
    "    'param_clf__C', 'param_clf__solver',\n",
    "    'mean_test_score', 'std_test_score',\n",
    "    'mean_train_score', 'rank_test_score'\n",
    "]].sort_values('rank_test_score')\n",
    "\n",
    "print(\"\\n=== TOP 5 PARAMETER COMBINATIONS ===\")\n",
    "print(results_summary.head().to_string(index=False))\n",
    "\n",
    "# Visualize C parameter effect\n",
    "plt.figure(figsize=(10, 6))\n",
    "for solver in param_grid['clf__solver']:\n",
    "    mask = results_df['param_clf__solver'] == solver\n",
    "    plt.plot(\n",
    "        results_df[mask]['param_clf__C'],\n",
    "        results_df[mask]['mean_test_score'],\n",
    "        marker='o', label=f'Solver: {solver}'\n",
    "    )\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C (Regularization Parameter)')\n",
    "plt.ylabel('Mean CV ROC-AUC')\n",
    "plt.title('Grid Search: C Parameter Effect')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RandomizedSearchCV - Faster Alternative\n",
    "\n",
    "### When to Use Randomized Search\n",
    "\n",
    "**Use RandomizedSearchCV when:**\n",
    "- Parameter space is large\n",
    "- Continuous parameters\n",
    "- Time budget is limited\n",
    "- Initial exploration phase\n",
    "\n",
    "**Advantage:** Sample randomly instead of exhaustive search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest with randomized search\n",
    "rf_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', RandomForestClassifier(random_state=RANDOM_SEED))\n",
    "])\n",
    "\n",
    "# Define parameter distributions\n",
    "param_distributions = {\n",
    "    'clf__n_estimators': randint(50, 200),\n",
    "    'clf__max_depth': randint(3, 20),\n",
    "    'clf__min_samples_split': randint(2, 20),\n",
    "    'clf__min_samples_leaf': randint(1, 10)\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    rf_pipeline,\n",
    "    param_distributions,\n",
    "    n_iter=20,  # Try 20 random combinations\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED),\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_SEED,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(\"=== RANDOMIZED SEARCH ===\")\n",
    "print(f\"Parameter space: {param_distributions}\")\n",
    "print(f\"Sampling 20 random combinations...\")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters: {random_search.best_params_}\")\n",
    "print(f\"Best CV score: {random_search.best_score_:.4f}\")\n",
    "print(f\"Validation score: {random_search.score(X_val, y_val):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd PAUSE-AND-DO Exercise 2 (10 minutes)\n",
    "\n",
    "**Task:** Run a small grid (2-3 params) and report best CV score.\n",
    "\n",
    "Already done above! Now create a baseline report table:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive baseline report\n",
    "baseline_report = []\n",
    "\n",
    "# Model 1: Simple baseline\n",
    "baseline_report.append({\n",
    "    'Model': 'Logistic (default)',\n",
    "    'Val_ROC_AUC': baseline_score,\n",
    "    'Parameters': 'C=1.0, penalty=l2',\n",
    "    'Features': X.shape[1],\n",
    "    'Notes': 'Simple baseline'\n",
    "})\n",
    "\n",
    "# Model 2: Grid search best\n",
    "baseline_report.append({\n",
    "    'Model': 'Logistic (tuned)',\n",
    "    'Val_ROC_AUC': grid_search.score(X_val, y_val),\n",
    "    'Parameters': str(grid_search.best_params_),\n",
    "    'Features': X.shape[1],\n",
    "    'Notes': 'Grid search optimized'\n",
    "})\n",
    "\n",
    "# Model 3: Random Forest tuned\n",
    "baseline_report.append({\n",
    "    'Model': 'Random Forest (tuned)',\n",
    "    'Val_ROC_AUC': random_search.score(X_val, y_val),\n",
    "    'Parameters': str(random_search.best_params_),\n",
    "    'Features': X.shape[1],\n",
    "    'Notes': 'Random search optimized'\n",
    "})\n",
    "\n",
    "report_df = pd.DataFrame(baseline_report)\n",
    "print(\"=== PROJECT BASELINE REPORT ===\")\n",
    "print(report_df.to_string(index=False))\n",
    "\n",
    "# Identify champion\n",
    "best_idx = report_df['Val_ROC_AUC'].idxmax()\n",
    "print(f\"\\n\u2713 Champion model: {report_df.loc[best_idx, 'Model']}\")\n",
    "print(f\"\u2713 Validation ROC-AUC: {report_df.loc[best_idx, 'Val_ROC_AUC']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Project Baseline Notebook Scaffold\n",
    "\n",
    "### Required Components for Project Baseline\n",
    "\n",
    "1. **Data Loading and Audit**\n",
    "   - Load dataset\n",
    "   - Check for issues\n",
    "   - Document data quality\n",
    "\n",
    "2. **Train/Val/Test Splits**\n",
    "   - Proper splits with stratification\n",
    "   - Lock test set away\n",
    "\n",
    "3. **Baseline Model**\n",
    "   - Simple model (mean/mode/simple classifier)\n",
    "   - Establishes floor performance\n",
    "\n",
    "4. **Improved Model**\n",
    "   - Preprocessing pipeline\n",
    "   - Tuned hyperparameters\n",
    "   - CV evaluation\n",
    "\n",
    "5. **Evaluation Report**\n",
    "   - Multiple metrics\n",
    "   - Comparison table\n",
    "   - Visualizations\n",
    "\n",
    "6. **Documentation**\n",
    "   - Modeling choices explained\n",
    "   - Assumptions documented\n",
    "   - Next steps identified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Gemini Prompts for Tuning\n",
    "\n",
    "### Example Prompts:\n",
    "\n",
    "**Prompt 1: Generate Parameter Grid**\n",
    "```\n",
    "I'm tuning a Random Forest classifier for a binary classification task.\n",
    "Generate a reasonable parameter grid for GridSearchCV including:\n",
    "- n_estimators\n",
    "- max_depth\n",
    "- min_samples_split\n",
    "\n",
    "Keep it small (< 20 combinations) for initial exploration.\n",
    "```\n",
    "\n",
    "**Prompt 2: Optimize Grid**\n",
    "```\n",
    "I ran GridSearchCV and found best params: {results}\n",
    "Help me design a refined grid search around these values\n",
    "to fine-tune performance.\n",
    "```\n",
    "\n",
    "**Prompt 3: Debug Search**\n",
    "```\n",
    "My RandomizedSearchCV is taking too long. Here's my config: {config}\n",
    "Help me reduce search time while maintaining good coverage.\n",
    "```\n",
    "\n",
    "**Remember:**\n",
    "- Verify Gemini's suggestions\n",
    "- Start small, then expand\n",
    "- Always use CV, never single split\n",
    "- Document your search strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Wrap-Up: Key Takeaways\n\n### What We Learned Today:\n\n1. **Pipeline Feature Engineering**: Keep everything inside pipelines\n2. **GridSearchCV**: Exhaustive search for small parameter spaces\n3. **RandomizedSearchCV**: Faster exploration of large spaces\n4. **Baseline Reports**: Document all models systematically\n5. **Project Readiness**: Structure for reproducible modeling\n\n### Critical Rules:\n\n> **\"All feature engineering must be in the pipeline\"**\n\n> **\"Start with small grids, then refine\"**\n\n> **\"Document every modeling choice\"**\n\n### Next Steps:\n\n- Next notebook: Midterm - Business case practicum\n- **Project Milestone 2 checkpoint**: Draft baseline notebook\n- Apply today's patterns to your project dataset\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n\n- James, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). *An Introduction to Statistical Learning with Python* - Python labs on feature engineering\n- scikit-learn User Guide: [Grid search](https://scikit-learn.org/stable/modules/grid_search.html)\n- scikit-learn User Guide: [Pipeline parameter tuning](https://scikit-learn.org/stable/modules/compose.html#pipeline-tuning)\n- Provost, F., & Fawcett, T. (2013). *Data Science for Business* - Evaluation and business framing\n\n---\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "Thank you!\n",
    "\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}