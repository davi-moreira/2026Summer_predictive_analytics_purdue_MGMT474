{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests - Bagging, OOB Intuition, and Feature Importance\n",
    "\n",
    "<hr>\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/notebooks/figures/mgmt_474_ai_logo_02-modified.png\" width=\"200\"/>\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "# <center><a class=\"tocSkip\"></center>\n",
    "# <center>MGMT47400 Predictive Analytics</center>\n",
    "# <center>Professor: Davi Moreira </center>\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/blob/main/notebooks/12_random_forests_importance.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Explain bagging and why forests reduce variance\n",
    "2. Train a random forest and tune the most impactful knobs\n",
    "3. Use permutation importance responsibly\n",
    "4. Compare forest vs tree vs linear/logistic baselines\n",
    "5. Produce project-ready model comparison tables\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.metrics import roc_auc_score\nimport warnings\n\nwarnings.filterwarnings('ignore')\npd.set_option('display.precision', 4)\nRANDOM_SEED = 474\nnp.random.seed(RANDOM_SEED)\nprint(\"\u2713 Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The setup cell loads `RandomForestClassifier` from scikit-learn's ensemble module alongside the single-tree classifier and logistic regression we used in the previous notebook. `permutation_importance` from `sklearn.inspection` is the model-agnostic importance method we will use later.\n",
    "\n",
    "The confirmation message `Setup complete!` with **RANDOM_SEED = 474** ensures reproducibility. All forest randomness (bootstrap sampling, feature subsets at each split) flows from this single seed.\n",
    "\n",
    "**Key takeaway:** Importing `n_jobs=-1` later in the forest constructor will parallelize tree training across all CPU cores, which is important because forests train 100-300 independent trees.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. From Single Tree to Forest: The Bagging Idea\n",
    "\n",
    "### The Variance Problem with Single Trees\n",
    "\n",
    "**Problem:** Decision trees are unstable\n",
    "- Small change in data \u2192 completely different tree\n",
    "- High variance in predictions\n",
    "- Overfitting on individual quirks\n",
    "\n",
    "### The Solution: Bootstrap Aggregating (Bagging)\n",
    "\n",
    "**Algorithm:**\n",
    "1. Create B bootstrap samples (random sampling with replacement)\n",
    "2. Train one tree on each bootstrap sample\n",
    "3. Aggregate predictions (average for regression, vote for classification)\n",
    "\n",
    "**Why it works:**\n",
    "- Averaging reduces variance\n",
    "- Each tree sees slightly different data\n",
    "- Errors cancel out through averaging\n",
    "\n",
    "### Random Forest = Bagging + Random Feature Selection\n",
    "\n",
    "**Extra randomness:** At each split, only consider random subset of features\n",
    "- Decorrelates trees\n",
    "- Prevents one strong feature from dominating\n",
    "- Further variance reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = load_breast_cancer(as_frame=True)\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RANDOM_SEED, stratify=y)\n",
    "\n",
    "print(f\"Train: {len(X_train)} | Test: {len(X_test)}\")\n",
    "print(f\"Features: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The breast cancer dataset is split into **398 training** and **171 test** samples with stratification, preserving the roughly 63/37 benign-to-malignant class balance in each split. The dataset has **30 features**, all continuous measurements of cell nuclei (mean, standard error, and worst-case values for 10 properties like radius, texture, and symmetry).\n",
    "\n",
    "**Why this matters:** Having 30 features is important for Random Forests because the algorithm randomly selects a subset of features at each split. With 30 features and the default `max_features='sqrt'`, each split considers roughly 5-6 candidate features, creating diversity among the trees.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Single Tree vs Random Forest\n",
    "\n",
    "The core claim of Random Forests is that averaging many decorrelated trees produces a model with **lower variance** and **higher overall accuracy** than any single tree. The experiment below makes this concrete: we train one decision tree (depth-5) and one forest of 100 depth-5 trees on the same breast cancer dataset, using the same 5-fold stratified CV.\n",
    "\n",
    "Watch for two things in the output: (1) the forest's mean ROC-AUC should be higher, and (2) its standard deviation across folds should be smaller. A tighter spread means the forest's predictions are more stable under different data partitions, which is exactly what variance reduction buys you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare single tree vs forest\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "# Single tree (tuned depth)\n",
    "tree = DecisionTreeClassifier(max_depth=5, random_state=RANDOM_SEED)\n",
    "tree_scores = cross_val_score(tree, X_train, y_train, cv=cv, scoring='roc_auc')\n",
    "\n",
    "# Random forest\n",
    "forest = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=RANDOM_SEED)\n",
    "forest_scores = cross_val_score(forest, X_train, y_train, cv=cv, scoring='roc_auc')\n",
    "\n",
    "print(\"=== SINGLE TREE VS RANDOM FOREST ===\")\n",
    "print(f\"\\nSingle Tree (depth=5):\")\n",
    "print(f\"  CV ROC-AUC: {tree_scores.mean():.4f} \u00b1 {tree_scores.std():.4f}\")\n",
    "print(f\"  Fold scores: {tree_scores.round(4)}\")\n",
    "\n",
    "print(f\"\\nRandom Forest (100 trees, depth=5):\")\n",
    "print(f\"  CV ROC-AUC: {forest_scores.mean():.4f} \u00b1 {forest_scores.std():.4f}\")\n",
    "print(f\"  Fold scores: {forest_scores.round(4)}\")\n",
    "\n",
    "print(f\"\\n=== IMPROVEMENT ===\")\n",
    "print(f\"Mean improvement: {(forest_scores.mean() - tree_scores.mean()):.4f}\")\n",
    "print(f\"Variance reduction: {(tree_scores.std() - forest_scores.std()):.4f}\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "positions = [1, 2]\n",
    "bp = ax.boxplot([tree_scores, forest_scores], positions=positions, widths=0.6, patch_artist=True)\n",
    "for patch, color in zip(bp['boxes'], ['lightblue', 'lightgreen']):\n",
    "    patch.set_facecolor(color)\n",
    "ax.set_xticklabels(['Single Tree', 'Random Forest'])\n",
    "ax.set_ylabel('ROC-AUC Score')\n",
    "ax.set_title('Variance Reduction: Single Tree vs Random Forest')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udca1 Forest has higher mean AND lower variance\")\n",
    "print(\"\ud83d\udca1 More stable predictions across different data samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The printout shows fold-level ROC-AUC scores for both models. The single tree's scores vary more across folds (higher standard deviation), while the forest's scores cluster tightly around a higher mean. Typical numbers: the single tree scores around **0.95-0.97** with std ~0.02, while the forest scores **0.98-0.99** with std ~0.01.\n",
    "\n",
    "The box plot reinforces this visually. The forest's box is both **higher** (better median) and **narrower** (less variance) than the single tree's box. The \"Improvement\" line quantifies the mean lift, and the \"Variance reduction\" line shows how much tighter the forest's spread is.\n",
    "\n",
    "**Why this matters:** This is the entire motivation for ensemble methods. A single tree is a high-variance estimator; averaging 100 trees with different bootstrap samples and random feature subsets dramatically stabilizes predictions without sacrificing accuracy.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tuning Random Forests\n",
    "\n",
    "### Most Important Hyperparameters\n",
    "\n",
    "**n_estimators** (number of trees)\n",
    "- More trees = better performance (usually)\n",
    "- Diminishing returns after ~100-500\n",
    "- More trees = longer training time\n",
    "\n",
    "**max_features** (features per split)\n",
    "- sqrt(n_features) for classification (default)\n",
    "- n_features/3 for regression\n",
    "- Lower values = more decorrelation\n",
    "\n",
    "**max_depth** (tree depth)\n",
    "- Controls individual tree complexity\n",
    "- None = grow until pure (common for forests)\n",
    "\n",
    "**min_samples_split** (minimum samples to split)\n",
    "- Higher values = simpler trees\n",
    "- Prevents overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of number of trees\n",
    "n_estimators_range = [10, 25, 50, 100, 200, 300]\n",
    "results = []\n",
    "\n",
    "for n_est in n_estimators_range:\n",
    "    rf = RandomForestClassifier(n_estimators=n_est, random_state=RANDOM_SEED, n_jobs=-1)\n",
    "    scores = cross_val_score(rf, X_train, y_train, cv=cv, scoring='roc_auc')\n",
    "    results.append({\n",
    "        'n_estimators': n_est,\n",
    "        'cv_mean': scores.mean(),\n",
    "        'cv_std': scores.std()\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"=== N_ESTIMATORS SWEEP ===\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(results_df['n_estimators'], results_df['cv_mean'], \n",
    "             yerr=results_df['cv_std'], marker='o', capsize=5, linewidth=2)\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('CV ROC-AUC')\n",
    "plt.title('Random Forest: Effect of Number of Trees')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udca1 Performance plateaus after ~100-200 trees\")\n",
    "print(\"\ud83d\udca1 Use more trees for final model, fewer for experimentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The table shows how CV ROC-AUC changes as we increase the number of trees from 10 to 300. Performance improves rapidly from 10 to 50 trees, then plateaus around 100-200. Going from 200 to 300 trees typically adds less than **0.001** to the mean ROC-AUC.\n",
    "\n",
    "The error-bar plot makes the diminishing returns clear: the curve flattens well before 300 trees. Meanwhile, training time scales linearly with `n_estimators`, so there is a practical cost to adding more trees beyond the plateau.\n",
    "\n",
    "Standard deviation also decreases slightly with more trees, because averaging over a larger ensemble further stabilizes the estimate. However, the reduction is marginal after ~100 trees.\n",
    "\n",
    "**Key takeaway:** Use 100-200 trees for experimentation and tuning. For a final production model you can bump to 500+ for a tiny extra boost, but the gains will be minimal. The real performance levers are `max_features` and `max_depth`.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd PAUSE-AND-DO Exercise 1 (10 minutes)\n",
    "\n",
    "**Task:** Tune `n_estimators` and `max_features` minimally and report effects.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE: Tune max_features\n",
    "import math\n",
    "\n",
    "max_features_options = ['sqrt', 'log2', 0.3, 0.5, None]\n",
    "tuning_results = []\n",
    "\n",
    "for max_feat in max_features_options:\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_features=max_feat,\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    scores = cross_val_score(rf, X_train, y_train, cv=cv, scoring='roc_auc')\n",
    "    tuning_results.append({\n",
    "        'max_features': str(max_feat),\n",
    "        'cv_mean': scores.mean(),\n",
    "        'cv_std': scores.std()\n",
    "    })\n",
    "\n",
    "tuning_df = pd.DataFrame(tuning_results)\n",
    "print(\"=== MAX_FEATURES TUNING ===\")\n",
    "print(tuning_df.to_string(index=False))\n",
    "\n",
    "best_idx = tuning_df['cv_mean'].idxmax()\n",
    "print(f\"\\n\u2713 Best max_features: {tuning_df.loc[best_idx, 'max_features']} ({tuning_df.loc[best_idx, 'cv_mean']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The table compares five `max_features` settings: `sqrt` (~5 features), `log2` (~5 features), `0.3` (9 features), `0.5` (15 features), and `None` (all 30 features). On the breast cancer dataset, `sqrt` and `log2` often perform similarly and are competitive with higher values.\n",
    "\n",
    "Using all features (`None`) makes each tree more similar to every other tree, which reduces the variance-reduction benefit of the forest. Lower values force more diversity among trees. However, setting `max_features` too low can hurt if no single small subset of features carries enough signal.\n",
    "\n",
    "The best setting is highlighted at the bottom. In practice, `sqrt` is the recommended default for classification and is rarely far from optimal.\n",
    "\n",
    "**Key takeaway:** `max_features` controls the bias-variance tradeoff at the individual tree level. Lower values mean more tree diversity (lower correlation between trees), which improves ensemble performance up to a point.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANALYSIS:\n",
    "\n",
    "**Effect of max_features:**  \n",
    "[What did you observe? How does it affect performance?]\n",
    "\n",
    "**Recommendation:**  \n",
    "[Which value would you use in production? Why?]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Out-of-Bag (OOB) Score\n",
    "\n",
    "### Free Cross-Validation\n",
    "\n",
    "**Insight:** Each tree is trained on ~63% of data (bootstrap)\n",
    "- Remaining ~37% is \"out-of-bag\" for that tree\n",
    "- Can use OOB samples to estimate test performance\n",
    "- No need for separate validation set!\n",
    "\n",
    "**OOB Score \u2248 Cross-Validation Score**\n",
    "- Faster than CV\n",
    "- Uses all data for training\n",
    "- Good for initial model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare OOB vs CV\n",
    "rf_oob = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    oob_score=True,\n",
    "    random_state=RANDOM_SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_oob.fit(X_train, y_train)\n",
    "oob_score = rf_oob.oob_score_\n",
    "\n",
    "# Compare to CV\n",
    "cv_scores = cross_val_score(rf_oob, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "print(\"=== OOB VS CROSS-VALIDATION ===\")\n",
    "print(f\"OOB Score (free): {oob_score:.4f}\")\n",
    "print(f\"CV Score (5-fold): {cv_scores.mean():.4f} \u00b1 {cv_scores.std():.4f}\")\n",
    "print(f\"Difference: {abs(oob_score - cv_scores.mean()):.4f}\")\n",
    "\n",
    "print(\"\\n\ud83d\udca1 OOB score is a good proxy for test performance\")\n",
    "print(\"\ud83d\udca1 Use OOB for quick iterations, CV for final evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "Two numbers are printed side by side: the **OOB score** (computed internally by the forest from out-of-bag samples) and the **5-fold CV score**. Both are accuracy estimates. Typically they agree to within **0.005-0.01**, confirming that OOB is a reliable free proxy for cross-validation.\n",
    "\n",
    "The OOB approach works because each tree is trained on only ~63 % of the data (a bootstrap sample), so the remaining ~37 % serves as a built-in validation set for that tree. Aggregating these per-tree OOB predictions across all 100 trees yields an unbiased performance estimate.\n",
    "\n",
    "**Why this matters:** OOB scores are computed during training with zero extra cost, while 5-fold CV requires fitting 5 separate forests. For quick hyperparameter screening, OOB can save significant compute time. Reserve full CV for the final comparison.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Importance\n",
    "\n",
    "### Two Types of Importance\n",
    "\n",
    "**1. Gini/Entropy Importance (built-in)**\n",
    "- Based on how much each feature reduces impurity\n",
    "- Fast to compute\n",
    "- \u26a0\ufe0f Biased toward high-cardinality features\n",
    "- \u26a0\ufe0f Can be misleading with correlated features\n",
    "\n",
    "**2. Permutation Importance (recommended)**\n",
    "- Shuffle feature, measure performance drop\n",
    "- More reliable\n",
    "- Slower to compute\n",
    "- Works for any model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final forest\n",
    "rf_final = RandomForestClassifier(n_estimators=200, random_state=RANDOM_SEED, n_jobs=-1)\n",
    "rf_final.fit(X_train, y_train)\n",
    "\n",
    "# Built-in importance\n",
    "builtin_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': rf_final.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"=== BUILT-IN FEATURE IMPORTANCE (Top 10) ===\")\n",
    "print(builtin_importance.head(10).to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_features = builtin_importance.head(15)\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Gini Importance')\n",
    "plt.title('Top 15 Features by Built-in Importance')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The table and horizontal bar chart show the top 15 features ranked by built-in Gini importance. Features from the \"worst\" category (worst radius, worst concave points, worst perimeter) typically dominate because they capture extreme cell-nucleus measurements that strongly distinguish malignant from benign tumors.\n",
    "\n",
    "The importance values sum to 1.0 across all 30 features. A steep dropoff after the top 3-5 features suggests that the model's decisions are driven by a small subset of measurements, while the remaining features contribute little to the splits.\n",
    "\n",
    "However, remember the caveat: Gini importance is biased toward continuous features with many unique split points, and it double-counts correlated features. Features like `worst radius` and `worst perimeter` are highly correlated (r > 0.99), so their combined Gini importance overstates the unique information each provides.\n",
    "\n",
    "**Key takeaway:** Treat Gini importance as a quick screening tool, not a definitive ranking. The next section shows permutation importance, which is more reliable.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Permutation Importance (Recommended)\n",
    "\n",
    "Built-in Gini importance has a known flaw: it is biased toward high-cardinality and continuous features, and it can be misleading when features are correlated. **Permutation importance** avoids these pitfalls by measuring how much model performance drops when a single feature's values are randomly shuffled. If shuffling a feature barely affects ROC-AUC, that feature is not important for the model's decisions.\n",
    "\n",
    "We compute permutation importance on the **test set** (not training set) so the estimates reflect genuine predictive value rather than memorized patterns. Each feature is shuffled 10 times to produce a mean and standard deviation, giving us confidence intervals on the importance estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation importance\n",
    "perm_importance = permutation_importance(\n",
    "    rf_final, X_test, y_test,\n",
    "    n_repeats=10,\n",
    "    random_state=RANDOM_SEED,\n",
    "    scoring='roc_auc'\n",
    ")\n",
    "\n",
    "perm_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance_mean': perm_importance.importances_mean,\n",
    "    'importance_std': perm_importance.importances_std\n",
    "}).sort_values('importance_mean', ascending=False)\n",
    "\n",
    "print(\"=== PERMUTATION IMPORTANCE (Top 10) ===\")\n",
    "print(perm_df.head(10).to_string(index=False))\n",
    "\n",
    "# Compare built-in vs permutation\n",
    "comparison = builtin_importance.merge(\n",
    "    perm_df,\n",
    "    on='feature',\n",
    "    suffixes=('_builtin', '_perm')\n",
    ").head(10)\n",
    "\n",
    "print(\"\\n=== TOP 10: BUILT-IN VS PERMUTATION ===\")\n",
    "print(comparison[['feature', 'importance', 'importance_mean']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The permutation importance table ranks features by how much test-set ROC-AUC drops when each feature is shuffled. The `importance_std` column shows the variability across 10 shuffle repeats, acting as a confidence interval. A feature with high mean importance but also high std should be interpreted cautiously.\n",
    "\n",
    "The comparison table at the bottom shows built-in Gini importance alongside permutation importance for the top 10 features. You will likely notice differences in ranking: some features that rank high by Gini (because they are used in many splits) may rank lower by permutation (because other correlated features compensate when one is shuffled). For example, `worst perimeter` and `worst radius` often swap positions between the two methods.\n",
    "\n",
    "Features with permutation importance near zero or negative are essentially irrelevant to the model's predictions on new data, even if they appear in many tree splits.\n",
    "\n",
    "**Why this matters:** Permutation importance is the recommended method for production because it is model-agnostic, computed on held-out data, and less susceptible to correlation artifacts than Gini importance.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd PAUSE-AND-DO Exercise 2 (10 minutes)\n",
    "\n",
    "**Task:** Compute permutation importance and write 3 interpretation bullets.\n",
    "\n",
    "Already done above! Now analyze:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR INTERPRETATION:\n",
    "\n",
    "**Bullet 1: Top Features**  \n",
    "[Which features are most important? Why might this be?]\n",
    "\n",
    "**Bullet 2: Differences**  \n",
    "[How do built-in vs permutation importance differ?]\n",
    "\n",
    "**Bullet 3: Business Insight**  \n",
    "[What does this tell you about the prediction task?]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comprehensive Model Comparison\n",
    "\n",
    "Before concluding, we compare every model type encountered so far: Logistic Regression (linear baseline), a single tuned Decision Tree, and Random Forests with 100 and 200 trees. All models are evaluated under the same `StratifiedKFold` CV object to ensure fair comparison.\n",
    "\n",
    "The resulting table and bar chart make it easy to see how much ensemble methods improve over single models. Pay attention not only to the mean ROC-AUC but also to the standard deviation (stability) and the gap between CV and test scores (generalization). A model that has both the highest mean and the lowest spread is the clear champion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', LogisticRegression(random_state=RANDOM_SEED, max_iter=1000))\n",
    "    ]),\n",
    "    'Decision Tree (tuned)': DecisionTreeClassifier(max_depth=5, random_state=RANDOM_SEED),\n",
    "    'Random Forest (100)': RandomForestClassifier(n_estimators=100, random_state=RANDOM_SEED, n_jobs=-1),\n",
    "    'Random Forest (200)': RandomForestClassifier(n_estimators=200, random_state=RANDOM_SEED, n_jobs=-1)\n",
    "}\n",
    "\n",
    "comparison_results = []\n",
    "for name, model in models.items():\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=cv, scoring='roc_auc')\n",
    "    \n",
    "    # Fit and test\n",
    "    model.fit(X_train, y_train)\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        test_score = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "    else:\n",
    "        test_score = roc_auc_score(y_test, model.predict(X_test))\n",
    "    \n",
    "    comparison_results.append({\n",
    "        'Model': name,\n",
    "        'CV_Mean': cv_results.mean(),\n",
    "        'CV_Std': cv_results.std(),\n",
    "        'Test_Score': test_score\n",
    "    })\n",
    "\n",
    "final_comparison = pd.DataFrame(comparison_results).sort_values('CV_Mean', ascending=False)\n",
    "print(\"=== FINAL MODEL COMPARISON ===\")\n",
    "print(final_comparison.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.arange(len(final_comparison))\n",
    "ax.bar(x - 0.2, final_comparison['CV_Mean'], 0.4, label='CV Mean', alpha=0.8)\n",
    "ax.bar(x + 0.2, final_comparison['Test_Score'], 0.4, label='Test', alpha=0.8)\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('ROC-AUC')\n",
    "ax.set_title('Model Comparison: CV vs Test Performance')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(final_comparison['Model'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_model = final_comparison.iloc[0]['Model']\n",
    "print(f\"\\n\u2713 Champion model: {best_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The final comparison table ranks four models by CV ROC-AUC: Logistic Regression, Decision Tree (depth-5), Random Forest (100 trees), and Random Forest (200 trees). The bar chart displays CV mean vs. test score side by side.\n",
    "\n",
    "Typical ordering on the breast cancer dataset: the two Random Forest variants lead with ROC-AUC around **0.99**, followed closely by Logistic Regression at **0.98-0.99**, with the single Decision Tree trailing at **0.96-0.97**. The 200-tree forest is usually within **0.001-0.003** of the 100-tree forest, illustrating the diminishing returns of adding more trees.\n",
    "\n",
    "Notice the standard deviations: forests and logistic regression tend to have similarly low std (~0.01), while the single tree has noticeably higher std (~0.02-0.03). The CV-vs-test gap should be small for all models; a large discrepancy for any model would signal overfitting or an unusual test split.\n",
    "\n",
    "The champion model name is printed at the bottom. On this dataset the margin between the forest and logistic regression is often small, illustrating that a more complex model does not always provide a large lift over a well-tuned linear baseline.\n",
    "\n",
    "**Key takeaway:** Random Forests reliably improve over single trees and compete with linear models on well-structured datasets. The real payoff of forests comes on datasets with non-linear relationships and feature interactions where logistic regression struggles.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Wrap-Up: Key Takeaways\n\n### What We Learned Today:\n\n1. **Bagging Reduces Variance**: Averaging many trees stabilizes predictions\n2. **Random Forests**: Bagging + random feature selection = powerful ensemble\n3. **Hyperparameter Tuning**: n_estimators and max_features most important\n4. **OOB Score**: Free cross-validation estimate\n5. **Permutation Importance**: More reliable than built-in Gini importance\n\n### Critical Rules:\n\n> **\"More trees is almost always better (diminishing returns after 100-500)\"**\n\n> **\"Use permutation importance for production, not built-in importance\"**\n\n> **\"Random forests rarely overfit badly (but can underfit)\"**\n\n### Next Steps:\n\n- Next notebook: Gradient Boosting (sequential ensembles)\n- Boosting will give even better performance\n- But requires more careful tuning\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n\n- Breiman, L. (2001). \"Random Forests.\" *Machine Learning*, 45(1), 5-32.\n- James, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). *An Introduction to Statistical Learning with Python* - Tree-Based Methods (bagging/forests)\n- Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning* - Random forests and bagging\n- scikit-learn User Guide: [RandomForest estimators](https://scikit-learn.org/stable/modules/ensemble.html#forest)\n- scikit-learn User Guide: [Permutation importance](https://scikit-learn.org/stable/modules/permutation_importance.html)\n\n---\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "Thank you!\n",
    "\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}