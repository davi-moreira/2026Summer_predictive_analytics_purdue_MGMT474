{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fairness and Ethics Basics - Responsible Predictive Analytics\n",
    "\n",
    "<hr>\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/notebooks/figures/mgmt_474_ai_logo_02-modified.png\" width=\"200\"/>\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "# <center><a class=\"tocSkip\"></center>\n",
    "# <center>MGMT47400 Predictive Analytics</center>\n",
    "# <center>Professor: Davi Moreira </center>\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/blob/main/notebooks/17_fairness_slicing_model_cards.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Identify fairness risks and ethical failure modes in predictive systems\n",
    "2. Compute basic group fairness diagnostics (when sensitive attributes exist)\n",
    "3. Use slicing to detect performance disparities across segments\n",
    "4. Write a model card-style limitations and responsible-use section\n",
    "5. Apply responsible AI framing to the course project deliverable\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "This cell loads the standard data-wrangling and plotting libraries plus\n",
    "scikit-learn metrics we will need for sliced evaluation. We lock\n",
    "`RANDOM_SEED = 474` so that the synthetic data generation and group\n",
    "assignments are identical every time the notebook is run, making all\n",
    "fairness comparisons fully reproducible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nimport warnings\n\nwarnings.filterwarnings('ignore')\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (12, 6)\n\nRANDOM_SEED = 474\nnp.random.seed(RANDOM_SEED)\n\nprint(\"\u2713 Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The `Setup complete!` line confirms that all imports -- including\n",
    "`accuracy_score`, `precision_score`, `recall_score`, `f1_score`, and\n",
    "`confusion_matrix` -- loaded without error. The random seed is fixed at\n",
    "**474**, which means the synthetic group assignments and train/test splits\n",
    "will be identical on every run.\n",
    "\n",
    "**Why this matters:** Reproducibility is especially important in fairness\n",
    "analysis because small changes in the random split can flip the direction\n",
    "of a disparity, making conclusions unreliable.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fairness Vocabulary\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "**Disparity**: Systematic difference in outcomes across groups\n",
    "\n",
    "**Harm**: Negative impact on individuals or groups\n",
    "- **Allocation harm**: Withholding opportunities or resources\n",
    "- **Quality-of-service harm**: Worse performance for some groups\n",
    "- **Representational harm**: Stereotyping or denigration\n",
    "\n",
    "**Proxy variables**: Features correlated with sensitive attributes\n",
    "\n",
    "**Feedback loops**: Model decisions influence future data, potentially amplifying bias\n",
    "\n",
    "### Common Fairness Metrics:\n",
    "\n",
    "1. **Demographic Parity**: Equal selection rate across groups  \n",
    "   P(\u0177=1 | A=a) = P(\u0177=1 | A=b)\n",
    "\n",
    "2. **Equal Opportunity**: Equal true positive rate across groups  \n",
    "   P(\u0177=1 | y=1, A=a) = P(\u0177=1 | y=1, A=b)\n",
    "\n",
    "3. **Equalized Odds**: Equal TPR and FPR across groups\n",
    "\n",
    "\u26a0\ufe0f **Important**: These metrics often conflict with each other!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Synthetic Data with Group Attribute\n",
    "\n",
    "We use `make_classification` to create 2,000 samples with 15 features\n",
    "(10 informative, 5 redundant) and a 60/40 positive-negative class split.\n",
    "A synthetic `group` column is added **after** feature generation -- it is\n",
    "randomly assigned and deliberately **not** used as a training feature. This\n",
    "setup mirrors real-world scenarios where a sensitive attribute exists in the\n",
    "data but is excluded from the model, yet performance may still differ across\n",
    "groups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification data\n",
    "X, y = make_classification(\n",
    "    n_samples=2000, n_features=15, n_informative=10,\n",
    "    n_redundant=5, weights=[0.6, 0.4],\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Add synthetic \"group\" attribute (not used in training)\n",
    "# Group A: 60%, Group B: 40%\n",
    "group = np.random.choice(['Group_A', 'Group_B'], size=len(X), p=[0.6, 0.4])\n",
    "\n",
    "# Create DataFrame\n",
    "feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['target'] = y\n",
    "df['group'] = group\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nGroup distribution:\")\n",
    "print(df['group'].value_counts())\n",
    "print(f\"\\nTarget distribution by group:\")\n",
    "print(pd.crosstab(df['group'], df['target'], normalize='index'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The dataset has **2,000 rows** and **17 columns** (15 features + target +\n",
    "group). The group distribution should be approximately **60 % Group_A** and\n",
    "**40 % Group_B**, matching the probabilities we set. The cross-tabulation of\n",
    "target by group shows whether the positive-class base rate differs between\n",
    "groups -- if it does, even a perfect model will produce different selection\n",
    "rates, which is the root of many fairness dilemmas.\n",
    "\n",
    "**Key takeaway:** Understanding the base rate by group *before* modelling is\n",
    "essential. A difference in base rates does not automatically mean the model\n",
    "is unfair, but it does mean that equal selection rates and equal accuracy\n",
    "cannot both be achieved simultaneously.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Model (Without Using Group Attribute)\n",
    "\n",
    "We train a Random Forest on the 15 numeric features only -- the `group`\n",
    "column is deliberately excluded. The train/test split is stratified by group\n",
    "so both sets contain proportional representation. After training, we compute\n",
    "overall accuracy, precision, recall, and F1 on the test set as a baseline\n",
    "before drilling into group-level performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data (stratified by group to ensure representation)\n",
    "train_df, test_df = train_test_split(df, test_size=0.3, stratify=df['group'], random_state=RANDOM_SEED)\n",
    "\n",
    "# Separate features, target, and group\n",
    "X_train = train_df[feature_names]\n",
    "y_train = train_df['target']\n",
    "group_train = train_df['group']\n",
    "\n",
    "X_test = test_df[feature_names]\n",
    "y_test = test_df['target']\n",
    "group_test = test_df['group']\n",
    "\n",
    "# Train model WITHOUT group attribute\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=RANDOM_SEED, n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\n=== OVERALL MODEL PERFORMANCE ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1: {f1_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The overall **accuracy**, **precision**, **recall**, and **F1** are computed\n",
    "on the full test set without any group breakdown. These numbers set the\n",
    "baseline: they tell us how the model performs *on average*. A strong overall\n",
    "F1 does not guarantee that both groups are served equally well -- that is\n",
    "exactly what the sliced evaluation in the next section will reveal.\n",
    "\n",
    "**Why this matters:** Reporting only aggregate metrics is a common pitfall\n",
    "in responsible AI. Stakeholders deserve to know whether performance is\n",
    "uniform across the populations the model will affect.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Slice-Based Performance Evaluation\n",
    "\n",
    "### 5.1 Performance by Group\n",
    "\n",
    "Aggregate metrics can mask important disparities. Slicing the test set by\n",
    "`group` and computing accuracy, precision, recall, and F1 for each segment\n",
    "separately reveals whether the model serves all groups equally. Even when\n",
    "overall accuracy looks strong, one group may suffer noticeably lower recall\n",
    "or precision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics by group\n",
    "slice_results = []\n",
    "\n",
    "for grp in ['Group_A', 'Group_B']:\n",
    "    mask = group_test == grp\n",
    "    y_true_grp = y_test[mask]\n",
    "    y_pred_grp = y_pred[mask]\n",
    "    \n",
    "    slice_results.append({\n",
    "        'Group': grp,\n",
    "        'Sample_Size': len(y_true_grp),\n",
    "        'Accuracy': accuracy_score(y_true_grp, y_pred_grp),\n",
    "        'Precision': precision_score(y_true_grp, y_pred_grp, zero_division=0),\n",
    "        'Recall': recall_score(y_true_grp, y_pred_grp, zero_division=0),\n",
    "        'F1': f1_score(y_true_grp, y_pred_grp, zero_division=0)\n",
    "    })\n",
    "\n",
    "slice_df = pd.DataFrame(slice_results)\n",
    "\n",
    "print(\"\\n=== PERFORMANCE BY GROUP ===\")\n",
    "print(slice_df.to_string(index=False))\n",
    "\n",
    "# Compute disparities\n",
    "print(\"\\n=== PERFORMANCE DISPARITIES ===\")\n",
    "for metric in ['Accuracy', 'Precision', 'Recall', 'F1']:\n",
    "    gap = slice_df.loc[0, metric] - slice_df.loc[1, metric]\n",
    "    print(f\"{metric} Gap (A - B): {gap:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The **Performance by Group** table shows accuracy, precision, recall, and F1\n",
    "for Group_A and Group_B side by side. The **Performance Disparities** section\n",
    "computes the signed gap (A minus B) for each metric. A positive gap means\n",
    "Group_A scores higher; a negative gap means Group_B does. Pay special\n",
    "attention to the **Recall gap**: it tells you whether qualified individuals\n",
    "in one group are more likely to be missed.\n",
    "\n",
    "**Key takeaway:** Even small metric gaps (e.g., 0.03 in recall) can compound\n",
    "into large cumulative effects when the model runs on thousands of cases.\n",
    "Always compute both the gap magnitude and the sample sizes to assess\n",
    "practical significance.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance gaps\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, slice_df.loc[0, metrics], width, label='Group A', alpha=0.8)\n",
    "ax.bar(x + width/2, slice_df.loc[1, metrics], width, label='Group B', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Metric')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Model Performance by Group')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The grouped bar chart plots accuracy, precision, recall, and F1 side by\n",
    "side for Group_A and Group_B. Bars of similar height indicate comparable\n",
    "performance; a visible height difference on any metric flags a potential\n",
    "disparity worth investigating. Because the y-axis runs from 0 to 1, even a\n",
    "bar-height difference of a few percentage points is easy to spot.\n",
    "\n",
    "**Why this matters:** Visualisations make disparities tangible for\n",
    "non-technical stakeholders. A chart that shows one group's recall bar\n",
    "noticeably shorter than the other communicates the issue far more\n",
    "effectively than a table of numbers.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd PAUSE-AND-DO Exercise 1 (5 minutes)\n",
    "\n",
    "**Task:** Create slice performance table and highlight one disparity (if any).\n",
    "\n",
    "**Instructions:**\n",
    "1. Review the slice performance results above\n",
    "2. Identify the largest performance gap between groups\n",
    "3. Assess whether this gap is meaningful/concerning\n",
    "4. Propose one hypothesis for why the gap exists\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR DISPARITY ANALYSIS HERE:\n",
    "\n",
    "**Largest Disparity:**  \n",
    "[Which metric and how large?]\n",
    "\n",
    "**Is it Concerning?:**  \n",
    "[Your assessment with reasoning]\n",
    "\n",
    "**Hypothesis:**  \n",
    "[Why might this disparity exist?]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Fairness Metrics\n",
    "\n",
    "### 6.1 Selection Rate (Demographic Parity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute selection rates\n",
    "fairness_metrics = []\n",
    "\n",
    "for grp in ['Group_A', 'Group_B']:\n",
    "    mask = group_test == grp\n",
    "    y_pred_grp = y_pred[mask]\n",
    "    y_true_grp = y_test[mask]\n",
    "    \n",
    "    # Selection rate\n",
    "    selection_rate = y_pred_grp.mean()\n",
    "    \n",
    "    # Confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true_grp, y_pred_grp).ravel()\n",
    "    \n",
    "    # Rates\n",
    "    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0  # True Positive Rate\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0  # False Positive Rate\n",
    "    \n",
    "    fairness_metrics.append({\n",
    "        'Group': grp,\n",
    "        'Selection_Rate': selection_rate,\n",
    "        'TPR': tpr,\n",
    "        'FPR': fpr\n",
    "    })\n",
    "\n",
    "fairness_df = pd.DataFrame(fairness_metrics)\n",
    "\n",
    "print(\"\\n=== FAIRNESS METRICS ===\")\n",
    "print(fairness_df.to_string(index=False))\n",
    "\n",
    "# Compute fairness gaps\n",
    "print(\"\\n=== FAIRNESS GAPS ===\")\n",
    "sr_gap = fairness_df.loc[0, 'Selection_Rate'] - fairness_df.loc[1, 'Selection_Rate']\n",
    "tpr_gap = fairness_df.loc[0, 'TPR'] - fairness_df.loc[1, 'TPR']\n",
    "fpr_gap = fairness_df.loc[0, 'FPR'] - fairness_df.loc[1, 'FPR']\n",
    "\n",
    "print(f\"Selection Rate Gap: {sr_gap:+.4f}\")\n",
    "print(f\"TPR Gap (Equal Opportunity): {tpr_gap:+.4f}\")\n",
    "print(f\"FPR Gap: {fpr_gap:+.4f}\")\n",
    "\n",
    "print(\"\\n\u26a0\ufe0f Interpretation:\")\n",
    "print(\"  - Selection Rate: Are both groups selected at similar rates?\")\n",
    "print(\"  - TPR Gap: Do qualified individuals have equal chance across groups?\")\n",
    "print(\"  - FPR Gap: Are false positives distributed equally?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "Three fairness diagnostics are printed. The **Selection Rate** per group\n",
    "measures demographic parity: if both groups are selected (predicted positive)\n",
    "at the same rate, the Selection Rate Gap is near zero. The **TPR gap**\n",
    "captures equal opportunity: whether truly positive individuals are caught\n",
    "at equal rates regardless of group. The **FPR gap** rounds out equalized\n",
    "odds: whether truly negative individuals are falsely flagged at equal rates.\n",
    "\n",
    "**Key takeaway:** These three metrics often conflict -- reducing the\n",
    "selection-rate gap may widen the TPR gap, and vice versa. Document which\n",
    "metric your organisation prioritises and why.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Card Template\n",
    "\n",
    "### 7.1 Model Card Structure\n",
    "\n",
    "A Model Card is a short, structured document that accompanies a trained model\n",
    "and discloses its intended use, training data, performance metrics,\n",
    "limitations, and ethical considerations. The template below follows the\n",
    "format proposed by Mitchell et al. (2019). Filling it out forces you to\n",
    "think beyond accuracy and confront potential harms before deployment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Card: [Your Model Name]\n",
    "\n",
    "## Model Details\n",
    "- **Model Type**: Random Forest Classifier\n",
    "- **Version**: 1.0\n",
    "- **Date**: June 9, 2027\n",
    "- **Developed By**: [Your name/team]\n",
    "\n",
    "## Intended Use\n",
    "**Primary Use Cases:**\n",
    "- [Describe primary intended application]\n",
    "- [Context where model should be used]\n",
    "\n",
    "**Out-of-Scope Uses:**\n",
    "- [Contexts where model should NOT be used]\n",
    "- [Scenarios where model may fail or cause harm]\n",
    "\n",
    "## Training Data\n",
    "- **Source**: [Dataset name and source]\n",
    "- **Size**: [Number of samples]\n",
    "- **Time Period**: [When data was collected]\n",
    "- **Geography**: [Where data was collected]\n",
    "- **Preprocessing**: [Key preprocessing steps]\n",
    "\n",
    "## Evaluation Data\n",
    "- **Source**: [Test set details]\n",
    "- **Size**: [Number of samples]\n",
    "- **Split Method**: [How test set was created]\n",
    "\n",
    "## Metrics\n",
    "**Overall Performance:**\n",
    "- Accuracy: X.XX\n",
    "- Precision: X.XX\n",
    "- Recall: X.XX\n",
    "- F1: X.XX\n",
    "\n",
    "**Performance by Group:**\n",
    "- Group A: [Metrics]\n",
    "- Group B: [Metrics]\n",
    "- Performance Gap: [Description]\n",
    "\n",
    "## Limitations\n",
    "1. **Data Limitations**:\n",
    "   - [Describe data quality issues, biases, or gaps]\n",
    "   - [Temporal or geographic limitations]\n",
    "\n",
    "2. **Model Limitations**:\n",
    "   - [Known failure modes or weak segments]\n",
    "   - [Assumptions that may not hold]\n",
    "   - [Performance disparities across groups]\n",
    "\n",
    "3. **Deployment Limitations**:\n",
    "   - [Contexts where model should not be used]\n",
    "   - [Required human oversight]\n",
    "   - [Monitoring requirements]\n",
    "\n",
    "## Ethical Considerations\n",
    "**Potential Harms:**\n",
    "- [Allocation harms - who might be denied opportunities?]\n",
    "- [Quality-of-service harms - who might receive worse predictions?]\n",
    "- [Feedback loop risks - how might the model affect future data?]\n",
    "\n",
    "**Mitigation Strategies:**\n",
    "- [What steps are taken to reduce harm?]\n",
    "- [How are performance disparities addressed?]\n",
    "- [What oversight mechanisms exist?]\n",
    "\n",
    "## Recommendations\n",
    "1. **Usage Recommendations**:\n",
    "   - [How should model outputs be used?]\n",
    "   - [What human review is required?]\n",
    "\n",
    "2. **Monitoring**:\n",
    "   - [What metrics should be tracked over time?]\n",
    "   - [How often should model be re-evaluated?]\n",
    "\n",
    "3. **Update Schedule**:\n",
    "   - [When should model be retrained?]\n",
    "   - [What triggers retraining?]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd PAUSE-AND-DO Exercise 2 (5 minutes)\n",
    "\n",
    "**Task:** Draft a model card limitations section (6-8 lines).\n",
    "\n",
    "**Instructions:**\n",
    "1. Review the model card template above\n",
    "2. Write a limitations section for your project model\n",
    "3. Include data limitations, model limitations, and ethical considerations\n",
    "4. Be specific and evidence-based\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR MODEL CARD LIMITATIONS SECTION:\n",
    "\n",
    "## Limitations\n",
    "\n",
    "1. **Data Limitations**:  \n",
    "[Your text here - be specific]\n",
    "\n",
    "2. **Model Limitations**:  \n",
    "[Your text here - reference specific performance issues]\n",
    "\n",
    "3. **Ethical Considerations**:  \n",
    "[Your text here - discuss potential harms]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Wrap-Up: Key Takeaways\n",
    "\n",
    "### What We Learned Today:\n",
    "\n",
    "1. **Fairness Vocabulary**: Disparity, harm, proxies, feedback loops\n",
    "2. **Slice Evaluation**: Performance analysis by demographic groups\n",
    "3. **Fairness Metrics**: Selection rate, TPR/FPR gaps, equalized odds\n",
    "4. **Model Cards**: Structured documentation of model details and limitations\n",
    "5. **Responsible Communication**: Honest disclosure of limitations and risks\n",
    "\n",
    "### Responsible AI Best Practices:\n",
    "\n",
    "- \u2713 Always evaluate performance across relevant demographic groups\n",
    "- \u2713 Document known limitations and failure modes\n",
    "- \u2713 Consider both allocation and quality-of-service harms\n",
    "- \u2713 Be cautious about fairness claims - trade-offs are inevitable\n",
    "- \u2713 Plan for monitoring and regular re-evaluation\n",
    "- \u2713 Involve diverse stakeholders in evaluation and deployment decisions\n",
    "\n",
    "### Remember:\n",
    "\n",
    "> **\"Perfect fairness is impossible - but honest documentation of limitations is essential.\"**  \n",
    "> Focus on transparency, not perfection.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n\n- Barocas, S., Hardt, M., & Narayanan, A. (2019). *Fairness and Machine Learning*. [fairmlbook.org](http://fairmlbook.org/)\n- Hardt, M., Price, E., & Srebro, N. (2016). \"Equality of Opportunity in Supervised Learning.\" *NeurIPS*.\n- Mitchell, M., et al. (2019). \"Model Cards for Model Reporting.\" *FAT**.\n- Chouldechova, A. (2017). \"Fair prediction with disparate impact.\" *Big Data*.\n- Selbst, A.D., et al. (2019). \"Fairness and Abstraction in Sociotechnical Systems.\" *FAT**.\n\n---\n\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "Thank you!\n",
    "\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}