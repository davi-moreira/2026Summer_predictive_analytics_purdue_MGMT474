{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fairness and Ethics Basics - Responsible Predictive Analytics\n",
    "\n",
    "<hr>\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/notebooks/figures/mgmt_474_ai_logo_02-modified.png\" width=\"200\"/>\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "# <center><a class=\"tocSkip\"></center>\n",
    "# <center>MGMT47400 Predictive Analytics</center>\n",
    "# <center>Professor: Davi Moreira </center>\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/blob/main/notebooks/17_fairness_slicing_model_cards.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Identify fairness risks and ethical failure modes in predictive systems\n",
    "2. Compute basic group fairness diagnostics (when sensitive attributes exist)\n",
    "3. Use slicing to detect performance disparities across segments\n",
    "4. Write a model card-style limitations and responsible-use section\n",
    "5. Apply responsible AI framing to the course project deliverable\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nimport warnings\n\nwarnings.filterwarnings('ignore')\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (12, 6)\n\nRANDOM_SEED = 474\nnp.random.seed(RANDOM_SEED)\n\nprint(\"\u2713 Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fairness Vocabulary\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "**Disparity**: Systematic difference in outcomes across groups\n",
    "\n",
    "**Harm**: Negative impact on individuals or groups\n",
    "- **Allocation harm**: Withholding opportunities or resources\n",
    "- **Quality-of-service harm**: Worse performance for some groups\n",
    "- **Representational harm**: Stereotyping or denigration\n",
    "\n",
    "**Proxy variables**: Features correlated with sensitive attributes\n",
    "\n",
    "**Feedback loops**: Model decisions influence future data, potentially amplifying bias\n",
    "\n",
    "### Common Fairness Metrics:\n",
    "\n",
    "1. **Demographic Parity**: Equal selection rate across groups  \n",
    "   P(\u0177=1 | A=a) = P(\u0177=1 | A=b)\n",
    "\n",
    "2. **Equal Opportunity**: Equal true positive rate across groups  \n",
    "   P(\u0177=1 | y=1, A=a) = P(\u0177=1 | y=1, A=b)\n",
    "\n",
    "3. **Equalized Odds**: Equal TPR and FPR across groups\n",
    "\n",
    "\u26a0\ufe0f **Important**: These metrics often conflict with each other!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Synthetic Data with Group Attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification data\n",
    "X, y = make_classification(\n",
    "    n_samples=2000, n_features=15, n_informative=10,\n",
    "    n_redundant=5, weights=[0.6, 0.4],\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Add synthetic \"group\" attribute (not used in training)\n",
    "# Group A: 60%, Group B: 40%\n",
    "group = np.random.choice(['Group_A', 'Group_B'], size=len(X), p=[0.6, 0.4])\n",
    "\n",
    "# Create DataFrame\n",
    "feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['target'] = y\n",
    "df['group'] = group\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nGroup distribution:\")\n",
    "print(df['group'].value_counts())\n",
    "print(f\"\\nTarget distribution by group:\")\n",
    "print(pd.crosstab(df['group'], df['target'], normalize='index'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Model (Without Using Group Attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data (stratified by group to ensure representation)\n",
    "train_df, test_df = train_test_split(df, test_size=0.3, stratify=df['group'], random_state=RANDOM_SEED)\n",
    "\n",
    "# Separate features, target, and group\n",
    "X_train = train_df[feature_names]\n",
    "y_train = train_df['target']\n",
    "group_train = train_df['group']\n",
    "\n",
    "X_test = test_df[feature_names]\n",
    "y_test = test_df['target']\n",
    "group_test = test_df['group']\n",
    "\n",
    "# Train model WITHOUT group attribute\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=RANDOM_SEED, n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\n=== OVERALL MODEL PERFORMANCE ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1: {f1_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Slice-Based Performance Evaluation\n",
    "\n",
    "### 5.1 Performance by Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics by group\n",
    "slice_results = []\n",
    "\n",
    "for grp in ['Group_A', 'Group_B']:\n",
    "    mask = group_test == grp\n",
    "    y_true_grp = y_test[mask]\n",
    "    y_pred_grp = y_pred[mask]\n",
    "    \n",
    "    slice_results.append({\n",
    "        'Group': grp,\n",
    "        'Sample_Size': len(y_true_grp),\n",
    "        'Accuracy': accuracy_score(y_true_grp, y_pred_grp),\n",
    "        'Precision': precision_score(y_true_grp, y_pred_grp, zero_division=0),\n",
    "        'Recall': recall_score(y_true_grp, y_pred_grp, zero_division=0),\n",
    "        'F1': f1_score(y_true_grp, y_pred_grp, zero_division=0)\n",
    "    })\n",
    "\n",
    "slice_df = pd.DataFrame(slice_results)\n",
    "\n",
    "print(\"\\n=== PERFORMANCE BY GROUP ===\")\n",
    "print(slice_df.to_string(index=False))\n",
    "\n",
    "# Compute disparities\n",
    "print(\"\\n=== PERFORMANCE DISPARITIES ===\")\n",
    "for metric in ['Accuracy', 'Precision', 'Recall', 'F1']:\n",
    "    gap = slice_df.loc[0, metric] - slice_df.loc[1, metric]\n",
    "    print(f\"{metric} Gap (A - B): {gap:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance gaps\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, slice_df.loc[0, metrics], width, label='Group A', alpha=0.8)\n",
    "ax.bar(x + width/2, slice_df.loc[1, metrics], width, label='Group B', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Metric')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Model Performance by Group')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd PAUSE-AND-DO Exercise 1 (10 minutes)\n",
    "\n",
    "**Task:** Create slice performance table and highlight one disparity (if any).\n",
    "\n",
    "**Instructions:**\n",
    "1. Review the slice performance results above\n",
    "2. Identify the largest performance gap between groups\n",
    "3. Assess whether this gap is meaningful/concerning\n",
    "4. Propose one hypothesis for why the gap exists\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR DISPARITY ANALYSIS HERE:\n",
    "\n",
    "**Largest Disparity:**  \n",
    "[Which metric and how large?]\n",
    "\n",
    "**Is it Concerning?:**  \n",
    "[Your assessment with reasoning]\n",
    "\n",
    "**Hypothesis:**  \n",
    "[Why might this disparity exist?]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Fairness Metrics\n",
    "\n",
    "### 6.1 Selection Rate (Demographic Parity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute selection rates\n",
    "fairness_metrics = []\n",
    "\n",
    "for grp in ['Group_A', 'Group_B']:\n",
    "    mask = group_test == grp\n",
    "    y_pred_grp = y_pred[mask]\n",
    "    y_true_grp = y_test[mask]\n",
    "    \n",
    "    # Selection rate\n",
    "    selection_rate = y_pred_grp.mean()\n",
    "    \n",
    "    # Confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true_grp, y_pred_grp).ravel()\n",
    "    \n",
    "    # Rates\n",
    "    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0  # True Positive Rate\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0  # False Positive Rate\n",
    "    \n",
    "    fairness_metrics.append({\n",
    "        'Group': grp,\n",
    "        'Selection_Rate': selection_rate,\n",
    "        'TPR': tpr,\n",
    "        'FPR': fpr\n",
    "    })\n",
    "\n",
    "fairness_df = pd.DataFrame(fairness_metrics)\n",
    "\n",
    "print(\"\\n=== FAIRNESS METRICS ===\")\n",
    "print(fairness_df.to_string(index=False))\n",
    "\n",
    "# Compute fairness gaps\n",
    "print(\"\\n=== FAIRNESS GAPS ===\")\n",
    "sr_gap = fairness_df.loc[0, 'Selection_Rate'] - fairness_df.loc[1, 'Selection_Rate']\n",
    "tpr_gap = fairness_df.loc[0, 'TPR'] - fairness_df.loc[1, 'TPR']\n",
    "fpr_gap = fairness_df.loc[0, 'FPR'] - fairness_df.loc[1, 'FPR']\n",
    "\n",
    "print(f\"Selection Rate Gap: {sr_gap:+.4f}\")\n",
    "print(f\"TPR Gap (Equal Opportunity): {tpr_gap:+.4f}\")\n",
    "print(f\"FPR Gap: {fpr_gap:+.4f}\")\n",
    "\n",
    "print(\"\\n\u26a0\ufe0f Interpretation:\")\n",
    "print(\"  - Selection Rate: Are both groups selected at similar rates?\")\n",
    "print(\"  - TPR Gap: Do qualified individuals have equal chance across groups?\")\n",
    "print(\"  - FPR Gap: Are false positives distributed equally?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Card Template\n",
    "\n",
    "### 7.1 Model Card Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Card: [Your Model Name]\n",
    "\n",
    "## Model Details\n",
    "- **Model Type**: Random Forest Classifier\n",
    "- **Version**: 1.0\n",
    "- **Date**: June 9, 2027\n",
    "- **Developed By**: [Your name/team]\n",
    "\n",
    "## Intended Use\n",
    "**Primary Use Cases:**\n",
    "- [Describe primary intended application]\n",
    "- [Context where model should be used]\n",
    "\n",
    "**Out-of-Scope Uses:**\n",
    "- [Contexts where model should NOT be used]\n",
    "- [Scenarios where model may fail or cause harm]\n",
    "\n",
    "## Training Data\n",
    "- **Source**: [Dataset name and source]\n",
    "- **Size**: [Number of samples]\n",
    "- **Time Period**: [When data was collected]\n",
    "- **Geography**: [Where data was collected]\n",
    "- **Preprocessing**: [Key preprocessing steps]\n",
    "\n",
    "## Evaluation Data\n",
    "- **Source**: [Test set details]\n",
    "- **Size**: [Number of samples]\n",
    "- **Split Method**: [How test set was created]\n",
    "\n",
    "## Metrics\n",
    "**Overall Performance:**\n",
    "- Accuracy: X.XX\n",
    "- Precision: X.XX\n",
    "- Recall: X.XX\n",
    "- F1: X.XX\n",
    "\n",
    "**Performance by Group:**\n",
    "- Group A: [Metrics]\n",
    "- Group B: [Metrics]\n",
    "- Performance Gap: [Description]\n",
    "\n",
    "## Limitations\n",
    "1. **Data Limitations**:\n",
    "   - [Describe data quality issues, biases, or gaps]\n",
    "   - [Temporal or geographic limitations]\n",
    "\n",
    "2. **Model Limitations**:\n",
    "   - [Known failure modes or weak segments]\n",
    "   - [Assumptions that may not hold]\n",
    "   - [Performance disparities across groups]\n",
    "\n",
    "3. **Deployment Limitations**:\n",
    "   - [Contexts where model should not be used]\n",
    "   - [Required human oversight]\n",
    "   - [Monitoring requirements]\n",
    "\n",
    "## Ethical Considerations\n",
    "**Potential Harms:**\n",
    "- [Allocation harms - who might be denied opportunities?]\n",
    "- [Quality-of-service harms - who might receive worse predictions?]\n",
    "- [Feedback loop risks - how might the model affect future data?]\n",
    "\n",
    "**Mitigation Strategies:**\n",
    "- [What steps are taken to reduce harm?]\n",
    "- [How are performance disparities addressed?]\n",
    "- [What oversight mechanisms exist?]\n",
    "\n",
    "## Recommendations\n",
    "1. **Usage Recommendations**:\n",
    "   - [How should model outputs be used?]\n",
    "   - [What human review is required?]\n",
    "\n",
    "2. **Monitoring**:\n",
    "   - [What metrics should be tracked over time?]\n",
    "   - [How often should model be re-evaluated?]\n",
    "\n",
    "3. **Update Schedule**:\n",
    "   - [When should model be retrained?]\n",
    "   - [What triggers retraining?]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd PAUSE-AND-DO Exercise 2 (10 minutes)\n",
    "\n",
    "**Task:** Draft a model card limitations section (6-8 lines).\n",
    "\n",
    "**Instructions:**\n",
    "1. Review the model card template above\n",
    "2. Write a limitations section for your project model\n",
    "3. Include data limitations, model limitations, and ethical considerations\n",
    "4. Be specific and evidence-based\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR MODEL CARD LIMITATIONS SECTION:\n",
    "\n",
    "## Limitations\n",
    "\n",
    "1. **Data Limitations**:  \n",
    "[Your text here - be specific]\n",
    "\n",
    "2. **Model Limitations**:  \n",
    "[Your text here - reference specific performance issues]\n",
    "\n",
    "3. **Ethical Considerations**:  \n",
    "[Your text here - discuss potential harms]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Wrap-Up: Key Takeaways\n",
    "\n",
    "### What We Learned Today:\n",
    "\n",
    "1. **Fairness Vocabulary**: Disparity, harm, proxies, feedback loops\n",
    "2. **Slice Evaluation**: Performance analysis by demographic groups\n",
    "3. **Fairness Metrics**: Selection rate, TPR/FPR gaps, equalized odds\n",
    "4. **Model Cards**: Structured documentation of model details and limitations\n",
    "5. **Responsible Communication**: Honest disclosure of limitations and risks\n",
    "\n",
    "### Responsible AI Best Practices:\n",
    "\n",
    "- \u2713 Always evaluate performance across relevant demographic groups\n",
    "- \u2713 Document known limitations and failure modes\n",
    "- \u2713 Consider both allocation and quality-of-service harms\n",
    "- \u2713 Be cautious about fairness claims - trade-offs are inevitable\n",
    "- \u2713 Plan for monitoring and regular re-evaluation\n",
    "- \u2713 Involve diverse stakeholders in evaluation and deployment decisions\n",
    "\n",
    "### Remember:\n",
    "\n",
    "> **\"Perfect fairness is impossible - but honest documentation of limitations is essential.\"**  \n",
    "> Focus on transparency, not perfection.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n\n- Barocas, S., Hardt, M., & Narayanan, A. (2019). *Fairness and Machine Learning*. [fairmlbook.org](http://fairmlbook.org/)\n- Hardt, M., Price, E., & Srebro, N. (2016). \"Equality of Opportunity in Supervised Learning.\" *NeurIPS*.\n- Mitchell, M., et al. (2019). \"Model Cards for Model Reporting.\" *FAT**.\n- Chouldechova, A. (2017). \"Fair prediction with disparate impact.\" *Big Data*.\n- Selbst, A.D., et al. (2019). \"Fairness and Abstraction in Sociotechnical Systems.\" *FAT**.\n\n---\n\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "Thank you!\n",
    "\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}