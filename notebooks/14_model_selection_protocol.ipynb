{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 14: Model Selection and Comparison - Making the Call Like a Professional\n",
    "\n",
    "**MGMT 47400 - Predictive Analytics**  \n",
    "**4-Week Online Course**  \n",
    "**Day 14 - Friday June 4, 2027**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/blob/main/notebooks/14_model_selection_protocol.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Build a standardized model comparison workflow (same CV, same metric)\n",
    "2. Use multiple metrics without \"metric shopping\"\n",
    "3. Select a champion model and justify it (performance, stability, interpretability, cost)\n",
    "4. Create a reproducible experiment log table\n",
    "5. Prepare project improved-model plan for submission\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import make_scorer, roc_auc_score, accuracy_score, f1_score\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.precision', 4)\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "print(\"‚úì Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Model Selection Problem\n",
    "\n",
    "### Common Mistakes\n",
    "\n",
    "**‚ùå Wrong:**\n",
    "- Comparing models on different data splits\n",
    "- Using different metrics for different models\n",
    "- Choosing model based on test set performance\n",
    "- \"Metric shopping\" (trying metrics until one model wins)\n",
    "- Ignoring computational cost\n",
    "- Not documenting why a model was chosen\n",
    "\n",
    "**‚úì Right:**\n",
    "- Same cross-validation folds for all models\n",
    "- Single primary metric (with supporting metrics)\n",
    "- Select based on CV, validate on test\n",
    "- Document selection criteria upfront\n",
    "- Consider multiple factors (performance, stability, cost, interpretability)\n",
    "- Create reproducible experiment logs\n",
    "\n",
    "### Fair Comparison Protocol\n",
    "\n",
    "```\n",
    "1. Define evaluation criteria BEFORE training\n",
    "   - Primary metric\n",
    "   - Supporting metrics\n",
    "   - Constraints (time, interpretability, etc.)\n",
    "\n",
    "2. Use SAME cross-validation for all models\n",
    "   - Same CV object\n",
    "   - Same random seed\n",
    "   - Same data\n",
    "\n",
    "3. Compare on PRIMARY metric\n",
    "   - Use supporting metrics to break ties\n",
    "   - Document tradeoffs\n",
    "\n",
    "4. Select champion, THEN evaluate on test\n",
    "   - Test set touched ONCE\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = load_breast_cancer(as_frame=True)\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y)\n",
    "\n",
    "print(f\"Train: {len(X_train)} | Test: {len(X_test)} (LOCKED until final selection)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Model Comparison Harness\n",
    "\n",
    "### Comparison Harness Function\n",
    "\n",
    "A harness ensures fair comparison by:\n",
    "- Using same CV folds\n",
    "- Evaluating same metrics\n",
    "- Tracking fit/score times\n",
    "- Returning structured results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models_comprehensive(models_dict, X, y, cv, scoring_metrics, primary_metric='roc_auc'):\n",
    "    \"\"\"\n",
    "    Comprehensive model comparison with multiple metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    models_dict : dict\n",
    "        Dictionary of {name: model} pairs\n",
    "    X, y : array-like\n",
    "        Training data\n",
    "    cv : cross-validator\n",
    "        CV splitter (SAME for all models)\n",
    "    scoring_metrics : list\n",
    "        List of metric names to evaluate\n",
    "    primary_metric : str\n",
    "        Primary metric for ranking\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : Comparison table\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for name, model in models_dict.items():\n",
    "        print(f\"Evaluating: {name}...\")\n",
    "        \n",
    "        # Time the evaluation\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Cross-validate with multiple metrics\n",
    "        cv_results = cross_validate(\n",
    "            model, X, y, cv=cv,\n",
    "            scoring=scoring_metrics,\n",
    "            return_train_score=True,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        # Build result row\n",
    "        row = {'Model': name}\n",
    "        \n",
    "        # Add metrics\n",
    "        for metric in scoring_metrics:\n",
    "            test_scores = cv_results[f'test_{metric}']\n",
    "            train_scores = cv_results[f'train_{metric}']\n",
    "            row[f'{metric}_cv_mean'] = test_scores.mean()\n",
    "            row[f'{metric}_cv_std'] = test_scores.std()\n",
    "            row[f'{metric}_train_mean'] = train_scores.mean()\n",
    "        \n",
    "        # Add timing\n",
    "        row['fit_time_mean'] = cv_results['fit_time'].mean()\n",
    "        row['total_cv_time'] = elapsed_time\n",
    "        \n",
    "        # Overfitting gap for primary metric\n",
    "        row['overfit_gap'] = row[f'{primary_metric}_train_mean'] - row[f'{primary_metric}_cv_mean']\n",
    "        \n",
    "        results.append(row)\n",
    "    \n",
    "    # Create DataFrame and sort by primary metric\n",
    "    df = pd.DataFrame(results).sort_values(f'{primary_metric}_cv_mean', ascending=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"‚úì Comparison harness ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Candidate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define candidate models\n",
    "candidate_models = {\n",
    "    'Logistic Regression': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', LogisticRegression(C=1.0, random_state=RANDOM_SEED, max_iter=1000))\n",
    "    ]),\n",
    "    \n",
    "    'Logistic (L1)': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', LogisticRegression(C=0.1, penalty='l1', solver='liblinear', random_state=RANDOM_SEED))\n",
    "    ]),\n",
    "    \n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=RANDOM_SEED),\n",
    "    \n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=200, max_depth=10, random_state=RANDOM_SEED, n_jobs=-1\n",
    "    ),\n",
    "    \n",
    "    'Gradient Boosting': GradientBoostingClassifier(\n",
    "        n_estimators=100, learning_rate=0.1, max_depth=3, random_state=RANDOM_SEED\n",
    "    )\n",
    "}\n",
    "\n",
    "print(f\"Candidate models: {len(candidate_models)}\")\n",
    "for name in candidate_models.keys():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù PAUSE-AND-DO Exercise 1 (10 minutes)\n",
    "\n",
    "**Task:** Implement the comparison harness for 3 candidate models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation setup\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "metrics = ['roc_auc', 'accuracy', 'f1']\n",
    "primary_metric = 'roc_auc'\n",
    "\n",
    "print(\"=== EVALUATION PROTOCOL ===\")\n",
    "print(f\"Cross-validation: {cv}\")\n",
    "print(f\"Primary metric: {primary_metric}\")\n",
    "print(f\"Supporting metrics: {[m for m in metrics if m != primary_metric]}\")\n",
    "print(f\"\\nRunning comparison...\\n\")\n",
    "\n",
    "# Run comparison\n",
    "comparison_df = compare_models_comprehensive(\n",
    "    candidate_models, X_train, y_train, cv, metrics, primary_metric\n",
    ")\n",
    "\n",
    "print(\"\\n=== COMPREHENSIVE MODEL COMPARISON ===\")\n",
    "display_cols = ['Model', 'roc_auc_cv_mean', 'roc_auc_cv_std', 'accuracy_cv_mean', \n",
    "                'f1_cv_mean', 'overfit_gap', 'total_cv_time']\n",
    "print(comparison_df[display_cols].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Metric Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize metric comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# ROC-AUC\n",
    "axes[0, 0].barh(comparison_df['Model'], comparison_df['roc_auc_cv_mean'], \n",
    "                xerr=comparison_df['roc_auc_cv_std'], alpha=0.7)\n",
    "axes[0, 0].set_xlabel('ROC-AUC')\n",
    "axes[0, 0].set_title('ROC-AUC (Primary Metric)')\n",
    "axes[0, 0].invert_yaxis()\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 1].barh(comparison_df['Model'], comparison_df['accuracy_cv_mean'],\n",
    "                xerr=comparison_df['accuracy_cv_std'], alpha=0.7, color='orange')\n",
    "axes[0, 1].set_xlabel('Accuracy')\n",
    "axes[0, 1].set_title('Accuracy')\n",
    "axes[0, 1].invert_yaxis()\n",
    "\n",
    "# F1\n",
    "axes[1, 0].barh(comparison_df['Model'], comparison_df['f1_cv_mean'],\n",
    "                xerr=comparison_df['f1_cv_std'], alpha=0.7, color='green')\n",
    "axes[1, 0].set_xlabel('F1 Score')\n",
    "axes[1, 0].set_title('F1 Score')\n",
    "axes[1, 0].invert_yaxis()\n",
    "\n",
    "# Training time\n",
    "axes[1, 1].barh(comparison_df['Model'], comparison_df['total_cv_time'], alpha=0.7, color='red')\n",
    "axes[1, 1].set_xlabel('Time (seconds)')\n",
    "axes[1, 1].set_title('Total CV Time')\n",
    "axes[1, 1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Look for consistency across metrics\")\n",
    "print(\"üí° Consider time/performance tradeoffs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Champion Selection Memo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select champion\n",
    "champion_row = comparison_df.iloc[0]\n",
    "runner_up_row = comparison_df.iloc[1]\n",
    "\n",
    "print(\"=== CHAMPION SELECTION MEMO ===\")\n",
    "print(f\"\\nSelected Model: {champion_row['Model']}\")\n",
    "print(f\"\\nPrimary Metric ({primary_metric}):\")\n",
    "print(f\"  Champion: {champion_row['roc_auc_cv_mean']:.4f} ¬± {champion_row['roc_auc_cv_std']:.4f}\")\n",
    "print(f\"  Runner-up ({runner_up_row['Model']}): {runner_up_row['roc_auc_cv_mean']:.4f} ¬± {runner_up_row['roc_auc_cv_std']:.4f}\")\n",
    "print(f\"  Advantage: {(champion_row['roc_auc_cv_mean'] - runner_up_row['roc_auc_cv_mean']):.4f}\")\n",
    "\n",
    "print(f\"\\nSupporting Evidence:\")\n",
    "print(f\"  Accuracy: {champion_row['accuracy_cv_mean']:.4f}\")\n",
    "print(f\"  F1: {champion_row['f1_cv_mean']:.4f}\")\n",
    "print(f\"  Overfit gap: {champion_row['overfit_gap']:.4f}\")\n",
    "print(f\"  CV time: {champion_row['total_cv_time']:.2f}s\")\n",
    "\n",
    "print(f\"\\nJustification:\")\n",
    "print(f\"  1. Best {primary_metric} by {(champion_row['roc_auc_cv_mean'] - runner_up_row['roc_auc_cv_mean'])*100:.2f} percentage points\")\n",
    "print(f\"  2. Reasonable overfitting ({champion_row['overfit_gap']:.4f} gap)\")\n",
    "print(f\"  3. Acceptable training time ({champion_row['total_cv_time']:.1f}s for 5-fold CV)\")\n",
    "print(f\"  4. Consistent performance across folds (std = {champion_row['roc_auc_cv_std']:.4f})\")\n",
    "\n",
    "print(f\"\\nRisks/Limitations:\")\n",
    "if 'Gradient Boosting' in champion_row['Model'] or 'Random Forest' in champion_row['Model']:\n",
    "    print(f\"  - Less interpretable than linear models\")\n",
    "    print(f\"  - Requires feature importance analysis for explanations\")\n",
    "if champion_row['total_cv_time'] > 10:\n",
    "    print(f\"  - Longer training time may impact iteration speed\")\n",
    "if champion_row['overfit_gap'] > 0.05:\n",
    "    print(f\"  - Moderate overfitting detected - monitor on new data\")\n",
    "\n",
    "print(f\"\\nRecommendation: Proceed with {champion_row['Model']} for final evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù PAUSE-AND-DO Exercise 2 (10 minutes)\n",
    "\n",
    "**Task:** Write a champion selection memo (5 bullets + 1 risk).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR CHAMPION SELECTION MEMO:\n",
    "\n",
    "**Selected Champion:** [Model name]\n",
    "\n",
    "**Supporting Evidence:**\n",
    "1. [Primary metric performance]\n",
    "2. [Supporting metric 1]\n",
    "3. [Supporting metric 2]\n",
    "4. [Stability (std)]\n",
    "5. [Other consideration]\n",
    "\n",
    "**Key Risk:**\n",
    "[Most important limitation or risk]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train champion on full training set\n",
    "champion_model = candidate_models[champion_row['Model']]\n",
    "champion_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on test set (FIRST AND ONLY TIME)\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, classification_report\n",
    "\n",
    "y_pred_test = champion_model.predict(X_test)\n",
    "y_proba_test = champion_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "test_roc_auc = roc_auc_score(y_test, y_proba_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "test_f1 = f1_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"=== FINAL TEST SET EVALUATION ===\")\n",
    "print(f\"\\nChampion: {champion_row['Model']}\")\n",
    "print(f\"\\nTest Set Performance:\")\n",
    "print(f\"  ROC-AUC: {test_roc_auc:.4f}\")\n",
    "print(f\"  Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"  F1: {test_f1:.4f}\")\n",
    "\n",
    "print(f\"\\nCV vs Test Comparison:\")\n",
    "print(f\"  CV ROC-AUC: {champion_row['roc_auc_cv_mean']:.4f} ¬± {champion_row['roc_auc_cv_std']:.4f}\")\n",
    "print(f\"  Test ROC-AUC: {test_roc_auc:.4f}\")\n",
    "print(f\"  Difference: {abs(test_roc_auc - champion_row['roc_auc_cv_mean']):.4f}\")\n",
    "\n",
    "if abs(test_roc_auc - champion_row['roc_auc_cv_mean']) < 2 * champion_row['roc_auc_cv_std']:\n",
    "    print(f\"\\n‚úì Test performance within expected range (< 2 std from CV mean)\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Test performance differs from CV - investigate further\")\n",
    "\n",
    "print(f\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_test, target_names=data.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Experiment Log Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reproducible experiment log\n",
    "experiment_log = comparison_df.copy()\n",
    "\n",
    "# Add metadata\n",
    "experiment_log['date'] = pd.Timestamp.now().strftime('%Y-%m-%d')\n",
    "experiment_log['dataset'] = 'breast_cancer'\n",
    "experiment_log['n_samples'] = len(X_train)\n",
    "experiment_log['n_features'] = X_train.shape[1]\n",
    "experiment_log['cv_folds'] = cv.n_splits\n",
    "experiment_log['random_seed'] = RANDOM_SEED\n",
    "\n",
    "# Mark champion\n",
    "experiment_log['is_champion'] = experiment_log['Model'] == champion_row['Model']\n",
    "\n",
    "# Add test score for champion\n",
    "experiment_log['test_roc_auc'] = experiment_log['Model'].apply(\n",
    "    lambda x: test_roc_auc if x == champion_row['Model'] else np.nan\n",
    ")\n",
    "\n",
    "print(\"=== EXPERIMENT LOG ===\")\n",
    "log_cols = ['date', 'Model', 'roc_auc_cv_mean', 'roc_auc_cv_std', 'test_roc_auc', 'is_champion']\n",
    "print(experiment_log[log_cols].to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "# experiment_log.to_csv('model_experiments.csv', index=False)\n",
    "print(\"\\nüí° Experiment log ready for version control\")\n",
    "print(\"üí° Track all experiments for reproducibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Wrap-Up: Key Takeaways\n",
    "\n",
    "### What We Learned Today:\n",
    "\n",
    "1. **Fair Comparison**: Same CV, same metrics, same data\n",
    "2. **Multi-Metric Evaluation**: Primary metric + supporting evidence\n",
    "3. **Champion Selection**: Documented, justified, reproducible\n",
    "4. **Test Discipline**: Touch test set once, after selection\n",
    "5. **Experiment Logging**: Track everything for reproducibility\n",
    "\n",
    "### Critical Rules:\n",
    "\n",
    "> **\"Same CV folds for all models, always\"**\n",
    "\n",
    "> **\"Select on CV, validate on test (once)\"**\n",
    "\n",
    "> **\"Document selection criteria before training\"**\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Day 15: Model interpretation + error analysis + Project Milestone 3\n",
    "- We'll interpret the champion model\n",
    "- Deliver improved model for project\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "- James, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). *An Introduction to Statistical Learning with Python* - Model Assessment and Selection (protocols for fair comparison)\n",
    "- Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning* - Selection bias and repeated peeking hazards\n",
    "- scikit-learn User Guide: [Model evaluation](https://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "- scikit-learn User Guide: [Parameter tuning best practices](https://scikit-learn.org/stable/modules/grid_search.html)\n",
    "\n",
    "---\n",
    "\n",
    "**End of Day 14 Notebook**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
