{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection and Comparison - Making the Call Like a Professional\n",
    "\n",
    "<hr>\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/notebooks/figures/mgmt_474_ai_logo_02-modified.png\" width=\"200\"/>\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "# <center><a class=\"tocSkip\"></center>\n",
    "# <center>MGMT47400 Predictive Analytics</center>\n",
    "# <center>Professor: Davi Moreira </center>\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/blob/main/notebooks/14_model_selection_protocol.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Build a standardized model comparison workflow (same CV, same metric)\n",
    "2. Use multiple metrics without \"metric shopping\"\n",
    "3. Select a champion model and justify it (performance, stability, interpretability, cost)\n",
    "4. Create a reproducible experiment log table\n",
    "5. Prepare project improved-model plan for submission\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import cross_validate, StratifiedKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import make_scorer, roc_auc_score, accuracy_score, f1_score\nimport time\nimport warnings\n\nwarnings.filterwarnings('ignore')\npd.set_option('display.precision', 4)\nRANDOM_SEED = 474\nnp.random.seed(RANDOM_SEED)\nprint(\"\u2713 Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The setup cell imports `cross_validate` (not `cross_val_score`) because we need to evaluate **multiple metrics** simultaneously and retrieve training scores alongside test scores. The `make_scorer` import is available for custom metrics, though we will use scikit-learn's built-in scoring strings here.\n",
    "\n",
    "The `time` module is imported to measure wall-clock training time for each model, which becomes an important selection criterion when models have similar accuracy but very different computational costs.\n",
    "\n",
    "**Key takeaway:** This notebook shifts focus from \"how do I build a model\" to \"how do I systematically choose between models.\" The imports reflect that shift: the emphasis is on evaluation infrastructure rather than new algorithms.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Model Selection Problem\n",
    "\n",
    "### Common Mistakes\n",
    "\n",
    "**\u274c Wrong:**\n",
    "- Comparing models on different data splits\n",
    "- Using different metrics for different models\n",
    "- Choosing model based on test set performance\n",
    "- \"Metric shopping\" (trying metrics until one model wins)\n",
    "- Ignoring computational cost\n",
    "- Not documenting why a model was chosen\n",
    "\n",
    "**\u2713 Right:**\n",
    "- Same cross-validation folds for all models\n",
    "- Single primary metric (with supporting metrics)\n",
    "- Select based on CV, validate on test\n",
    "- Document selection criteria upfront\n",
    "- Consider multiple factors (performance, stability, cost, interpretability)\n",
    "- Create reproducible experiment logs\n",
    "\n",
    "### Fair Comparison Protocol\n",
    "\n",
    "```\n",
    "1. Define evaluation criteria BEFORE training\n",
    "   - Primary metric\n",
    "   - Supporting metrics\n",
    "   - Constraints (time, interpretability, etc.)\n",
    "\n",
    "2. Use SAME cross-validation for all models\n",
    "   - Same CV object\n",
    "   - Same random seed\n",
    "   - Same data\n",
    "\n",
    "3. Compare on PRIMARY metric\n",
    "   - Use supporting metrics to break ties\n",
    "   - Document tradeoffs\n",
    "\n",
    "4. Select champion, THEN evaluate on test\n",
    "   - Test set touched ONCE\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = load_breast_cancer(as_frame=True)\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y)\n",
    "\n",
    "print(f\"Train: {len(X_train)} | Test: {len(X_test)} (LOCKED until final selection)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The data is split into **455 training** and **114 test** samples using an 80/20 split with stratification. Note that this notebook uses `test_size=0.2` rather than 0.3, giving slightly more training data. The test set is explicitly labeled as \"LOCKED until final selection\" to reinforce the discipline of not peeking.\n",
    "\n",
    "The breast cancer dataset has **30 features** and a roughly 63/37 benign-to-malignant class distribution, which is preserved in both splits through stratification.\n",
    "\n",
    "**Why this matters:** The test set lockdown is the single most important methodological point in this notebook. Every model will be evaluated solely on cross-validation during the selection phase. The test set exists only to provide a final, unbiased generalization estimate after the champion is chosen.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build Model Comparison Harness\n",
    "\n",
    "### Comparison Harness Function\n",
    "\n",
    "A harness ensures fair comparison by:\n",
    "- Using same CV folds\n",
    "- Evaluating same metrics\n",
    "- Tracking fit/score times\n",
    "- Returning structured results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models_comprehensive(models_dict, X, y, cv, scoring_metrics, primary_metric='roc_auc'):\n",
    "    \"\"\"\n",
    "    Comprehensive model comparison with multiple metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    models_dict : dict\n",
    "        Dictionary of {name: model} pairs\n",
    "    X, y : array-like\n",
    "        Training data\n",
    "    cv : cross-validator\n",
    "        CV splitter (SAME for all models)\n",
    "    scoring_metrics : list\n",
    "        List of metric names to evaluate\n",
    "    primary_metric : str\n",
    "        Primary metric for ranking\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : Comparison table\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for name, model in models_dict.items():\n",
    "        print(f\"Evaluating: {name}...\")\n",
    "        \n",
    "        # Time the evaluation\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Cross-validate with multiple metrics\n",
    "        cv_results = cross_validate(\n",
    "            model, X, y, cv=cv,\n",
    "            scoring=scoring_metrics,\n",
    "            return_train_score=True,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        # Build result row\n",
    "        row = {'Model': name}\n",
    "        \n",
    "        # Add metrics\n",
    "        for metric in scoring_metrics:\n",
    "            test_scores = cv_results[f'test_{metric}']\n",
    "            train_scores = cv_results[f'train_{metric}']\n",
    "            row[f'{metric}_cv_mean'] = test_scores.mean()\n",
    "            row[f'{metric}_cv_std'] = test_scores.std()\n",
    "            row[f'{metric}_train_mean'] = train_scores.mean()\n",
    "        \n",
    "        # Add timing\n",
    "        row['fit_time_mean'] = cv_results['fit_time'].mean()\n",
    "        row['total_cv_time'] = elapsed_time\n",
    "        \n",
    "        # Overfitting gap for primary metric\n",
    "        row['overfit_gap'] = row[f'{primary_metric}_train_mean'] - row[f'{primary_metric}_cv_mean']\n",
    "        \n",
    "        results.append(row)\n",
    "    \n",
    "    # Create DataFrame and sort by primary metric\n",
    "    df = pd.DataFrame(results).sort_values(f'{primary_metric}_cv_mean', ascending=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"\u2713 Comparison harness ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The cell defines the `compare_models_comprehensive()` function and prints `Comparison harness ready`. No models have been evaluated yet; this cell only sets up the infrastructure.\n",
    "\n",
    "The function accepts a dictionary of named models, a CV splitter, and a list of scoring metrics. For each model it calls `cross_validate` with `return_train_score=True` to capture both train and validation performance, computes the overfit gap, and records wall-clock time. The results are returned as a sorted DataFrame with the best model on the primary metric at the top.\n",
    "\n",
    "Using a single harness function guarantees that every model sees the exact same CV folds, the same metrics, and the same timing methodology. This eliminates the most common source of unfair model comparison: inconsistent evaluation setups.\n",
    "\n",
    "**Key takeaway:** Writing a comparison harness function once and reusing it is a professional best practice. It prevents subtle bugs (e.g., accidentally using different CV objects for different models) that can silently invalidate your conclusions.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Candidate Models\n",
    "\n",
    "A fair comparison requires defining all candidate models **before** seeing any results, just as a clinical trial registers its protocol before collecting data. This prevents \"model shopping,\" where you keep trying new models until one happens to score well on this particular data split.\n",
    "\n",
    "Our candidate pool spans the spectrum from simple to complex: two logistic regression variants (standard L2 and sparse L1), a single decision tree, a Random Forest, and Gradient Boosting. Each model is wrapped in a scikit-learn `Pipeline` where necessary so that preprocessing (e.g., scaling for logistic regression) is included in the cross-validation loop and does not leak information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define candidate models\n",
    "candidate_models = {\n",
    "    'Logistic Regression': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', LogisticRegression(C=1.0, random_state=RANDOM_SEED, max_iter=1000))\n",
    "    ]),\n",
    "    \n",
    "    'Logistic (L1)': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', LogisticRegression(C=0.1, penalty='l1', solver='liblinear', random_state=RANDOM_SEED))\n",
    "    ]),\n",
    "    \n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=RANDOM_SEED),\n",
    "    \n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=200, max_depth=10, random_state=RANDOM_SEED, n_jobs=-1\n",
    "    ),\n",
    "    \n",
    "    'Gradient Boosting': GradientBoostingClassifier(\n",
    "        n_estimators=100, learning_rate=0.1, max_depth=3, random_state=RANDOM_SEED\n",
    "    )\n",
    "}\n",
    "\n",
    "print(f\"Candidate models: {len(candidate_models)}\")\n",
    "for name in candidate_models.keys():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "Five candidate models are printed: **Logistic Regression** (L2 regularization, C=1.0), **Logistic (L1)** (L1/Lasso regularization, C=0.1 for aggressive sparsity), **Decision Tree** (max_depth=5), **Random Forest** (200 trees, max_depth=10), and **Gradient Boosting** (100 trees, learning rate 0.1, max_depth=3).\n",
    "\n",
    "The logistic regression models are wrapped in `Pipeline` objects with `StandardScaler` so that feature scaling happens inside each CV fold, preventing data leakage. The tree-based models do not need scaling because decision trees are invariant to monotone transformations of features.\n",
    "\n",
    "Having both L1 and L2 logistic regression lets us test whether feature sparsity helps. L1 will zero out irrelevant features, potentially improving generalization when many of the 30 breast cancer features are redundant.\n",
    "\n",
    "**Key takeaway:** Define your candidate pool before seeing any results. Resist the temptation to add or remove models after peeking at initial scores.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd PAUSE-AND-DO Exercise 1 (10 minutes)\n",
    "\n",
    "**Task:** Implement the comparison harness for 3 candidate models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation setup\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "metrics = ['roc_auc', 'accuracy', 'f1']\n",
    "primary_metric = 'roc_auc'\n",
    "\n",
    "print(\"=== EVALUATION PROTOCOL ===\")\n",
    "print(f\"Cross-validation: {cv}\")\n",
    "print(f\"Primary metric: {primary_metric}\")\n",
    "print(f\"Supporting metrics: {[m for m in metrics if m != primary_metric]}\")\n",
    "print(f\"\\nRunning comparison...\\n\")\n",
    "\n",
    "# Run comparison\n",
    "comparison_df = compare_models_comprehensive(\n",
    "    candidate_models, X_train, y_train, cv, metrics, primary_metric\n",
    ")\n",
    "\n",
    "print(\"\\n=== COMPREHENSIVE MODEL COMPARISON ===\")\n",
    "display_cols = ['Model', 'roc_auc_cv_mean', 'roc_auc_cv_std', 'accuracy_cv_mean', \n",
    "                'f1_cv_mean', 'overfit_gap', 'total_cv_time']\n",
    "print(comparison_df[display_cols].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The evaluation protocol is printed first: 5-fold `StratifiedKFold` with `roc_auc` as the primary metric and `accuracy` and `f1` as supporting metrics. Then the harness evaluates each model and prints its name as it runs.\n",
    "\n",
    "The comprehensive comparison table shows all five models ranked by CV ROC-AUC. Typical results: ensemble models (RF and GBM) lead with CV ROC-AUC near **0.99**, Logistic Regression follows closely at **0.98-0.99**, L1 Logistic may be slightly lower due to aggressive sparsity, and the Decision Tree trails at **0.95-0.97**.\n",
    "\n",
    "The `overfit_gap` column is critical: it shows how much each model's training performance exceeds its CV performance. A gap above **0.05** signals potential overfitting. Ensemble methods typically have moderate gaps (0.01-0.03), while the Decision Tree may show a larger gap.\n",
    "\n",
    "The `total_cv_time` column reveals computational costs. Logistic Regression and Decision Trees are typically under 0.1 seconds, while Random Forest and Gradient Boosting take 0.5-2 seconds for 5-fold CV on this small dataset. These differences scale dramatically on larger datasets.\n",
    "\n",
    "**Why this matters:** This table is the centerpiece of the model selection process. Every subsequent decision (champion selection, test evaluation, deployment) flows from this single, fair, multi-metric comparison.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Metric Reporting\n",
    "\n",
    "Evaluating models on a single metric can be misleading. ROC-AUC measures ranking ability, accuracy captures overall correctness, and F1 balances precision and recall. A model that maximizes ROC-AUC might not maximize accuracy if the classification threshold is suboptimal. Reporting all three metrics (plus training time) gives a multi-dimensional view of each candidate.\n",
    "\n",
    "The 2x2 visualization below plots each metric as a horizontal bar chart with error bars representing cross-validation standard deviation. The bottom-right panel adds training time so you can spot cases where a marginal performance gain comes at a steep computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize metric comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# ROC-AUC\n",
    "axes[0, 0].barh(comparison_df['Model'], comparison_df['roc_auc_cv_mean'], \n",
    "                xerr=comparison_df['roc_auc_cv_std'], alpha=0.7)\n",
    "axes[0, 0].set_xlabel('ROC-AUC')\n",
    "axes[0, 0].set_title('ROC-AUC (Primary Metric)')\n",
    "axes[0, 0].invert_yaxis()\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 1].barh(comparison_df['Model'], comparison_df['accuracy_cv_mean'],\n",
    "                xerr=comparison_df['accuracy_cv_std'], alpha=0.7, color='orange')\n",
    "axes[0, 1].set_xlabel('Accuracy')\n",
    "axes[0, 1].set_title('Accuracy')\n",
    "axes[0, 1].invert_yaxis()\n",
    "\n",
    "# F1\n",
    "axes[1, 0].barh(comparison_df['Model'], comparison_df['f1_cv_mean'],\n",
    "                xerr=comparison_df['f1_cv_std'], alpha=0.7, color='green')\n",
    "axes[1, 0].set_xlabel('F1 Score')\n",
    "axes[1, 0].set_title('F1 Score')\n",
    "axes[1, 0].invert_yaxis()\n",
    "\n",
    "# Training time\n",
    "axes[1, 1].barh(comparison_df['Model'], comparison_df['total_cv_time'], alpha=0.7, color='red')\n",
    "axes[1, 1].set_xlabel('Time (seconds)')\n",
    "axes[1, 1].set_title('Total CV Time')\n",
    "axes[1, 1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\ud83d\udca1 Look for consistency across metrics\")\n",
    "print(\"\ud83d\udca1 Consider time/performance tradeoffs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The 2x2 panel displays four perspectives on the same five models. The **top-left** (ROC-AUC) is the primary metric; models are ranked by this. The **top-right** (Accuracy) often tells a similar story but can diverge when class imbalance matters. The **bottom-left** (F1 Score) balances precision and recall, important when false negatives have different costs than false positives. The **bottom-right** (Training Time) shows computational cost.\n",
    "\n",
    "Look for consistency: a model that ranks first in ROC-AUC but third in F1 may have a threshold calibration issue. Error bars (horizontal lines at each bar's tip) show cross-validation variability. A model with overlapping error bars against another model is not statistically distinguishable on that metric.\n",
    "\n",
    "If two models have nearly identical ROC-AUC, the supporting metrics and training time break the tie. A Logistic Regression that scores 0.985 in 0.05 seconds may be preferable to a Gradient Boosting model that scores 0.990 in 1.5 seconds, depending on deployment constraints.\n",
    "\n",
    "**Key takeaway:** Multi-metric visualization prevents \"tunnel vision\" on a single number. Always check that your champion is robust across multiple evaluation criteria before committing.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Champion Selection Memo\n",
    "\n",
    "Selecting a champion model is not just about picking the highest number; it is a documented business decision. The memo format below forces you to state the primary metric, quantify the margin over the runner-up, assess stability (std), check for overfitting (train-vs-CV gap), and flag risks. This structure ensures that the selection can be reviewed, challenged, and reproduced by a colleague.\n",
    "\n",
    "In industry, model selection memos become part of the audit trail. Regulators, compliance teams, and business stakeholders can trace exactly why a particular model was deployed, making the process transparent and defensible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select champion\n",
    "champion_row = comparison_df.iloc[0]\n",
    "runner_up_row = comparison_df.iloc[1]\n",
    "\n",
    "print(\"=== CHAMPION SELECTION MEMO ===\")\n",
    "print(f\"\\nSelected Model: {champion_row['Model']}\")\n",
    "print(f\"\\nPrimary Metric ({primary_metric}):\")\n",
    "print(f\"  Champion: {champion_row['roc_auc_cv_mean']:.4f} \u00b1 {champion_row['roc_auc_cv_std']:.4f}\")\n",
    "print(f\"  Runner-up ({runner_up_row['Model']}): {runner_up_row['roc_auc_cv_mean']:.4f} \u00b1 {runner_up_row['roc_auc_cv_std']:.4f}\")\n",
    "print(f\"  Advantage: {(champion_row['roc_auc_cv_mean'] - runner_up_row['roc_auc_cv_mean']):.4f}\")\n",
    "\n",
    "print(f\"\\nSupporting Evidence:\")\n",
    "print(f\"  Accuracy: {champion_row['accuracy_cv_mean']:.4f}\")\n",
    "print(f\"  F1: {champion_row['f1_cv_mean']:.4f}\")\n",
    "print(f\"  Overfit gap: {champion_row['overfit_gap']:.4f}\")\n",
    "print(f\"  CV time: {champion_row['total_cv_time']:.2f}s\")\n",
    "\n",
    "print(f\"\\nJustification:\")\n",
    "print(f\"  1. Best {primary_metric} by {(champion_row['roc_auc_cv_mean'] - runner_up_row['roc_auc_cv_mean'])*100:.2f} percentage points\")\n",
    "print(f\"  2. Reasonable overfitting ({champion_row['overfit_gap']:.4f} gap)\")\n",
    "print(f\"  3. Acceptable training time ({champion_row['total_cv_time']:.1f}s for 5-fold CV)\")\n",
    "print(f\"  4. Consistent performance across folds (std = {champion_row['roc_auc_cv_std']:.4f})\")\n",
    "\n",
    "print(f\"\\nRisks/Limitations:\")\n",
    "if 'Gradient Boosting' in champion_row['Model'] or 'Random Forest' in champion_row['Model']:\n",
    "    print(f\"  - Less interpretable than linear models\")\n",
    "    print(f\"  - Requires feature importance analysis for explanations\")\n",
    "if champion_row['total_cv_time'] > 10:\n",
    "    print(f\"  - Longer training time may impact iteration speed\")\n",
    "if champion_row['overfit_gap'] > 0.05:\n",
    "    print(f\"  - Moderate overfitting detected - monitor on new data\")\n",
    "\n",
    "print(f\"\\nRecommendation: Proceed with {champion_row['Model']} for final evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The champion selection memo is a structured report. It names the winning model, quantifies the margin over the runner-up on the primary metric (ROC-AUC), and provides supporting evidence from accuracy, F1, overfit gap, and CV time.\n",
    "\n",
    "The **Justification** section gives four numbered arguments for the selection. Look at argument 2 (overfitting gap): a gap below **0.02** is excellent, meaning the model's training performance is close to its validation performance. Argument 4 (fold stability) reports the standard deviation; values below **0.01** indicate very consistent performance across data subsets.\n",
    "\n",
    "The **Risks/Limitations** section is equally important. If the champion is an ensemble model, the memo flags reduced interpretability. If training time is high, it flags iteration speed. These risks must be weighed against the performance advantage.\n",
    "\n",
    "The final recommendation to \"Proceed with [Model] for final evaluation\" explicitly gates the next step: only after writing this memo should you touch the test set.\n",
    "\n",
    "**Why this matters:** In professional settings, this memo is the document that a manager, stakeholder, or regulator reviews. It transforms a subjective model choice into a transparent, evidence-based decision.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd PAUSE-AND-DO Exercise 2 (10 minutes)\n",
    "\n",
    "**Task:** Write a champion selection memo (5 bullets + 1 risk).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR CHAMPION SELECTION MEMO:\n",
    "\n",
    "**Selected Champion:** [Model name]\n",
    "\n",
    "**Supporting Evidence:**\n",
    "1. [Primary metric performance]\n",
    "2. [Supporting metric 1]\n",
    "3. [Supporting metric 2]\n",
    "4. [Stability (std)]\n",
    "5. [Other consideration]\n",
    "\n",
    "**Key Risk:**\n",
    "[Most important limitation or risk]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final Test Set Evaluation\n",
    "\n",
    "The test set has been locked away since the initial data split. We now touch it **exactly once** to get an unbiased estimate of the champion model's real-world performance. This single-use discipline is critical: if you peek at the test set multiple times during model selection, you are implicitly optimizing for it and the final estimate will be overly optimistic.\n",
    "\n",
    "After evaluation, we compare the test score to the CV estimate. If the two agree (the test score falls within roughly two standard deviations of the CV mean), we have strong evidence that the model generalizes well. A large discrepancy in either direction warrants investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train champion on full training set\n",
    "champion_model = candidate_models[champion_row['Model']]\n",
    "champion_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on test set (FIRST AND ONLY TIME)\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, classification_report\n",
    "\n",
    "y_pred_test = champion_model.predict(X_test)\n",
    "y_proba_test = champion_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "test_roc_auc = roc_auc_score(y_test, y_proba_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "test_f1 = f1_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"=== FINAL TEST SET EVALUATION ===\")\n",
    "print(f\"\\nChampion: {champion_row['Model']}\")\n",
    "print(f\"\\nTest Set Performance:\")\n",
    "print(f\"  ROC-AUC: {test_roc_auc:.4f}\")\n",
    "print(f\"  Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"  F1: {test_f1:.4f}\")\n",
    "\n",
    "print(f\"\\nCV vs Test Comparison:\")\n",
    "print(f\"  CV ROC-AUC: {champion_row['roc_auc_cv_mean']:.4f} \u00b1 {champion_row['roc_auc_cv_std']:.4f}\")\n",
    "print(f\"  Test ROC-AUC: {test_roc_auc:.4f}\")\n",
    "print(f\"  Difference: {abs(test_roc_auc - champion_row['roc_auc_cv_mean']):.4f}\")\n",
    "\n",
    "if abs(test_roc_auc - champion_row['roc_auc_cv_mean']) < 2 * champion_row['roc_auc_cv_std']:\n",
    "    print(f\"\\n\u2713 Test performance within expected range (< 2 std from CV mean)\")\n",
    "else:\n",
    "    print(f\"\\n\u26a0\ufe0f Test performance differs from CV - investigate further\")\n",
    "\n",
    "print(f\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_test, target_names=data.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The final evaluation reports three metrics on the 114 held-out test samples: **ROC-AUC**, **Accuracy**, and **F1**. These numbers represent the model's best estimate of real-world performance, untainted by any selection bias.\n",
    "\n",
    "The **CV vs Test Comparison** section is the key diagnostic. If the test ROC-AUC falls within two standard deviations of the CV mean (i.e., the check mark message appears), the model generalizes as expected. On the breast cancer dataset, typical test ROC-AUC is around **0.98-1.00**, consistent with the high CV scores.\n",
    "\n",
    "The detailed **Classification Report** breaks down precision, recall, and F1 for each class (malignant and benign). Pay special attention to the malignant class (the minority class): a model that achieves 99 % accuracy but misses 10 % of malignant cases may be unacceptable in a clinical setting. The recall for malignant tells you the sensitivity of the model.\n",
    "\n",
    "**Key takeaway:** The test set evaluation is the final checkpoint. If the test score is dramatically lower than the CV estimate, something went wrong in the selection process (data leakage, overfitting to CV, or an unusual test split). If it agrees, you can confidently report this performance to stakeholders.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Experiment Log Template\n",
    "\n",
    "Professional data science teams track every model experiment in a structured log: dataset metadata, model name, hyperparameters, CV scores, test scores, and timestamps. This experiment log serves three purposes: (1) it prevents re-running experiments you have already tried, (2) it enables reproducibility months later, and (3) it provides evidence for model governance and audit.\n",
    "\n",
    "The code below creates a DataFrame that captures all of this information in one table. In practice, you would append each new experiment to a persistent CSV or database, building a complete history of your modeling efforts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reproducible experiment log\n",
    "experiment_log = comparison_df.copy()\n",
    "\n",
    "# Add metadata\n",
    "experiment_log['date'] = pd.Timestamp.now().strftime('%Y-%m-%d')\n",
    "experiment_log['dataset'] = 'breast_cancer'\n",
    "experiment_log['n_samples'] = len(X_train)\n",
    "experiment_log['n_features'] = X_train.shape[1]\n",
    "experiment_log['cv_folds'] = cv.n_splits\n",
    "experiment_log['random_seed'] = RANDOM_SEED\n",
    "\n",
    "# Mark champion\n",
    "experiment_log['is_champion'] = experiment_log['Model'] == champion_row['Model']\n",
    "\n",
    "# Add test score for champion\n",
    "experiment_log['test_roc_auc'] = experiment_log['Model'].apply(\n",
    "    lambda x: test_roc_auc if x == champion_row['Model'] else np.nan\n",
    ")\n",
    "\n",
    "print(\"=== EXPERIMENT LOG ===\")\n",
    "log_cols = ['date', 'Model', 'roc_auc_cv_mean', 'roc_auc_cv_std', 'test_roc_auc', 'is_champion']\n",
    "print(experiment_log[log_cols].to_string(index=False))\n",
    "\n",
    "# Save to CSV\n",
    "# experiment_log.to_csv('model_experiments.csv', index=False)\n",
    "print(\"\\n\ud83d\udca1 Experiment log ready for version control\")\n",
    "print(\"\ud83d\udca1 Track all experiments for reproducibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The experiment log table shows one row per model with columns for date, model name, CV ROC-AUC (mean and std), test ROC-AUC (only for the champion), and a boolean `is_champion` flag. Metadata columns record the dataset name, sample count (**455** training samples), feature count (**30**), CV folds (**5**), and random seed (**474**).\n",
    "\n",
    "Only the champion model has a test score; all other rows show `NaN` for `test_roc_auc`. This is deliberate: non-champion models were never evaluated on the test set, preserving the single-use discipline.\n",
    "\n",
    "In a real project, you would append this log to a persistent file (CSV, database, or experiment tracking tool like MLflow) after every modeling session. Over weeks of work, the log accumulates all experiments, making it easy to answer questions like \"Did we already try L1 regularization with C=0.01?\" or \"What was our best score three weeks ago?\"\n",
    "\n",
    "**Key takeaway:** The experiment log is the institutional memory of a data science project. Without it, teams waste time re-running experiments and lose track of what has been tried.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Wrap-Up: Key Takeaways\n\n### What We Learned Today:\n\n1. **Fair Comparison**: Same CV, same metrics, same data\n2. **Multi-Metric Evaluation**: Primary metric + supporting evidence\n3. **Champion Selection**: Documented, justified, reproducible\n4. **Test Discipline**: Touch test set once, after selection\n5. **Experiment Logging**: Track everything for reproducibility\n\n### Critical Rules:\n\n> **\"Same CV folds for all models, always\"**\n\n> **\"Select on CV, validate on test (once)\"**\n\n> **\"Document selection criteria before training\"**\n\n### Next Steps:\n\n- Next notebook: Model interpretation + error analysis + Project Milestone 3\n- We'll interpret the champion model\n- Deliver improved model for project\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n\n- James, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). *An Introduction to Statistical Learning with Python* - Model Assessment and Selection (protocols for fair comparison)\n- Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning* - Selection bias and repeated peeking hazards\n- scikit-learn User Guide: [Model evaluation](https://scikit-learn.org/stable/modules/model_evaluation.html)\n- scikit-learn User Guide: [Parameter tuning best practices](https://scikit-learn.org/stable/modules/grid_search.html)\n\n---\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "Thank you!\n",
    "\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}