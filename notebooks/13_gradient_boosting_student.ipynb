{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting - Performance with Discipline (and Leakage Avoidance)\n",
    "\n",
    "<hr>\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/notebooks/figures/mgmt_474_ai_logo_02-modified.png\" width=\"200\"/>\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "# <center><a class=\"tocSkip\"></center>\n",
    "# <center>MGMT47400 Predictive Analytics</center>\n",
    "# <center>Professor: Davi Moreira </center>\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/blob/main/notebooks/13_gradient_boosting_student.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Explain boosting vs bagging at a high level\n",
    "2. Train a gradient boosting model with sensible defaults\n",
    "3. Tune learning rate / depth / estimators with runtime controls\n",
    "4. Compare boosted model vs forest under consistent CV\n",
    "5. Identify and control overfitting in boosting\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "> **ðŸ“‹ Participation Reminder:** This notebook contains **2 PAUSE-AND-DO exercises**. You are expected to complete all exercises before submitting your notebook.\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## ðŸ’¼ Why This Matters: Learning from Mistakes, One Tree at a Time\n\nThe **State Health Department** wants to build a second-opinion system â€” an automated tool that flags cases where the primary model and a boosting model disagree, triggering a senior pathologist review. Unlike random forests (which build trees independently), gradient boosting builds them sequentially: each new tree focuses specifically on the cases the previous trees got wrong.\n\nThis \"learn from your mistakes\" strategy often produces the most accurate models in practice. Companies like Google, Netflix, and major healthcare systems rely on gradient boosting for their most demanding prediction tasks.\n\n> **Today's focus:** Understanding the boosting paradigm, building GradientBoosting and HistGradientBoosting classifiers, and comparing them against random forests on the breast cancer dataset.\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, RandomizedSearchCV\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom scipy.stats import randint, uniform\nimport warnings\n\nwarnings.filterwarnings('ignore')\npd.set_option('display.precision', 4)\nRANDOM_SEED = 474\nnp.random.seed(RANDOM_SEED)\nprint(\"âœ“ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The setup cell imports `GradientBoostingClassifier` from scikit-learn's ensemble module and `RandomizedSearchCV` for efficient hyperparameter tuning. The `scipy.stats` distributions (`randint`, `uniform`) define the random search spaces we will use later.\n",
    "\n",
    "The confirmation message `Setup complete!` with **RANDOM_SEED = 474** means all stochastic operations (data splits, random search iterations, bootstrap samples inside the booster) will produce identical results every time you run this notebook.\n",
    "\n",
    "**Key takeaway:** Gradient Boosting is deterministic given a fixed seed, unlike some implementations of random forests that use parallel randomness. This makes debugging easier but also means a single seed can give atypically good or bad results, which is why we always use cross-validation.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Boosting vs Bagging\n",
    "\n",
    "### Bagging (Random Forest)\n",
    "- **Strategy**: Train many trees independently on bootstrap samples\n",
    "- **Combine**: Average (or vote) predictions\n",
    "- **Effect**: Reduces variance\n",
    "- **Trees**: Can be deep (low bias, high variance)\n",
    "- **Parallelizable**: Yes\n",
    "\n",
    "### Boosting (Gradient Boosting)\n",
    "- **Strategy**: Train trees sequentially, each correcting previous errors\n",
    "- **Combine**: Weighted sum of predictions\n",
    "- **Effect**: Reduces bias (and some variance)\n",
    "- **Trees**: Should be shallow (reduce variance of individual trees)\n",
    "- **Parallelizable**: Not really (sequential by nature)\n",
    "\n",
    "### Gradient Boosting Algorithm (Simplified)\n",
    "\n",
    "```\n",
    "1. Start with initial prediction (e.g., mean)\n",
    "2. For iteration m = 1 to M:\n",
    "   a. Calculate residuals (errors) from current model\n",
    "   b. Fit shallow tree to predict residuals\n",
    "   c. Add tree to model with learning rate Î·:\n",
    "      F_m(x) = F_(m-1)(x) + Î· * tree_m(x)\n",
    "3. Final model: F_M(x) = sum of all trees\n",
    "```\n",
    "\n",
    "**Key insight**: Each tree tries to fix mistakes of previous trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Gemini Prompt:** \"Load Breast Cancer dataset, split 70/30 with stratification (seed 474), print sizes.\"\n",
    ">\n",
    "> **After running, verify:**\n",
    "> - Train ~398 samples, test ~171\n",
    "> - Target is binary (0/1)\n",
    "> - Split uses stratify=y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = load_breast_cancer(as_frame=True)\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RANDOM_SEED, stratify=y)\n",
    "\n",
    "print(f\"Train: {len(X_train)} | Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The data is split into **398 training** and **171 test** samples with stratification, identical to the split used in the previous notebooks. Keeping the same split across notebooks ensures that performance numbers are directly comparable, so when we say \"GBM beats RF by 0.005,\" that comparison is on the exact same data.\n",
    "\n",
    "**Why this matters:** Consistent data splits across notebooks are essential for the cumulative model comparison table we will build in the model selection notebook. If each notebook used a different random split, the comparisons would be meaningless.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline Gradient Boosting\n",
    "\n",
    "Before tuning anything, we train a `GradientBoostingClassifier` with scikit-learn's defaults: 100 trees, learning rate 0.1, and max depth 3. These defaults are surprisingly reasonable for many datasets and serve as our performance anchor. We also train a Random Forest under the same cross-validation to establish a direct baseline.\n",
    "\n",
    "The comparison below answers a practical question: does gradient boosting out-of-the-box beat the Random Forest we tuned in the previous notebook? On the breast cancer dataset (569 samples, 30 features), the two methods often perform within a percentage point of each other, but boosting's sequential error-correction mechanism can edge ahead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Gemini Prompt:** \"Train GradientBoostingClassifier (defaults: 100 trees, lr=0.1, depth=3) and RandomForest(100 trees). Compare using 5-fold stratified CV ROC-AUC.\"\n",
    ">\n",
    "> **After running, verify:**\n",
    "> - Both models report CV ROC-AUC mean and std\n",
    "> - GBM advantage over RF is printed\n",
    "> - StratifiedKFold uses shuffle=True, seed=474\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple GBM with defaults\n",
    "gbm = GradientBoostingClassifier(random_state=RANDOM_SEED)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "gbm_scores = cross_val_score(gbm, X_train, y_train, cv=cv, scoring='roc_auc')\n",
    "\n",
    "print(\"=== GRADIENT BOOSTING (defaults) ===\")\n",
    "print(f\"Default params:\")\n",
    "print(f\"  n_estimators: 100\")\n",
    "print(f\"  learning_rate: 0.1\")\n",
    "print(f\"  max_depth: 3\")\n",
    "print(f\"\\nCV ROC-AUC: {gbm_scores.mean():.4f} Â± {gbm_scores.std():.4f}\")\n",
    "\n",
    "# Compare to Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=RANDOM_SEED, n_jobs=-1)\n",
    "rf_scores = cross_val_score(rf, X_train, y_train, cv=cv, scoring='roc_auc')\n",
    "\n",
    "print(f\"\\n=== RANDOM FOREST (100 trees) ===\")\n",
    "print(f\"CV ROC-AUC: {rf_scores.mean():.4f} Â± {rf_scores.std():.4f}\")\n",
    "\n",
    "print(f\"\\n=== COMPARISON ===\")\n",
    "print(f\"GBM advantage: {(gbm_scores.mean() - rf_scores.mean()):.4f}\")\n",
    "print(\"\\nðŸ’¡ GBM often outperforms RF (but needs more tuning)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The output shows two blocks: GBM defaults (100 trees, learning rate 0.1, max depth 3) and Random Forest (100 trees). Both report 5-fold stratified CV ROC-AUC with mean and standard deviation.\n",
    "\n",
    "On the breast cancer dataset, default GBM typically achieves a CV ROC-AUC around **0.99**, while the 100-tree Random Forest scores around **0.98-0.99**. The GBM advantage line at the bottom quantifies the lift, often in the range of **0.002-0.010**. A positive value means boosting outperforms bagging out of the box.\n",
    "\n",
    "The standard deviations are worth comparing too. GBM's std is often similar to or slightly larger than RF's, meaning boosting is not necessarily more stable across folds. This is expected: boosting reduces bias but does not explicitly target variance reduction the way bagging does.\n",
    "\n",
    "**Key takeaway:** Default Gradient Boosting is a strong out-of-the-box performer. The fact that it matches or beats a 100-tree Random Forest with no tuning makes it a compelling first choice for structured tabular data.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Learning Rate Tradeoff\n",
    "\n",
    "### Learning Rate (Î·, or `learning_rate`)\n",
    "\n",
    "**High learning rate (e.g., 0.3)**\n",
    "- âœ“ Faster training (need fewer trees)\n",
    "- âœ— Risk of overfitting\n",
    "- âœ— May miss optimal solution\n",
    "\n",
    "**Low learning rate (e.g., 0.01)**\n",
    "- âœ“ Better final performance (usually)\n",
    "- âœ“ More stable\n",
    "- âœ— Needs many more trees\n",
    "- âœ— Slower training\n",
    "\n",
    "**Rule of thumb**: Lower learning rate + more trees = better results (but longer training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Gemini Prompt:** \"Sweep learning rates [0.01,0.05,0.1,0.2,0.5] for GBM with 100 trees. Collect CV ROC-AUC and plot with error bars (log-scale x-axis).\"\n",
    ">\n",
    "> **After running, verify:**\n",
    "> - Table has 5 rows with lr, cv_mean, cv_std\n",
    "> - Plot uses log x-axis with error bars\n",
    "> - Lower LRs are competitive with higher ones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate sweep\n",
    "learning_rates = [0.01, 0.05, 0.1, 0.2, 0.5]\n",
    "lr_results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    gbm_lr = GradientBoostingClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=lr,\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    scores = cross_val_score(gbm_lr, X_train, y_train, cv=cv, scoring='roc_auc')\n",
    "    lr_results.append({\n",
    "        'learning_rate': lr,\n",
    "        'cv_mean': scores.mean(),\n",
    "        'cv_std': scores.std()\n",
    "    })\n",
    "\n",
    "lr_df = pd.DataFrame(lr_results)\n",
    "print(\"=== LEARNING RATE SWEEP (100 trees) ===\")\n",
    "print(lr_df.to_string(index=False))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(lr_df['learning_rate'], lr_df['cv_mean'], \n",
    "             yerr=lr_df['cv_std'], marker='o', capsize=5, linewidth=2)\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('CV ROC-AUC')\n",
    "plt.title('Effect of Learning Rate (100 trees)')\n",
    "plt.xscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Lower learning rate often performs better\")\n",
    "print(\"ðŸ’¡ But needs to compensate with more trees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The table shows CV ROC-AUC for five learning rates (0.01, 0.05, 0.1, 0.2, 0.5), all with 100 trees. At **learning_rate=0.01**, the model underfits because 100 trees are not enough to compensate for the tiny step size. Performance peaks around **0.05-0.1** and may decline at **0.5** where each tree overshoots the residuals.\n",
    "\n",
    "The log-scale plot reveals a characteristic inverted-U shape: too slow (left) means underfitting with 100 trees, too fast (right) means overfitting. The sweet spot is in the middle. Note that the curve shape depends on `n_estimators`: with 500 trees, the optimal learning rate would shift leftward because more trees compensate for smaller steps.\n",
    "\n",
    "**Why this matters:** Learning rate is the most important hyperparameter in gradient boosting. The rule of thumb is: set it as low as your compute budget allows, then increase `n_estimators` until validation performance plateaus.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Number of Trees vs Learning Rate\n",
    "\n",
    "Learning rate and number of trees are tightly coupled in gradient boosting. A lower learning rate means each tree contributes less, so you need more trees to reach the same performance level. The tradeoff is computation: 500 trees at learning rate 0.01 takes roughly 5x longer than 100 trees at 0.1, but often yields a better final model because the optimization path is smoother.\n",
    "\n",
    "The grid below tests four (n_estimators, learning_rate) configurations to illustrate this tradeoff. Look for the configuration that achieves the highest CV ROC-AUC; it is typically one of the low-learning-rate, high-tree-count combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Gemini Prompt:** \"Test four GBM configs pairing (n_estimators, lr): (50,0.1), (100,0.1), (200,0.05), (500,0.01). Evaluate with 5-fold CV ROC-AUC.\"\n",
    ">\n",
    "> **After running, verify:**\n",
    "> - Table has 4 rows showing n_estimators, lr, cv_mean, cv_std\n",
    "> - More trees + lower LR matches/beats fewer trees + higher LR\n",
    "> - Same CV object used for fair comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: low LR with more trees\n",
    "configs = [\n",
    "    {'n_estimators': 50, 'learning_rate': 0.1},\n",
    "    {'n_estimators': 100, 'learning_rate': 0.1},\n",
    "    {'n_estimators': 200, 'learning_rate': 0.05},\n",
    "    {'n_estimators': 500, 'learning_rate': 0.01},\n",
    "]\n",
    "\n",
    "config_results = []\n",
    "for config in configs:\n",
    "    gbm = GradientBoostingClassifier(random_state=RANDOM_SEED, **config)\n",
    "    scores = cross_val_score(gbm, X_train, y_train, cv=cv, scoring='roc_auc')\n",
    "    config_results.append({\n",
    "        'n_estimators': config['n_estimators'],\n",
    "        'learning_rate': config['learning_rate'],\n",
    "        'cv_mean': scores.mean(),\n",
    "        'cv_std': scores.std()\n",
    "    })\n",
    "\n",
    "config_df = pd.DataFrame(config_results)\n",
    "print(\"=== LEARNING RATE + N_ESTIMATORS COMBINATIONS ===\")\n",
    "print(config_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nðŸ’¡ More trees with lower learning rate often wins\")\n",
    "print(\"ðŸ’¡ But training time increases significantly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The table tests four specific configurations that illustrate the learning-rate / n_estimators tradeoff. The (50 trees, 0.1) configuration is the fastest but usually lowest-performing. The (500 trees, 0.01) configuration is the slowest but often achieves the highest CV ROC-AUC.\n",
    "\n",
    "Look at the middle rows: (100 trees, 0.1) and (200 trees, 0.05) often produce nearly identical performance, confirming that you can trade learning rate for tree count. The product `n_estimators * learning_rate` is sometimes called the \"effective budget\"; configurations with similar budgets tend to perform similarly.\n",
    "\n",
    "Training time differences become noticeable here: 500 trees takes about 5x longer than 100 trees. On the small breast cancer dataset this is still under a second, but on larger datasets the time difference can be minutes versus seconds.\n",
    "\n",
    "**Key takeaway:** For a final production model, invest in more trees with a smaller learning rate. For rapid prototyping, use fewer trees with a moderate learning rate (0.1) as a quick approximation.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ PAUSE-AND-DO Exercise 1 (5 minutes)\n",
    "\n",
    "**Task:** Train baseline GBM and compare against RF under CV.\n",
    "\n",
    "Already done above! Now analyze:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANALYSIS:\n",
    "\n",
    "**Performance Comparison:**  \n",
    "[Which performs better: GBM or RF? By how much?]\n",
    "\n",
    "**Training Time:**  \n",
    "[Estimate: which is faster to train?]\n",
    "\n",
    "**When to use each:**  \n",
    "[When would you choose GBM over RF?]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tuning Gradient Boosting\n",
    "\n",
    "### Key Hyperparameters\n",
    "\n",
    "1. **n_estimators**: Number of boosting stages\n",
    "2. **learning_rate**: Shrinkage parameter (Î·)\n",
    "3. **max_depth**: Maximum tree depth (keep low: 3-5)\n",
    "4. **min_samples_split**: Minimum samples to split\n",
    "5. **subsample**: Fraction of samples for fitting each tree (stochastic GB)\n",
    "\n",
    "### Tuning Strategy\n",
    "\n",
    "**Use RandomizedSearchCV with constraints:**\n",
    "- Limit search space to reasonable values\n",
    "- Use early stopping to save time\n",
    "- Monitor overfitting carefully"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Gemini Prompt:** \"Run RandomizedSearchCV (20 iters) on GBM: n_estimators(100-300), lr(0.01-0.2), max_depth(3-6), min_samples_split(2-19), subsample(0.7-1.0). Print best params and test ROC-AUC.\"\n",
    ">\n",
    "> **After running, verify:**\n",
    "> - Best parameters printed as dict\n",
    "> - Best CV ROC-AUC reported\n",
    "> - Test score computed with predict_proba and roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constrained randomized search\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(100, 300),\n",
    "    'learning_rate': uniform(0.01, 0.19),  # 0.01 to 0.2\n",
    "    'max_depth': randint(3, 7),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'subsample': uniform(0.7, 0.3)  # 0.7 to 1.0\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    GradientBoostingClassifier(random_state=RANDOM_SEED),\n",
    "    param_distributions,\n",
    "    n_iter=20,  # Try 20 combinations\n",
    "    cv=cv,\n",
    "    scoring='roc_auc',\n",
    "    random_state=RANDOM_SEED,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"=== RANDOMIZED SEARCH (20 iterations) ===\")\n",
    "print(\"Running...\")\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters: {random_search.best_params_}\")\n",
    "print(f\"Best CV score: {random_search.best_score_:.4f}\")\n",
    "\n",
    "# Test performance\n",
    "test_score = roc_auc_score(y_test, random_search.predict_proba(X_test)[:, 1])\n",
    "print(f\"Test score: {test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The `RandomizedSearchCV` output reports the best parameter combination found across 20 random trials. Typical best parameters on the breast cancer dataset: `learning_rate` around **0.05-0.15**, `max_depth` around **3-5**, `n_estimators` around **150-250**, and `subsample` around **0.8-1.0**.\n",
    "\n",
    "The **best CV score** should be equal to or slightly above the default GBM's CV score (around **0.99**). The **test score** printed below it should be consistent with the CV estimate. If the test score is much higher than the CV score, that is likely just variance on the small test set, not a real advantage.\n",
    "\n",
    "Notice the `subsample` parameter: values below 1.0 mean that each tree is trained on a random subset of the training data (stochastic gradient boosting). This adds a bagging-like regularization effect that can reduce overfitting, especially when the learning rate is higher.\n",
    "\n",
    "**Why this matters:** RandomizedSearchCV is the practical way to tune GBM in production. Exhaustive grid search over 5 hyperparameters with multiple values each would require thousands of fits; 20 random trials gets you into the right ballpark with manageable compute.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ PAUSE-AND-DO Exercise 2 (5 minutes)\n",
    "\n",
    "**Task:** Run constrained tuning and report best params + score.\n",
    "\n",
    "Already done! Now document your tuning strategy:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR TUNING STRATEGY:\n",
    "\n",
    "**Best Parameters Found:**  \n",
    "[List them]\n",
    "\n",
    "**Performance Improvement:**  \n",
    "[Compare to baseline GBM]\n",
    "\n",
    "**What would you try next:**  \n",
    "[If you had more time/compute, what would you tune?]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Overfitting in Boosting\n",
    "\n",
    "Unlike Random Forests, which are relatively resistant to overfitting because they average independent trees, gradient boosting can overfit severely if you train too many iterations. Each new tree is specifically designed to correct the current model's errors, so eventually the model starts fitting noise rather than signal.\n",
    "\n",
    "The staged-prediction plot below tracks ROC-AUC on both training and validation data at every iteration from 1 to 500 trees. The gap between the two curves is the overfitting signal. Where the validation curve peaks is the optimal number of trees; continuing beyond that point hurts generalization. In production, this pattern motivates **early stopping**: halt training when validation performance stops improving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Gemini Prompt:** \"Split train into smaller train/val (80/20). Fit GBM with 500 trees, lr=0.05. Use staged_predict_proba to track ROC-AUC per boosting iteration. Plot train vs val curves and find optimal n_trees.\"\n",
    ">\n",
    "> **After running, verify:**\n",
    "> - Two curves: train rising toward 1.0, val peaking then plateau\n",
    "> - Optimal number of trees printed\n",
    "> - Chart title indicates overfitting monitoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track train vs validation performance over iterations\n",
    "from sklearn.model_selection import train_test_split as split2\n",
    "\n",
    "X_t, X_v, y_t, y_v = split2(X_train, y_train, test_size=0.2, random_state=RANDOM_SEED, stratify=y_train)\n",
    "\n",
    "gbm_monitor = GradientBoostingClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=3,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "gbm_monitor.fit(X_t, y_t)\n",
    "\n",
    "# Compute staged predictions\n",
    "train_scores = []\n",
    "val_scores = []\n",
    "\n",
    "for i, (train_pred, val_pred) in enumerate(zip(\n",
    "    gbm_monitor.staged_predict_proba(X_t),\n",
    "    gbm_monitor.staged_predict_proba(X_v)\n",
    ")):\n",
    "    train_scores.append(roc_auc_score(y_t, train_pred[:, 1]))\n",
    "    val_scores.append(roc_auc_score(y_v, val_pred[:, 1]))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_scores)+1), train_scores, label='Train', linewidth=2)\n",
    "plt.plot(range(1, len(val_scores)+1), val_scores, label='Validation', linewidth=2)\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('ROC-AUC')\n",
    "plt.title('Training Progress: Monitoring Overfitting')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal number of trees\n",
    "optimal_n = np.argmax(val_scores) + 1\n",
    "print(f\"\\n=== OPTIMAL NUMBER OF TREES ===\")\n",
    "print(f\"Optimal n_estimators: {optimal_n}\")\n",
    "print(f\"Validation ROC-AUC: {val_scores[optimal_n-1]:.4f}\")\n",
    "print(f\"\\nðŸ’¡ Validation performance peaks, then plateaus (slight overfit after ~{optimal_n} trees)\")\n",
    "print(f\"ðŸ’¡ Could use early stopping in production\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The training curve (blue) rises steadily toward **1.0** and stays there, while the validation curve (orange) rises, peaks, and then either plateaus or gently declines. The optimal number of trees is printed below the plot, typically in the range of **50-200** for this dataset and learning rate (0.05).\n",
    "\n",
    "The gap between the two curves is the overfitting gap. At iteration 1, both curves are low (the model is a single shallow tree). As iterations increase, the training curve pulls away from the validation curve. If the validation curve starts declining after its peak, every additional tree past that point is fitting noise.\n",
    "\n",
    "In production, you would use **early stopping**: monitor validation loss during training and stop when it has not improved for a set number of rounds (the \"patience\" parameter). scikit-learn's `GradientBoostingClassifier` supports `n_iter_no_change` for this purpose, and libraries like XGBoost and LightGBM have built-in early-stopping callbacks.\n",
    "\n",
    "**Key takeaway:** Always monitor the validation curve during boosting. Training until convergence without validation monitoring is the single most common mistake with gradient boosting models.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Comparison: All Models\n",
    "\n",
    "We now have four model families in our toolkit: Logistic Regression, Decision Tree, Random Forest, and Gradient Boosting (both default and tuned). This final comparison table puts all five candidates on equal footing using the same `StratifiedKFold` cross-validation.\n",
    "\n",
    "The ranking typically shows ensemble methods (RF and GBM) at the top, with tuned Gradient Boosting achieving the highest ROC-AUC. However, the margin over a well-tuned Random Forest may be small, and the margin over Logistic Regression may be even smaller on a linearly separable dataset like breast cancer. The next notebook formalizes how to make the champion selection decision when margins are tight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Gemini Prompt:** \"Compare five models: LogReg, DecisionTree(depth=5), RF(200), GBM default, GBM tuned. Use 5-fold CV and test ROC-AUC. Sort by CV mean and identify champion.\"\n",
    ">\n",
    "> **After running, verify:**\n",
    "> - Table has 5 rows sorted by CV_Mean\n",
    "> - Each row shows Model, CV_Mean, CV_Std, Test_Score\n",
    "> - Champion identified at bottom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comparison\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "final_models = {\n",
    "    'Logistic Regression': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', LogisticRegression(random_state=RANDOM_SEED, max_iter=1000))\n",
    "    ]),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=RANDOM_SEED),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=200, random_state=RANDOM_SEED, n_jobs=-1),\n",
    "    'Gradient Boosting (default)': GradientBoostingClassifier(random_state=RANDOM_SEED),\n",
    "    'Gradient Boosting (tuned)': random_search.best_estimator_\n",
    "}\n",
    "\n",
    "final_results = []\n",
    "for name, model in final_models.items():\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='roc_auc')\n",
    "    \n",
    "    # Test score\n",
    "    if not hasattr(model, 'predict_proba'):\n",
    "        model.fit(X_train, y_train)\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        if name not in ['Gradient Boosting (tuned)']:  # Already fitted\n",
    "            model.fit(X_train, y_train)\n",
    "        test_pred = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        test_pred = model.predict(X_test)\n",
    "    \n",
    "    test_sc = roc_auc_score(y_test, test_pred)\n",
    "    \n",
    "    final_results.append({\n",
    "        'Model': name,\n",
    "        'CV_Mean': scores.mean(),\n",
    "        'CV_Std': scores.std(),\n",
    "        'Test_Score': test_sc\n",
    "    })\n",
    "\n",
    "final_df = pd.DataFrame(final_results).sort_values('CV_Mean', ascending=False)\n",
    "print(\"=== FINAL MODEL COMPARISON ===\")\n",
    "print(final_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nâœ“ Champion: {final_df.iloc[0]['Model']} (CV: {final_df.iloc[0]['CV_Mean']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The final table ranks five models by CV ROC-AUC. Typical ordering on the breast cancer dataset: **Gradient Boosting (tuned)** at the top with ROC-AUC near **0.99**, followed closely by **Gradient Boosting (default)** and **Random Forest**, then **Logistic Regression**, and finally the **Decision Tree** trailing by a few percentage points.\n",
    "\n",
    "The champion model is identified at the bottom. On this dataset the margins between the top three models are often razor-thin (< 0.01 in ROC-AUC), which raises an important question: is the extra complexity of gradient boosting worth the marginal gain over logistic regression? The answer depends on the deployment context, which is exactly what the next notebook (model selection protocol) formalizes.\n",
    "\n",
    "Compare the CV standard deviations: a model with slightly lower mean but also lower std may actually be preferable in a risk-sensitive application, because its worst-case performance is better.\n",
    "\n",
    "**Why this matters:** Building a cumulative comparison table across notebooks creates institutional knowledge. Instead of remembering that \"GBM was probably best,\" you have a documented, reproducible ranking under controlled conditions.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Wrap-Up: Key Takeaways\n\n### What We Learned Today:\n\n1. **Boosting vs Bagging**: Sequential vs parallel, bias vs variance reduction\n2. **Gradient Boosting**: Each tree corrects previous errors\n3. **Learning Rate**: Lower rates + more trees = better performance\n4. **Tuning Strategy**: Use constrained search, monitor overfitting\n5. **Performance**: GBM often beats RF, but needs more careful tuning\n\n### Critical Rules:\n\n> **\"Lower learning rate + more trees = better (but slower)\"**\n\n> **\"Keep trees shallow (depth 3-5) for boosting\"**\n\n> **\"Monitor overfitting closely with boosting\"**\n\n### Next Steps:\n\n- Next notebook: Model selection and comparison protocol\n- We'll formalize how to choose between models\n- Create reproducible experiment logs\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Participation Assignment Submission Instructions\n",
    "\n",
    "### To Submit This Notebook:\n",
    "\n",
    "1. **Complete all exercises**: Fill in both PAUSE-AND-DO exercise cells with your findings\n",
    "2. **Run All Cells**: Execute `Runtime â†’ Run all` to ensure everything works\n",
    "3. **Save a Copy**: `File â†’ Save a copy in Drive or Download the .ipynb extension`\n",
    "4. **Submit**: Upload your `.ipynb` file in the participation assignment you find in the course Brightspace page.\n",
    "\n",
    "### Before Submitting, Check:\n",
    "\n",
    "- [ ] All cells execute without errors\n",
    "- [ ] All outputs are visible\n",
    "- [ ] Both exercise responses are complete\n",
    "- [ ] Notebook is shared with correct permissions\n",
    "- [ ] You can explain every line of code you wrote\n",
    "\n",
    "### Next Step:\n",
    "\n",
    "Complete the **Quiz** in Brightspace (auto-graded)\n",
    "\n",
    "---\n",
    "\n",
    "## Bibliography\n\n- Friedman, J. H. (2001). \"Greedy Function Approximation: A Gradient Boosting Machine.\" *Annals of Statistics*.\n- James, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). *An Introduction to Statistical Learning with Python* - Tree-Based Methods (boosting overview)\n- Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning* - Boosting methods\n- scikit-learn User Guide: [Gradient boosting](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting)\n\n---\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "Thank you!\n",
    "\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
