{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization (Ridge/Lasso) + Project Proposal Sprint\n",
    "\n",
    "<hr>\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/notebooks/figures/mgmt_474_ai_logo_02-modified.png\" width=\"200\"/>\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "# <center><a class=\"tocSkip\"></center>\n",
    "# <center>MGMT47400 Predictive Analytics</center>\n",
    "# <center>Professor: Davi Moreira </center>\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/blob/main/notebooks/05_regularization_project_proposal.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Explain why regularization improves generalization\n",
    "2. Fit Ridge/Lasso with proper scaling and CV selection\n",
    "3. Interpret coefficient shrinkage and sparsity\n",
    "4. Draft a project proposal with a viable dataset + target + metric + split plan\n",
    "5. Use Gemini to scaffold code and then add guardrails (checks + comments)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "> **ðŸ“‹ Participation Reminder:** This notebook contains **2 PAUSE-AND-DO exercises**. You are expected to complete all exercises before submitting your notebook.\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## ðŸ’¼ Why This Matters: When the Model Memorizes Instead of Learns\n\nAfter engineering dozens of new features at **HomeValue Analytics** (interactions, polynomials, location-based), your model's training error plummets â€” but validation error barely improves. The CTO raises a red flag: *\"Are we memorizing noise in the training data, or learning real patterns that generalize?\"*\n\nThis is overfitting, the most common pitfall in predictive analytics. Regularization techniques (Ridge, Lasso, Elastic Net) add a penalty that forces the model to keep only the features that truly matter.\n\n> **Today's focus:** Applying regularization to prevent overfitting in our housing price model, and using Lasso's feature selection property to identify the most important predictors.\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV, ElasticNet\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport warnings\n\nwarnings.filterwarnings('ignore')\npd.set_option('display.precision', 4)\nRANDOM_SEED = 474\nnp.random.seed(RANDOM_SEED)\nprint(\"âœ“ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The `Setup complete!` confirmation means all imports loaded successfully. This notebook's import list adds several regularization-specific classes to the standard toolkit: **Ridge**, **RidgeCV**, **Lasso**, **LassoCV**, and **ElasticNet** from `sklearn.linear_model`. The `CV` variants include built-in cross-validation for automatic hyper-parameter tuning. As always, `RANDOM_SEED = 474` anchors all randomness.\n",
    "\n",
    "**Why this matters:** The cross-validated estimators (`RidgeCV`, `LassoCV`) save you from writing your own alpha-search loops. They handle the grid search internally and expose the best alpha as an attribute after fitting.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "We load the same California Housing dataset and apply the identical 60/20/20 split with `RANDOM_SEED = 474`. Because the splits match the previous notebooks exactly, any change in performance can be attributed solely to the new regularization techniques introduced here, not to data differences.\n",
    "The test set remains locked; all tuning uses the validation partition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Gemini Prompt:** \"Load the California Housing dataset using sklearn fetch_california_housing with as_frame=True. Separate features (X) from the target MedHouseVal (y), then do a 60/20/20 train/val/test split using two sequential train_test_split calls with random_state=RANDOM_SEED. Print the sizes of each set.\"\n",
    ">\n",
    "> **After running, verify:**\n",
    "> - Train set is ~60%, validation ~20%, test ~20% of total rows\n",
    "> - X has 8 feature columns (MedInc, HouseAge, AveRooms, etc.)\n",
    "> - y contains median house values (continuous target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "california = fetch_california_housing(as_frame=True)\n",
    "df = california.frame\n",
    "X = df.drop(columns=['MedHouseVal'])\n",
    "y = df['MedHouseVal']\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.20, random_state=RANDOM_SEED)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=RANDOM_SEED)\n",
    "\n",
    "print(f\"Train: {len(X_train)} | Val: {len(X_val)} | Test: {len(X_test)} (locked)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The printed split summary shows roughly **12,384 training**, **4,128 validation**, and **4,128 test** samples -- identical to previous notebooks. Consistency across notebooks is critical: it means the Ridge and Lasso results we compute here are directly comparable to the plain linear regression and polynomial models from the previous notebook, since they are evaluated on the exact same validation set.\n",
    "\n",
    "**Key takeaway:** Never change your random seed or split strategy mid-project. Doing so invalidates all prior comparisons and makes it impossible to tell whether performance changed because of the model or because of the data partition.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Why Regularization?\n",
    "\n",
    "### The Problem: Overfitting\n",
    "\n",
    "Without regularization, linear regression can:\n",
    "- Fit noise in training data\n",
    "- Produce unstable coefficients\n",
    "- Perform poorly on new data\n",
    "\n",
    "### The Solution: Penalize Complexity\n",
    "\n",
    "**Ridge (L2 penalty)**\n",
    "- Adds penalty: Î± Ã— (sum of squared coefficients)\n",
    "- Shrinks all coefficients toward zero\n",
    "- Keeps all features\n",
    "- Use when: All features might be relevant\n",
    "\n",
    "**Lasso (L1 penalty)**\n",
    "- Adds penalty: Î± Ã— (sum of absolute coefficients)\n",
    "- Can shrink coefficients to exactly zero\n",
    "- Performs feature selection\n",
    "- Use when: You expect many irrelevant features\n",
    "\n",
    "**Elastic Net**\n",
    "- Combines L1 + L2\n",
    "- Use when: You want feature selection but with less sensitivity than Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ridge Regression with Cross-Validation\n",
    "\n",
    "Ridge regression adds an L2 penalty to the ordinary least-squares loss, shrinking all coefficients toward zero without eliminating any. The strength of the penalty is controlled by the hyper-parameter **alpha** (higher alpha = stronger shrinkage).\n",
    "Rather than picking alpha by hand, we use `RidgeCV`, which performs 5-fold cross-validation over a grid of 50 candidate alpha values (log-spaced from 0.001 to 1000) and automatically selects the one that minimizes CV error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Gemini Prompt:** \"Build a Ridge regression pipeline with StandardScaler and RidgeCV. Use 50 alpha values log-spaced from 0.001 to 1000, and 5-fold CV inside RidgeCV to automatically select the best alpha. Fit on training data, then print the best alpha and R-squared on both train and validation sets.\"\n",
    ">\n",
    "> **After running, verify:**\n",
    "> - Best alpha is printed (a value between 0.001 and 1000)\n",
    "> - Train R-squared and Val R-squared are both reported\n",
    "> - Val R-squared is close to (but slightly below) Train R-squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge with automatic alpha tuning\n",
    "alphas = np.logspace(-3, 3, 50)\n",
    "\n",
    "ridge_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('ridge', RidgeCV(alphas=alphas, cv=5))\n",
    "])\n",
    "\n",
    "ridge_pipeline.fit(X_train, y_train)\n",
    "\n",
    "best_alpha = ridge_pipeline.named_steps['ridge'].alpha_\n",
    "train_score_ridge = ridge_pipeline.score(X_train, y_train)\n",
    "val_score_ridge = ridge_pipeline.score(X_val, y_val)\n",
    "\n",
    "print(\"=== RIDGE REGRESSION ===\")\n",
    "print(f\"Best alpha: {best_alpha:.4f}\")\n",
    "print(f\"Train RÂ²: {train_score_ridge:.4f}\")\n",
    "print(f\"Val RÂ²: {val_score_ridge:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The Ridge output reports three numbers: the **best alpha** selected by 5-fold CV, the **Train RÂ²**, and the **Val RÂ²**. The best alpha is typically a modest value (often in the single digits), meaning only light regularization was needed for this dataset. Train and validation RÂ² values should be very close to the unregularized OLS scores from the previous notebook, confirming that Ridge's primary benefit here is *stability* rather than a dramatic accuracy jump. The small train-validation gap indicates that overfitting is minimal even without regularization on this 8-feature dataset.\n",
    "\n",
    "**Why this matters:** Ridge shines when you have many correlated features or polynomial expansions where OLS coefficients become unstable. On a clean 8-feature dataset, the improvement is subtle but the *insurance* against overfitting is real.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ PAUSE-AND-DO Exercise 1 (5 minutes)\n\n**Task:** Run RidgeCV and summarize alpha choice + validation performance.\n\nThe code above already ran RidgeCV. Now:\n1. Interpret what the chosen alpha value means\n2. Compare to baseline linear regression (from previous notebooks)\n3. Explain why Ridge might help with generalization\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANALYSIS:\n",
    "\n",
    "**Alpha Interpretation:**  \n",
    "[What does the chosen alpha value mean? Higher = more regularization]\n",
    "\n",
    "**Performance Comparison:**  \n",
    "[How does Ridge compare to unregularized linear regression?]\n",
    "\n",
    "**Generalization:**  \n",
    "[Why might Ridge improve validation performance?]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Lasso Regression with Cross-Validation\n",
    "\n",
    "Lasso uses an L1 penalty instead of L2. The key difference is that L1 can drive coefficients to **exactly zero**, effectively performing automatic feature selection. This makes Lasso especially useful when you suspect that many features are irrelevant.\n",
    "`LassoCV` searches the same 50-value alpha grid with 5-fold cross-validation and selects the alpha that minimizes CV error. We set `max_iter=10000` to ensure convergence for stronger penalties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Gemini Prompt:** \"Build a Lasso regression pipeline with StandardScaler and LassoCV. Use the same 50 log-spaced alphas from 0.001 to 1000, 5-fold CV, and max_iter=10000. Fit on training data, then print the best alpha and R-squared on train and validation sets.\"\n",
    ">\n",
    "> **After running, verify:**\n",
    "> - Best alpha is printed (likely a small value since Lasso is aggressive)\n",
    "> - Train and Val R-squared are both reported\n",
    "> - Performance is comparable to Ridge (may be slightly lower due to feature zeroing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso with automatic alpha tuning\n",
    "lasso_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lasso', LassoCV(alphas=alphas, cv=5, max_iter=10000))\n",
    "])\n",
    "\n",
    "lasso_pipeline.fit(X_train, y_train)\n",
    "\n",
    "best_alpha_lasso = lasso_pipeline.named_steps['lasso'].alpha_\n",
    "train_score_lasso = lasso_pipeline.score(X_train, y_train)\n",
    "val_score_lasso = lasso_pipeline.score(X_val, y_val)\n",
    "\n",
    "print(\"=== LASSO REGRESSION ===\")\n",
    "print(f\"Best alpha: {best_alpha_lasso:.4f}\")\n",
    "print(f\"Train RÂ²: {train_score_lasso:.4f}\")\n",
    "print(f\"Val RÂ²: {val_score_lasso:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "Lasso reports the same three numbers: **best alpha**, **Train RÂ²**, and **Val RÂ²**. The best alpha for Lasso is typically much smaller than Ridge's because the L1 penalty acts more aggressively on individual coefficients. Pay attention to the RÂ² values: if they are nearly identical to Ridge's, the two methods are performing similarly on this dataset. The key differentiator appears in the next cell, where we inspect which coefficients Lasso set to zero.\n",
    "\n",
    "**Key takeaway:** Lasso's value proposition is not always higher accuracy -- it is *interpretability through sparsity*. By zeroing out weak features, Lasso tells you which variables genuinely drive the prediction and which are noise.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ PAUSE-AND-DO Exercise 2 (5 minutes)\n",
    "\n",
    "**Task:** Run LassoCV and identify top selected features (if any).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Gemini Prompt:** \"Extract the Lasso model coefficients into a DataFrame with columns Feature, Coefficient, and Abs_Coefficient. Sort by absolute value descending. Count how many coefficients are exactly zero vs non-zero, and print the selected features list.\"\n",
    ">\n",
    "> **After running, verify:**\n",
    "> - DataFrame shows all 8 features sorted by absolute coefficient magnitude\n",
    "> - Non-zero feature count is reported (Lasso may zero out some features)\n",
    "> - If feature selection occurred, the list of retained features is printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Lasso coefficients\n",
    "lasso_coefs = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Coefficient': lasso_pipeline.named_steps['lasso'].coef_,\n",
    "    'Abs_Coefficient': np.abs(lasso_pipeline.named_steps['lasso'].coef_)\n",
    "}).sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(\"=== LASSO COEFFICIENTS ===\")\n",
    "print(lasso_coefs)\n",
    "\n",
    "# Count non-zero coefficients\n",
    "n_nonzero = (lasso_coefs['Coefficient'] != 0).sum()\n",
    "n_total = len(lasso_coefs)\n",
    "\n",
    "print(f\"\\n=== FEATURE SELECTION ===\")\n",
    "print(f\"Non-zero features: {n_nonzero} / {n_total}\")\n",
    "print(f\"Features zeroed out: {n_total - n_nonzero}\")\n",
    "\n",
    "if n_nonzero < n_total:\n",
    "    print(f\"\\nâœ“ Lasso performed feature selection!\")\n",
    "    print(f\"\\nSelected features:\")\n",
    "    print(lasso_coefs[lasso_coefs['Coefficient'] != 0]['Feature'].tolist())\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ All features retained - alpha might be too small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The coefficient table lists all 8 features with their Lasso-assigned weights, sorted by absolute magnitude. The \"Feature Selection\" summary at the bottom reports how many coefficients are **non-zero** versus zeroed out. If all 8 remain non-zero, the chosen alpha was relatively light and Lasso kept every feature; if one or more are exactly zero, Lasso has performed automatic feature selection. Either way, compare the ranking to the OLS coefficients from the previous notebook: **MedInc** should still dominate, and features like **AveBedrms** or **Population** may have been shrunk toward (or to) zero.\n",
    "\n",
    "**Why this matters:** In real-world projects with dozens or hundreds of features, Lasso's ability to discard irrelevant variables can simplify your model dramatically, making it cheaper to deploy and easier to explain to stakeholders.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR FINDINGS:\n",
    "\n",
    "**Feature Selection:**  \n",
    "[How many features did Lasso select?]\n",
    "\n",
    "**Top Features:**  \n",
    "[Which features have the largest coefficients?]\n",
    "\n",
    "**Comparison:**  \n",
    "[How does Lasso performance compare to Ridge?]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Comparison Table\n",
    "\n",
    "With three models fitted (unregularized OLS, Ridge, and Lasso), we bring their key metrics into a single DataFrame for easy comparison. The table includes alpha values, train and validation RÂ², validation MAE, and the number of non-zero features. This side-by-side format makes it straightforward to evaluate the accuracy-complexity tradeoff.\n",
    "A model that achieves similar RÂ² to OLS but with fewer features or smaller coefficients is preferable for generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Gemini Prompt:** \"Fit a plain LinearRegression pipeline (with StandardScaler) as a baseline. Build a comparison DataFrame with columns Model, Alpha, Train_R2, Val_R2, Val_MAE, and Non_Zero_Features for Linear Regression, Ridge, and Lasso. Print the table and identify which model has the best validation R-squared.\"\n",
    ">\n",
    "> **After running, verify:**\n",
    "> - Three rows appear: Linear Regression, Ridge, and Lasso\n",
    "> - Val_MAE and Val_R2 allow direct comparison across all three models\n",
    "> - The best model by Val_R2 is identified at the bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline for comparison\n",
    "baseline_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "baseline_pipeline.fit(X_train, y_train)\n",
    "\n",
    "comparison = pd.DataFrame([\n",
    "    {\n",
    "        'Model': 'Linear Regression',\n",
    "        'Alpha': 'None',\n",
    "        'Train_R2': baseline_pipeline.score(X_train, y_train),\n",
    "        'Val_R2': baseline_pipeline.score(X_val, y_val),\n",
    "        'Val_MAE': mean_absolute_error(y_val, baseline_pipeline.predict(X_val)),\n",
    "        'Non_Zero_Features': len(X_train.columns)\n",
    "    },\n",
    "    {\n",
    "        'Model': 'Ridge',\n",
    "        'Alpha': f\"{best_alpha:.4f}\",\n",
    "        'Train_R2': train_score_ridge,\n",
    "        'Val_R2': val_score_ridge,\n",
    "        'Val_MAE': mean_absolute_error(y_val, ridge_pipeline.predict(X_val)),\n",
    "        'Non_Zero_Features': len(X_train.columns)\n",
    "    },\n",
    "    {\n",
    "        'Model': 'Lasso',\n",
    "        'Alpha': f\"{best_alpha_lasso:.4f}\",\n",
    "        'Train_R2': train_score_lasso,\n",
    "        'Val_R2': val_score_lasso,\n",
    "        'Val_MAE': mean_absolute_error(y_val, lasso_pipeline.predict(X_val)),\n",
    "        'Non_Zero_Features': n_nonzero\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"=== MODEL COMPARISON ===\")\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "best_model = comparison.loc[comparison['Val_R2'].idxmax(), 'Model']\n",
    "print(f\"\\nâœ“ Best validation performance: {best_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The three-row comparison table shows **Linear Regression** (alpha = None), **Ridge** (with its selected alpha), and **Lasso** (with its selected alpha). Columns include train RÂ², validation RÂ², validation MAE, and the count of non-zero features. On this dataset, all three models likely achieve very similar validation RÂ² (around **0.60**), because the 8-feature problem is simple enough that regularization has limited room to improve. The \"Best validation performance\" line names the winner, but the practical differences may be within noise. The non-zero-features column is where Lasso's value becomes visible: it achieves comparable accuracy with potentially fewer features.\n",
    "\n",
    "**Key takeaway:** When models perform similarly, prefer the simpler one. Ridge and Lasso provide insurance against overfitting that will pay off on more complex feature sets, even if the benefit is not dramatic here.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Coefficient Path Visualization\n",
    "\n",
    "A grouped bar chart comparing the 8 coefficients across all three models reveals the effect of regularization at a glance. Ridge coefficients should be uniformly smaller in absolute value than OLS coefficients (shrinkage), while Lasso coefficients may include exact zeros (sparsity).\n",
    "This visualization makes the abstract concept of \"penalty on coefficient magnitude\" concrete and interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Gemini Prompt:** \"Create a grouped bar chart comparing coefficients across Linear Regression, Ridge, and Lasso for all features. Use three bars per feature with different colors, rotated x-labels, a horizontal line at y=0, and a legend. Print observations about how Ridge shrinks coefficients and Lasso can zero some out.\"\n",
    ">\n",
    "> **After running, verify:**\n",
    "> - Bar chart shows three bars per feature (Linear, Ridge, Lasso side by side)\n",
    "> - Ridge bars are generally smaller in magnitude than Linear bars\n",
    "> - Lasso bars may be zero for some features, showing feature selection visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare coefficients across models\n",
    "coef_comparison = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Linear': baseline_pipeline.named_steps['regressor'].coef_,\n",
    "    'Ridge': ridge_pipeline.named_steps['ridge'].coef_,\n",
    "    'Lasso': lasso_pipeline.named_steps['lasso'].coef_\n",
    "})\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(coef_comparison))\n",
    "width = 0.25\n",
    "\n",
    "ax.bar(x - width, coef_comparison['Linear'], width, label='Linear', alpha=0.8)\n",
    "ax.bar(x, coef_comparison['Ridge'], width, label='Ridge', alpha=0.8)\n",
    "ax.bar(x + width, coef_comparison['Lasso'], width, label='Lasso', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Features')\n",
    "ax.set_ylabel('Coefficient Value')\n",
    "ax.set_title('Coefficient Comparison: Linear vs Ridge vs Lasso')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(coef_comparison['Feature'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ’¡ Notice how Ridge shrinks all coefficients\")\n",
    "print(\"ðŸ’¡ Notice how Lasso can zero out some features completely\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The grouped bar chart displays three bars per feature: blue for **Linear (OLS)**, orange for **Ridge**, and green for **Lasso**. For most features, the Ridge bars are slightly shorter than the OLS bars (uniform shrinkage), while Lasso bars may be noticeably shorter or completely absent for weak predictors. The printed tips below the chart summarize the pattern: Ridge shrinks *all* coefficients, Lasso can *zero out* some entirely.\n",
    "\n",
    "**Why this matters:** This visualization makes the abstract math of L1 vs. L2 penalties tangible. Stakeholders who cannot parse equations can immediately see that regularization \"calms down\" large coefficients and that Lasso is more aggressive about discarding features. Use charts like this when presenting model choices to non-technical audiences.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Project Proposal Builder\n",
    "\n",
    "### Week 1 Milestone: Project Proposal (Due Today)\n",
    "\n",
    "Your proposal should include:\n",
    "\n",
    "1. **Dataset Description**\n",
    "   - Source and link\n",
    "   - Number of samples and features\n",
    "   - Brief domain context\n",
    "\n",
    "2. **Predictive Task**\n",
    "   - Target variable (what are you predicting?)\n",
    "   - Prediction unit (what is one row?)\n",
    "   - Regression or classification?\n",
    "\n",
    "3. **Evaluation Plan**\n",
    "   - Primary metric (and why)\n",
    "   - Train/validation/test split strategy\n",
    "   - Baseline approach\n",
    "\n",
    "4. **Leakage Risk Assessment**\n",
    "   - List 3 potential sources of leakage\n",
    "   - How you'll prevent them\n",
    "\n",
    "5. **Initial Concerns**\n",
    "   - Data quality issues\n",
    "   - Missing values strategy\n",
    "   - Class imbalance (if classification)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“‹ PROJECT PROPOSAL TEMPLATE\n",
    "\n",
    "### 1. Dataset Description\n",
    "\n",
    "**Dataset Name:**  \n",
    "[Name]\n",
    "\n",
    "**Source:**  \n",
    "[URL or citation]\n",
    "\n",
    "**Size:**  \n",
    "[Number of samples Ã— features]\n",
    "\n",
    "**Domain Context:**  \n",
    "[Brief description of what the data represents]\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Predictive Task\n",
    "\n",
    "**Target Variable:**  \n",
    "[What are you predicting?]\n",
    "\n",
    "**Prediction Unit:**  \n",
    "[What does one row represent?]\n",
    "\n",
    "**Task Type:**  \n",
    "[Regression / Binary Classification / Multiclass Classification]\n",
    "\n",
    "**Business Question:**  \n",
    "[Why does this prediction matter?]\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Evaluation Plan\n",
    "\n",
    "**Primary Metric:**  \n",
    "[MAE / RMSE / RÂ² / Accuracy / F1 / ROC-AUC / etc.]\n",
    "\n",
    "**Metric Rationale:**  \n",
    "[Why is this metric aligned with the business goal?]\n",
    "\n",
    "**Split Strategy:**  \n",
    "- Training: [%]\n",
    "- Validation: [%]\n",
    "- Test: [%]\n",
    "- Special considerations: [Time-based? Stratified? Grouped?]\n",
    "\n",
    "**Baseline Approach:**  \n",
    "[Mean/median predictor, most frequent class, domain heuristic, etc.]\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Leakage Risk Assessment\n",
    "\n",
    "**Risk 1:**  \n",
    "[Describe potential leakage source and prevention strategy]\n",
    "\n",
    "**Risk 2:**  \n",
    "[Describe potential leakage source and prevention strategy]\n",
    "\n",
    "**Risk 3:**  \n",
    "[Describe potential leakage source and prevention strategy]\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Initial Concerns\n",
    "\n",
    "**Data Quality:**  \n",
    "[Issues you've identified or expect]\n",
    "\n",
    "**Missing Values:**  \n",
    "[Strategy for handling missingness]\n",
    "\n",
    "**Class Imbalance (if classification):**  \n",
    "[How you'll handle it]\n",
    "\n",
    "**Other Challenges:**  \n",
    "[Any other concerns]\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Success Criteria\n",
    "\n",
    "**Minimum Viable Model:**  \n",
    "[What performance would make this useful?]\n",
    "\n",
    "**Stretch Goal:**  \n",
    "[What would be excellent performance?]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Wrap-Up: Key Takeaways\n",
    "\n",
    "### What We Learned Today:\n",
    "\n",
    "1. **Regularization Intuition**: Penalize complexity to improve generalization\n",
    "2. **Ridge vs Lasso**: Shrinkage vs sparsity\n",
    "3. **Cross-Validation Selection**: Let CV choose alpha automatically\n",
    "4. **Coefficient Interpretation**: Regularization changes coefficient magnitudes\n",
    "5. **Project Planning**: Clear problem framing prevents wasted effort\n",
    "\n",
    "### Critical Rules:\n",
    "\n",
    "> **\"Always scale features before regularization\"**\n",
    "\n",
    "> **\"Use CV to tune alpha, don't peek at test\"**\n",
    "\n",
    "> **\"Start project planning early - dataset choice matters\"**\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- **Submit your project proposal today!**\n",
    "- Week 2 starts classification (logistic regression, metrics)\n",
    "- Start collecting your project dataset\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Participation Assignment Submission Instructions\n",
    "\n",
    "### To Submit This Notebook:\n",
    "\n",
    "1. **Complete all exercises**: Fill in both PAUSE-AND-DO exercise cells with your findings\n",
    "2. **Run All Cells**: Execute `Runtime â†’ Run all` to ensure everything works\n",
    "3. **Save a Copy**: `File â†’ Save a copy in Drive or Download the .ipynb extension`\n",
    "4. **Submit**: Upload your `.ipynb` file in the participation assignment you find in the course Brightspace page.\n",
    "\n",
    "### Before Submitting, Check:\n",
    "\n",
    "- [ ] All cells execute without errors\n",
    "- [ ] All outputs are visible\n",
    "- [ ] Both exercise responses are complete\n",
    "- [ ] Notebook is shared with correct permissions\n",
    "- [ ] You can explain every line of code you wrote\n",
    "\n",
    "### Next Step:\n",
    "\n",
    "Complete the **Quiz** in Brightspace (auto-graded)\n",
    "\n",
    "---\n",
    "\n",
    "## Bibliography\n\n- James, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). *An Introduction to Statistical Learning with Python* - Linear Model Selection and Regularization\n- Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning* - Shrinkage and regularization theory\n- scikit-learn User Guide: [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html), [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html), [ElasticNet](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html)\n\n---\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "Thank you!\n",
    "\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}