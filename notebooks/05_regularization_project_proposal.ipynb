{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization (Ridge/Lasso) + Project Proposal Sprint\n",
    "\n",
    "<hr>\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/notebooks/figures/mgmt_474_ai_logo_02-modified.png\" width=\"200\"/>\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "# <center><a class=\"tocSkip\"></center>\n",
    "# <center>MGMT47400 Predictive Analytics</center>\n",
    "# <center>Professor: Davi Moreira </center>\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/blob/main/notebooks/05_regularization_project_proposal.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Explain why regularization improves generalization\n",
    "2. Fit Ridge/Lasso with proper scaling and CV selection\n",
    "3. Interpret coefficient shrinkage and sparsity\n",
    "4. Draft a project proposal with a viable dataset + target + metric + split plan\n",
    "5. Use Gemini to scaffold code and then add guardrails (checks + comments)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV, ElasticNet\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport warnings\n\nwarnings.filterwarnings('ignore')\npd.set_option('display.precision', 4)\nRANDOM_SEED = 474\nnp.random.seed(RANDOM_SEED)\nprint(\"\u2713 Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "california = fetch_california_housing(as_frame=True)\n",
    "df = california.frame\n",
    "X = df.drop(columns=['MedHouseVal'])\n",
    "y = df['MedHouseVal']\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.20, random_state=RANDOM_SEED)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=RANDOM_SEED)\n",
    "\n",
    "print(f\"Train: {len(X_train)} | Val: {len(X_val)} | Test: {len(X_test)} (locked)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Why Regularization?\n",
    "\n",
    "### The Problem: Overfitting\n",
    "\n",
    "Without regularization, linear regression can:\n",
    "- Fit noise in training data\n",
    "- Produce unstable coefficients\n",
    "- Perform poorly on new data\n",
    "\n",
    "### The Solution: Penalize Complexity\n",
    "\n",
    "**Ridge (L2 penalty)**\n",
    "- Adds penalty: \u03b1 \u00d7 (sum of squared coefficients)\n",
    "- Shrinks all coefficients toward zero\n",
    "- Keeps all features\n",
    "- Use when: All features might be relevant\n",
    "\n",
    "**Lasso (L1 penalty)**\n",
    "- Adds penalty: \u03b1 \u00d7 (sum of absolute coefficients)\n",
    "- Can shrink coefficients to exactly zero\n",
    "- Performs feature selection\n",
    "- Use when: You expect many irrelevant features\n",
    "\n",
    "**Elastic Net**\n",
    "- Combines L1 + L2\n",
    "- Use when: You want feature selection but with less sensitivity than Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ridge Regression with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge with automatic alpha tuning\n",
    "alphas = np.logspace(-3, 3, 50)\n",
    "\n",
    "ridge_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('ridge', RidgeCV(alphas=alphas, cv=5))\n",
    "])\n",
    "\n",
    "ridge_pipeline.fit(X_train, y_train)\n",
    "\n",
    "best_alpha = ridge_pipeline.named_steps['ridge'].alpha_\n",
    "train_score_ridge = ridge_pipeline.score(X_train, y_train)\n",
    "val_score_ridge = ridge_pipeline.score(X_val, y_val)\n",
    "\n",
    "print(\"=== RIDGE REGRESSION ===\")\n",
    "print(f\"Best alpha: {best_alpha:.4f}\")\n",
    "print(f\"Train R\u00b2: {train_score_ridge:.4f}\")\n",
    "print(f\"Val R\u00b2: {val_score_ridge:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd PAUSE-AND-DO Exercise 1 (10 minutes)\n\n**Task:** Run RidgeCV and summarize alpha choice + validation performance.\n\nThe code above already ran RidgeCV. Now:\n1. Interpret what the chosen alpha value means\n2. Compare to baseline linear regression (from previous notebooks)\n3. Explain why Ridge might help with generalization\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANALYSIS:\n",
    "\n",
    "**Alpha Interpretation:**  \n",
    "[What does the chosen alpha value mean? Higher = more regularization]\n",
    "\n",
    "**Performance Comparison:**  \n",
    "[How does Ridge compare to unregularized linear regression?]\n",
    "\n",
    "**Generalization:**  \n",
    "[Why might Ridge improve validation performance?]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Lasso Regression with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso with automatic alpha tuning\n",
    "lasso_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lasso', LassoCV(alphas=alphas, cv=5, max_iter=10000))\n",
    "])\n",
    "\n",
    "lasso_pipeline.fit(X_train, y_train)\n",
    "\n",
    "best_alpha_lasso = lasso_pipeline.named_steps['lasso'].alpha_\n",
    "train_score_lasso = lasso_pipeline.score(X_train, y_train)\n",
    "val_score_lasso = lasso_pipeline.score(X_val, y_val)\n",
    "\n",
    "print(\"=== LASSO REGRESSION ===\")\n",
    "print(f\"Best alpha: {best_alpha_lasso:.4f}\")\n",
    "print(f\"Train R\u00b2: {train_score_lasso:.4f}\")\n",
    "print(f\"Val R\u00b2: {val_score_lasso:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd PAUSE-AND-DO Exercise 2 (10 minutes)\n",
    "\n",
    "**Task:** Run LassoCV and identify top selected features (if any).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Lasso coefficients\n",
    "lasso_coefs = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Coefficient': lasso_pipeline.named_steps['lasso'].coef_,\n",
    "    'Abs_Coefficient': np.abs(lasso_pipeline.named_steps['lasso'].coef_)\n",
    "}).sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(\"=== LASSO COEFFICIENTS ===\")\n",
    "print(lasso_coefs)\n",
    "\n",
    "# Count non-zero coefficients\n",
    "n_nonzero = (lasso_coefs['Coefficient'] != 0).sum()\n",
    "n_total = len(lasso_coefs)\n",
    "\n",
    "print(f\"\\n=== FEATURE SELECTION ===\")\n",
    "print(f\"Non-zero features: {n_nonzero} / {n_total}\")\n",
    "print(f\"Features zeroed out: {n_total - n_nonzero}\")\n",
    "\n",
    "if n_nonzero < n_total:\n",
    "    print(f\"\\n\u2713 Lasso performed feature selection!\")\n",
    "    print(f\"\\nSelected features:\")\n",
    "    print(lasso_coefs[lasso_coefs['Coefficient'] != 0]['Feature'].tolist())\n",
    "else:\n",
    "    print(f\"\\n\u26a0\ufe0f All features retained - alpha might be too small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR FINDINGS:\n",
    "\n",
    "**Feature Selection:**  \n",
    "[How many features did Lasso select?]\n",
    "\n",
    "**Top Features:**  \n",
    "[Which features have the largest coefficients?]\n",
    "\n",
    "**Comparison:**  \n",
    "[How does Lasso performance compare to Ridge?]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline for comparison\n",
    "baseline_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "baseline_pipeline.fit(X_train, y_train)\n",
    "\n",
    "comparison = pd.DataFrame([\n",
    "    {\n",
    "        'Model': 'Linear Regression',\n",
    "        'Alpha': 'None',\n",
    "        'Train_R2': baseline_pipeline.score(X_train, y_train),\n",
    "        'Val_R2': baseline_pipeline.score(X_val, y_val),\n",
    "        'Val_MAE': mean_absolute_error(y_val, baseline_pipeline.predict(X_val)),\n",
    "        'Non_Zero_Features': len(X_train.columns)\n",
    "    },\n",
    "    {\n",
    "        'Model': 'Ridge',\n",
    "        'Alpha': f\"{best_alpha:.4f}\",\n",
    "        'Train_R2': train_score_ridge,\n",
    "        'Val_R2': val_score_ridge,\n",
    "        'Val_MAE': mean_absolute_error(y_val, ridge_pipeline.predict(X_val)),\n",
    "        'Non_Zero_Features': len(X_train.columns)\n",
    "    },\n",
    "    {\n",
    "        'Model': 'Lasso',\n",
    "        'Alpha': f\"{best_alpha_lasso:.4f}\",\n",
    "        'Train_R2': train_score_lasso,\n",
    "        'Val_R2': val_score_lasso,\n",
    "        'Val_MAE': mean_absolute_error(y_val, lasso_pipeline.predict(X_val)),\n",
    "        'Non_Zero_Features': n_nonzero\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"=== MODEL COMPARISON ===\")\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "best_model = comparison.loc[comparison['Val_R2'].idxmax(), 'Model']\n",
    "print(f\"\\n\u2713 Best validation performance: {best_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Coefficient Path Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare coefficients across models\n",
    "coef_comparison = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Linear': baseline_pipeline.named_steps['regressor'].coef_,\n",
    "    'Ridge': ridge_pipeline.named_steps['ridge'].coef_,\n",
    "    'Lasso': lasso_pipeline.named_steps['lasso'].coef_\n",
    "})\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(coef_comparison))\n",
    "width = 0.25\n",
    "\n",
    "ax.bar(x - width, coef_comparison['Linear'], width, label='Linear', alpha=0.8)\n",
    "ax.bar(x, coef_comparison['Ridge'], width, label='Ridge', alpha=0.8)\n",
    "ax.bar(x + width, coef_comparison['Lasso'], width, label='Lasso', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Features')\n",
    "ax.set_ylabel('Coefficient Value')\n",
    "ax.set_title('Coefficient Comparison: Linear vs Ridge vs Lasso')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(coef_comparison['Feature'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\ud83d\udca1 Notice how Ridge shrinks all coefficients\")\n",
    "print(\"\ud83d\udca1 Notice how Lasso can zero out some features completely\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Project Proposal Builder\n",
    "\n",
    "### Week 1 Milestone: Project Proposal (Due Today)\n",
    "\n",
    "Your proposal should include:\n",
    "\n",
    "1. **Dataset Description**\n",
    "   - Source and link\n",
    "   - Number of samples and features\n",
    "   - Brief domain context\n",
    "\n",
    "2. **Predictive Task**\n",
    "   - Target variable (what are you predicting?)\n",
    "   - Prediction unit (what is one row?)\n",
    "   - Regression or classification?\n",
    "\n",
    "3. **Evaluation Plan**\n",
    "   - Primary metric (and why)\n",
    "   - Train/validation/test split strategy\n",
    "   - Baseline approach\n",
    "\n",
    "4. **Leakage Risk Assessment**\n",
    "   - List 3 potential sources of leakage\n",
    "   - How you'll prevent them\n",
    "\n",
    "5. **Initial Concerns**\n",
    "   - Data quality issues\n",
    "   - Missing values strategy\n",
    "   - Class imbalance (if classification)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udccb PROJECT PROPOSAL TEMPLATE\n",
    "\n",
    "### 1. Dataset Description\n",
    "\n",
    "**Dataset Name:**  \n",
    "[Name]\n",
    "\n",
    "**Source:**  \n",
    "[URL or citation]\n",
    "\n",
    "**Size:**  \n",
    "[Number of samples \u00d7 features]\n",
    "\n",
    "**Domain Context:**  \n",
    "[Brief description of what the data represents]\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Predictive Task\n",
    "\n",
    "**Target Variable:**  \n",
    "[What are you predicting?]\n",
    "\n",
    "**Prediction Unit:**  \n",
    "[What does one row represent?]\n",
    "\n",
    "**Task Type:**  \n",
    "[Regression / Binary Classification / Multiclass Classification]\n",
    "\n",
    "**Business Question:**  \n",
    "[Why does this prediction matter?]\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Evaluation Plan\n",
    "\n",
    "**Primary Metric:**  \n",
    "[MAE / RMSE / R\u00b2 / Accuracy / F1 / ROC-AUC / etc.]\n",
    "\n",
    "**Metric Rationale:**  \n",
    "[Why is this metric aligned with the business goal?]\n",
    "\n",
    "**Split Strategy:**  \n",
    "- Training: [%]\n",
    "- Validation: [%]\n",
    "- Test: [%]\n",
    "- Special considerations: [Time-based? Stratified? Grouped?]\n",
    "\n",
    "**Baseline Approach:**  \n",
    "[Mean/median predictor, most frequent class, domain heuristic, etc.]\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Leakage Risk Assessment\n",
    "\n",
    "**Risk 1:**  \n",
    "[Describe potential leakage source and prevention strategy]\n",
    "\n",
    "**Risk 2:**  \n",
    "[Describe potential leakage source and prevention strategy]\n",
    "\n",
    "**Risk 3:**  \n",
    "[Describe potential leakage source and prevention strategy]\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Initial Concerns\n",
    "\n",
    "**Data Quality:**  \n",
    "[Issues you've identified or expect]\n",
    "\n",
    "**Missing Values:**  \n",
    "[Strategy for handling missingness]\n",
    "\n",
    "**Class Imbalance (if classification):**  \n",
    "[How you'll handle it]\n",
    "\n",
    "**Other Challenges:**  \n",
    "[Any other concerns]\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Success Criteria\n",
    "\n",
    "**Minimum Viable Model:**  \n",
    "[What performance would make this useful?]\n",
    "\n",
    "**Stretch Goal:**  \n",
    "[What would be excellent performance?]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Wrap-Up: Key Takeaways\n",
    "\n",
    "### What We Learned Today:\n",
    "\n",
    "1. **Regularization Intuition**: Penalize complexity to improve generalization\n",
    "2. **Ridge vs Lasso**: Shrinkage vs sparsity\n",
    "3. **Cross-Validation Selection**: Let CV choose alpha automatically\n",
    "4. **Coefficient Interpretation**: Regularization changes coefficient magnitudes\n",
    "5. **Project Planning**: Clear problem framing prevents wasted effort\n",
    "\n",
    "### Critical Rules:\n",
    "\n",
    "> **\"Always scale features before regularization\"**\n",
    "\n",
    "> **\"Use CV to tune alpha, don't peek at test\"**\n",
    "\n",
    "> **\"Start project planning early - dataset choice matters\"**\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- **Submit your project proposal today!**\n",
    "- Week 2 starts classification (logistic regression, metrics)\n",
    "- Start collecting your project dataset\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n\n- James, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). *An Introduction to Statistical Learning with Python* - Linear Model Selection and Regularization\n- Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning* - Shrinkage and regularization theory\n- scikit-learn User Guide: [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html), [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html), [ElasticNet](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html)\n\n---\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "Thank you!\n",
    "\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}