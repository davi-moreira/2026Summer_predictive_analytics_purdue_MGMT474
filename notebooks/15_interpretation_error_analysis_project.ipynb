{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation - Feature Importance + Partial Dependence + Project Improved Model Delivery\n",
    "\n",
    "<hr>\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/notebooks/figures/mgmt_474_ai_logo_02-modified.png\" width=\"200\"/>\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "# <center><a class=\"tocSkip\"></center>\n",
    "# <center>MGMT47400 Predictive Analytics</center>\n",
    "# <center>Professor: Davi Moreira </center>\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/blob/main/notebooks/15_interpretation_error_analysis_project.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Generate model interpretation artifacts (permutation importance, PDP/ICE)\n",
    "2. Conduct error analysis to find systematic failure segments\n",
    "3. Communicate model behavior honestly (limits, caveats, instability)\n",
    "4. Deliver a project improved model with interpretation and error analysis\n",
    "5. Use Gemini to draft explanation text, then tighten it to evidence\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "This notebook uses the **California Housing** dataset (20,640 samples, 8 features) to demonstrate model interpretation and error analysis techniques. We switch from the breast cancer classification task to a regression task because continuous predictions make residual analysis and partial dependence plots more informative.\n",
    "\n",
    "The imports include `permutation_importance` and `PartialDependenceDisplay` from scikit-learn's inspection module, which provide model-agnostic tools for understanding how features influence predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.inspection import permutation_importance, PartialDependenceDisplay\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport warnings\n\nwarnings.filterwarnings('ignore')\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (12, 6)\n\nRANDOM_SEED = 474\nnp.random.seed(RANDOM_SEED)\n\nprint(\"\u2713 Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The setup cell loads the interpretation toolkit: `permutation_importance` for measuring feature contributions, `PartialDependenceDisplay` for visualizing how individual features affect predictions, and standard regression metrics (`mean_absolute_error`, `mean_squared_error`, `r2_score`). The figure size is set to (12, 6) for wider plots that accommodate feature names.\n",
    "\n",
    "The message `Setup complete!` with **RANDOM_SEED = 474** confirms all libraries are available. This notebook requires scikit-learn 1.0+ for the `PartialDependenceDisplay` API; older versions use a different function signature.\n",
    "\n",
    "**Key takeaway:** The inspection tools imported here (`permutation_importance`, `PartialDependenceDisplay`) are model-agnostic: they work with any scikit-learn estimator, not just Random Forests. You can reuse the same code with Gradient Boosting, Ridge regression, or any other model.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data and Train Champion Model\n",
    "\n",
    "We load the California Housing dataset and split it into train (60 %), validation (20 %), and test (20 %) sets. The target variable `MedHouseVal` represents the median house value in units of $100,000 for census block groups in California. Features include median income, house age, average number of rooms, geographic coordinates (latitude and longitude), and population density.\n",
    "\n",
    "We then train a `RandomForestRegressor` as our champion model. Random Forests are a natural choice for interpretation exercises because they are non-linear (so partial dependence plots show interesting curves) yet relatively stable (so permutation importance estimates are reliable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "california = fetch_california_housing(as_frame=True)\n",
    "df = california.frame\n",
    "\n",
    "X = df.drop(columns=['MedHouseVal'])\n",
    "y = df['MedHouseVal']\n",
    "\n",
    "# Split data\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.20, random_state=RANDOM_SEED)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=RANDOM_SEED)\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The dataset is split into three partitions: **Train** (~12,384 samples), **Val** (~4,128 samples), and **Test** (~4,128 samples), following the standard 60/20/20 ratio. The California Housing dataset has **8 features**: `MedInc` (median income), `HouseAge`, `AveRooms`, `AveBedrms`, `Population`, `AveOccup`, `Latitude`, and `Longitude`.\n",
    "\n",
    "The target `MedHouseVal` is capped at 5.0 ($500,000) in the original dataset, meaning very expensive properties are clipped. This cap will show up later in the error analysis as a ceiling effect where the model cannot predict values above 5.0.\n",
    "\n",
    "**Why this matters:** The three-way split is essential for this notebook's workflow. We train on the training set, compute all interpretation artifacts on the validation set, and reserve the test set for a final unbiased evaluation. Computing importance on validation data (not training data) ensures we measure genuine predictive value.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train champion model (Random Forest for interpretation)\n",
    "rf_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=RANDOM_SEED, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_val_pred = rf_model.predict(X_val)\n",
    "\n",
    "# Evaluation\n",
    "mae = mean_absolute_error(y_val, y_val_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "print(\"\\n=== CHAMPION MODEL PERFORMANCE ===\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R\u00b2: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "Three metrics are reported for the champion Random Forest on the validation set: **MAE** (mean absolute error, in $100k units), **RMSE** (root mean squared error), and **R-squared**. Typical values on this dataset with 100 trees and max_depth=10: MAE around **0.33** ($33,000 average error), RMSE around **0.48**, and R-squared around **0.80**.\n",
    "\n",
    "An **R-squared of 0.80** means the model explains 80 % of the variance in median house values using just 8 features. The remaining 20 % is driven by factors not in the dataset (school quality, neighborhood amenities, market conditions, etc.).\n",
    "\n",
    "The MAE of ~0.33 means that on average, the model's prediction is off by about $33,000. For a median house valued at $200,000, that is a ~16 % error, which is reasonable for a first model but leaves room for improvement.\n",
    "\n",
    "**Key takeaway:** These baseline metrics are the reference point for all subsequent analysis. When we find high-error segments later, we will compare their segment-specific MAE against this overall MAE of ~0.33.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Permutation Feature Importance\n",
    "\n",
    "### 3.1 Compute Permutation Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute permutation importance on validation set\n",
    "perm_importance = permutation_importance(\n",
    "    rf_model, X_val, y_val, \n",
    "    n_repeats=10, \n",
    "    random_state=RANDOM_SEED,\n",
    "    scoring='neg_mean_absolute_error'\n",
    ")\n",
    "\n",
    "# Create importance DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance_mean': perm_importance.importances_mean,\n",
    "    'importance_std': perm_importance.importances_std\n",
    "}).sort_values('importance_mean', ascending=False)\n",
    "\n",
    "print(\"\\n=== PERMUTATION FEATURE IMPORTANCE ===\")\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The permutation importance table lists all 8 features ranked by their impact on MAE (scored as `neg_mean_absolute_error`). The `importance_mean` column shows how much MAE increases when that feature is shuffled; higher values mean the feature is more important.\n",
    "\n",
    "**MedInc** (median income) typically dominates with importance around **0.30-0.50**, meaning shuffling income values increases MAE by $30,000-50,000. This makes economic sense: income is the strongest predictor of housing prices. **Latitude** and **Longitude** often rank second and third, capturing the geographic price premium of coastal California.\n",
    "\n",
    "Features with importance near zero (e.g., `Population`, `AveBedrms`) contribute little to the model's predictions on the validation set. Removing them would barely change performance, which is useful information for model simplification.\n",
    "\n",
    "The `importance_std` column shows variability across the 10 shuffle repeats. Features with high std relative to their mean should be interpreted cautiously: their importance is not precisely estimated.\n",
    "\n",
    "**Key takeaway:** Permutation importance answers the question \"which features does the model actually rely on for its predictions?\" On California Housing, the answer is overwhelmingly income and geography.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Visualize Importance\n",
    "\n",
    "A horizontal bar chart is the standard way to present permutation importance because it displays feature names legibly and lets you compare magnitudes at a glance. Error bars (one standard deviation from the 10 shuffle repeats) indicate how stable each importance estimate is.\n",
    "\n",
    "Features with error bars overlapping zero are not significantly important to the model. Features with large bars and tight error bars are the most reliably important drivers of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot permutation importance\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.barh(importance_df['feature'], importance_df['importance_mean'], xerr=importance_df['importance_std'])\n",
    "ax.set_xlabel('Permutation Importance (decrease in MAE)')\n",
    "ax.set_title('Feature Importance - Random Forest Model')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The horizontal bar chart provides a visual ranking of all 8 features. Error bars (horizontal lines extending from each bar) represent one standard deviation from the 10 permutation repeats. **MedInc** stands out as the tallest bar by a wide margin, followed by the geographic features.\n",
    "\n",
    "Features with bars that barely extend beyond zero (e.g., `Population`) are essentially noise from the model's perspective. You could remove them and the model's MAE would barely change.\n",
    "\n",
    "Notice whether any error bars overlap with zero: that would mean the feature's importance is not statistically distinguishable from zero. For the California Housing dataset, typically 5-6 features have clearly positive importance while 2-3 features are near zero.\n",
    "\n",
    "**Why this matters:** This chart is the primary deliverable for stakeholders who ask \"what drives the model's predictions?\" A domain expert can look at this chart and immediately verify whether the model makes sense (income and location driving prices is intuitive) or if something suspicious is happening (e.g., population being the top feature would be concerning).\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd PAUSE-AND-DO Exercise 1 (10 minutes)\n",
    "\n",
    "**Task:** Create permutation importance and write 3 evidence-based bullets about feature importance.\n",
    "\n",
    "**Instructions:**\n",
    "1. Review the permutation importance results above\n",
    "2. Identify the top 3 most important features\n",
    "3. Write 3 evidence-based interpretation bullets\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR INTERPRETATION HERE:\n",
    "\n",
    "**Finding 1:**  \n",
    "[Evidence-based interpretation of most important feature]\n",
    "\n",
    "**Finding 2:**  \n",
    "[Evidence-based interpretation of second feature]\n",
    "\n",
    "**Finding 3:**  \n",
    "[Evidence-based interpretation or pattern]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Partial Dependence Plots (PDP)\n",
    "\n",
    "### 4.1 Create PDP for Top Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top 4 features for PDP\n",
    "top_features = importance_df.head(4)['feature'].tolist()\n",
    "\n",
    "print(f\"Creating PDP for: {top_features}\")\n",
    "\n",
    "# Create partial dependence plots\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "display = PartialDependenceDisplay.from_estimator(\n",
    "    rf_model, X_val, top_features, \n",
    "    ax=ax, \n",
    "    kind=\"average\",\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "plt.suptitle('Partial Dependence Plots - Top 4 Features', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The partial dependence plots show four subplots, one for each of the top 4 features. Each curve shows the **average predicted house value** as that feature varies, while all other features are held at their actual values in the dataset.\n",
    "\n",
    "For **MedInc**: the PDP typically shows a strong positive relationship, rising steeply from income 0-5 and then flattening above income 8-10. This flattening occurs partly because the target is capped at 5.0 and partly because very few training samples have income above 10.\n",
    "\n",
    "For **Latitude** and **Longitude**: the PDPs capture the geographic price gradient. Lower latitude (Southern California) and specific longitude ranges (coastal areas) tend to have higher predicted values.\n",
    "\n",
    "For **HouseAge** or **AveRooms** (depending on the ranking): the relationships may be non-monotonic, showing that the Random Forest captures patterns that a linear model would miss.\n",
    "\n",
    "**Key takeaway:** PDPs transform a black-box model into interpretable feature-response curves. They answer the question \"how does the model's prediction change as I vary this one feature?\" The curves should align with domain knowledge; unexpected shapes warrant investigation.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Individual Conditional Expectation (ICE) Plots\n",
    "\n",
    "While PDP shows the **average** effect of a feature across all samples, ICE plots show the effect for **each individual sample** as a separate thin line. The thick PDP line is the average of all ICE curves. If the ICE curves are tightly bundled, the feature has a consistent effect across the population. If they fan out or cross, the feature interacts with other features, meaning its effect depends on context.\n",
    "\n",
    "Wide spread in ICE curves is a signal of **interaction effects** that the PDP alone would hide. For example, median income might increase predicted house value for coastal locations (low latitude) but have a smaller effect for inland locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ICE plots for top feature\n",
    "top_feature = top_features[0]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "display = PartialDependenceDisplay.from_estimator(\n",
    "    rf_model, X_val, [top_feature],\n",
    "    kind=\"both\",  # Shows both average PDP and individual ICE curves\n",
    "    ax=ax,\n",
    "    random_state=RANDOM_SEED,\n",
    "    ice_lines_kw={\"alpha\": 0.1}\n",
    ")\n",
    "plt.suptitle(f'ICE Plot for {top_feature}', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\u26a0\ufe0f Interpretation Notes:\")\n",
    "print(f\"  - PDP shows average effect of {top_feature} on predictions\")\n",
    "print(f\"  - ICE curves show effect for individual samples\")\n",
    "print(f\"  - Wide spread in ICE = interactions with other features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The ICE plot overlays hundreds of thin semi-transparent lines (individual samples) with a thick solid line (the PDP average). For **MedInc**, most ICE curves follow the same upward trend, but you can see variation: some curves are steeper and some are flatter, indicating that the effect of income on predicted house value depends on the other features of each sample.\n",
    "\n",
    "If the ICE curves cross each other, that is strong evidence of a **feature interaction**. For example, a sample in a high-latitude area (e.g., northern rural California) may show a flatter income-price curve than a sample in a coastal urban area, because the geographic premium amplifies the income effect.\n",
    "\n",
    "The spread of ICE curves at any given income level represents the heterogeneity of the model's response. A narrow band means income has a consistent effect; a wide band means the effect is context-dependent.\n",
    "\n",
    "**Why this matters:** ICE plots reveal individual-level variation that PDP averages hide. Before making policy recommendations based on PDP curves (e.g., \"increasing income by $10k raises predicted house value by $X\"), check ICE plots to see if that relationship holds uniformly or varies by subgroup.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Error Analysis\n",
    "\n",
    "### 5.1 Residual Analysis\n",
    "\n",
    "Error analysis goes beyond aggregate metrics (MAE, RMSE, R-squared) to examine **where and how** the model fails. Residual plots reveal systematic patterns: a funnel shape indicates heteroscedasticity (errors grow with predicted value), clusters of large residuals point to subpopulations the model struggles with, and non-zero mean residuals signal bias.\n",
    "\n",
    "The two-panel visualization below shows residuals vs. predicted values (left) and the residual distribution (right). An ideal model produces residuals that are randomly scattered around zero with constant variance and a symmetric, roughly normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute residuals\n",
    "residuals = y_val - y_val_pred\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Residual plot\n",
    "axes[0].scatter(y_val_pred, residuals, alpha=0.3)\n",
    "axes[0].axhline(y=0, color='r', linestyle='--')\n",
    "axes[0].set_xlabel('Predicted Values')\n",
    "axes[0].set_ylabel('Residuals')\n",
    "axes[0].set_title('Residual Plot')\n",
    "\n",
    "# Residual histogram\n",
    "axes[1].hist(residuals, bins=50, edgecolor='black')\n",
    "axes[1].set_xlabel('Residuals')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Residual Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nResidual Statistics:\")\n",
    "print(f\"  Mean: {residuals.mean():.4f}\")\n",
    "print(f\"  Std: {residuals.std():.4f}\")\n",
    "print(f\"  Median: {residuals.median():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The **left panel** (residuals vs. predicted values) reveals systematic patterns. On California Housing, you will typically see a **funnel shape**: residuals are small for low predicted values but fan out for high predicted values. This heteroscedasticity means the model is less accurate for expensive homes. You may also notice a cluster of large positive residuals near predicted value 5.0, caused by the target cap.\n",
    "\n",
    "The **right panel** (residual histogram) shows the distribution of errors. The distribution is roughly symmetric and centered near zero (mean residual close to 0.0), confirming no systematic bias. However, the right tail is heavier than the left, meaning the model underpredicts more often than it overpredicts, especially for high-value properties.\n",
    "\n",
    "The printed residual statistics give the numerical summary: mean near **0.00** (no bias), standard deviation around **0.45-0.50**, and median close to zero. A non-zero median would indicate skewed errors.\n",
    "\n",
    "**Why this matters:** Residual analysis is the diagnostic that tells you where your model fails systematically, not just on average. The funnel shape above directly motivates the segment error analysis in the next section.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Segment Error Analysis\n",
    "\n",
    "Aggregate error metrics can mask dramatic performance differences across subpopulations. Segment error analysis splits the validation set into groups based on the most important feature and computes error metrics separately for each group. This reveals whether the model performs well everywhere or only in certain regions of the feature space.\n",
    "\n",
    "We use quartiles of the top feature (typically median income) to create four segments. If one segment has dramatically higher MAE than the others, that segment represents a systematic failure mode that may require additional features, a different model architecture, or explicit post-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze errors by segments\n",
    "# Create price segments\n",
    "X_val_analysis = X_val.copy()\n",
    "X_val_analysis['y_true'] = y_val.values\n",
    "X_val_analysis['y_pred'] = y_val_pred\n",
    "X_val_analysis['abs_error'] = np.abs(residuals)\n",
    "\n",
    "# Segment by top feature\n",
    "top_feature = importance_df.iloc[0]['feature']\n",
    "X_val_analysis[f'{top_feature}_segment'] = pd.qcut(X_val_analysis[top_feature], q=4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
    "\n",
    "# Error by segment\n",
    "segment_errors = X_val_analysis.groupby(f'{top_feature}_segment').agg({\n",
    "    'abs_error': ['mean', 'median', 'std'],\n",
    "    'y_true': 'count'\n",
    "}).round(4)\n",
    "\n",
    "print(f\"\\n=== ERROR ANALYSIS BY {top_feature} SEGMENT ===\")\n",
    "print(segment_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The segment error table breaks down MAE, median error, error standard deviation, and sample count for each quartile of the top feature (typically **MedInc**). The columns are organized hierarchically: under `abs_error` you see mean, median, and std; under `y_true` you see the count.\n",
    "\n",
    "Typical pattern on California Housing: **Q1** (lowest income quartile) has relatively low MAE because these low-value homes cluster in a narrow price range that is easy to predict. **Q4** (highest income quartile) has dramatically higher MAE, often 2-3x the Q1 error, because high-income areas have diverse housing values and the target cap at 5.0 truncates the model's ability to distinguish luxury properties.\n",
    "\n",
    "The sample counts should be roughly equal across quartiles (each ~25 % of validation data), confirming that `pd.qcut` created balanced groups.\n",
    "\n",
    "**Key takeaway:** The high-income segment (Q4) is the model's primary failure mode. Any effort to improve the model should focus here: additional features (school ratings, proximity to coast), removing the target cap, or training a separate specialist model for the luxury segment.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize errors by segment\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "X_val_analysis.boxplot(column='abs_error', by=f'{top_feature}_segment', ax=ax)\n",
    "ax.set_xlabel(f'{top_feature} Quartile')\n",
    "ax.set_ylabel('Absolute Error')\n",
    "ax.set_title(f'Error Distribution by {top_feature} Segment')\n",
    "plt.suptitle('')  # Remove default title\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The box plot visualizes the error distribution for each quartile segment. Each box shows the interquartile range (IQR) of absolute errors, the line inside is the median, and the whiskers extend to 1.5x IQR. Outlier dots beyond the whiskers represent samples where the model made particularly large errors.\n",
    "\n",
    "The **Q4 box** is visibly taller and higher than the others, confirming quantitatively what the residual plot showed qualitatively: the model struggles most with high-income areas. The Q4 box also has more outliers (dots above the whisker), some with absolute errors exceeding $100,000.\n",
    "\n",
    "Compare the median line across boxes: Q1 and Q2 have medians near **0.15-0.25** ($15,000-25,000), while Q4's median is around **0.40-0.60** ($40,000-60,000). This 2-3x error ratio means the model's accuracy depends heavily on the income bracket of the property being valued.\n",
    "\n",
    "**Why this matters:** This box plot is a powerful communication tool for stakeholders. Instead of reporting a single MAE of ~$33,000, you can say: \"Our model predicts low-income properties to within ~$20,000 but high-income properties only to within ~$50,000. We should not use this model for luxury home valuations without additional improvements.\"\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd PAUSE-AND-DO Exercise 2 (10 minutes)\n",
    "\n",
    "**Task:** Run segment error analysis and identify one failure segment.\n",
    "\n",
    "**Instructions:**\n",
    "1. Review the segment error analysis above\n",
    "2. Identify which segment has the highest error\n",
    "3. Propose a hypothesis for why this segment performs poorly\n",
    "4. Suggest one potential improvement\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR SEGMENT ANALYSIS HERE:\n",
    "\n",
    "**Highest Error Segment:**  \n",
    "[Which segment and why?]\n",
    "\n",
    "**Hypothesis:**  \n",
    "[Why does this segment have higher errors?]\n",
    "\n",
    "**Potential Improvement:**  \n",
    "[What could help reduce errors in this segment?]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interpretation Narrative Template (Evidence-Based)\n",
    "\n",
    "### 6.1 Model Strengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Strengths (Evidence-Based):**\n",
    "\n",
    "1. **Overall Performance**: The Random Forest model achieves an R\u00b2 of [X.XX] on the validation set, explaining [XX]% of variance in housing prices.\n",
    "\n",
    "2. **Key Drivers**: Feature importance analysis reveals that [Top Feature] is the strongest predictor, with a permutation importance of [X.XX], followed by [Second Feature].\n",
    "\n",
    "3. **Predictive Patterns**: Partial dependence plots show [describe pattern] relationship between [feature] and predictions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Model Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Limitations (Honest Assessment):**\n",
    "\n",
    "1. **Segment Performance Gaps**: Error analysis reveals systematically higher errors in [segment description], with mean absolute error of [X.XX] compared to overall MAE of [X.XX].\n",
    "\n",
    "2. **Feature Interactions**: ICE plots show substantial variation in individual effects, suggesting complex interactions that the model may not fully capture.\n",
    "\n",
    "3. **Residual Patterns**: Residual analysis indicates [describe pattern], suggesting [interpretation].\n",
    "\n",
    "4. **Generalization Concerns**: The model is trained on [timeframe/geography], and may not generalize to [different context].\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recommendations:**\n",
    "\n",
    "1. **Use with Caution**: Apply elevated scrutiny to predictions for [high-error segment].\n",
    "\n",
    "2. **Feature Engineering**: Consider engineering additional features to capture [identified interaction].\n",
    "\n",
    "3. **Model Monitoring**: Track permutation importance stability over time to detect feature drift.\n",
    "\n",
    "4. **Error Bounds**: When communicating predictions, include confidence intervals, especially for [segment].\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Project Milestone 3 Scaffold\n",
    "\n",
    "### 7.1 Deliverable Checklist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Project Milestone 3 Requirements:**\n",
    "\n",
    "- [ ] Updated model comparison table (baseline vs improved models)\n",
    "- [ ] Champion model selection with justification\n",
    "- [ ] Permutation importance plot and interpretation\n",
    "- [ ] PDP/ICE plots for top 3-4 features\n",
    "- [ ] Segment error analysis with findings\n",
    "- [ ] Evidence-based interpretation narrative\n",
    "- [ ] Model limitations section (honest assessment)\n",
    "- [ ] Next steps and recommendations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Wrap-Up: Key Takeaways\n",
    "\n",
    "### What We Learned Today:\n",
    "\n",
    "1. **Permutation Importance**: Model-agnostic method to measure feature importance\n",
    "2. **Partial Dependence**: Visualizing average effect of features on predictions\n",
    "3. **ICE Plots**: Understanding individual-level feature effects and interactions\n",
    "4. **Error Analysis**: Finding systematic failure patterns through segmentation\n",
    "5. **Honest Communication**: Evidence-based interpretation with clear limitations\n",
    "\n",
    "### Interpretation Best Practices:\n",
    "\n",
    "- \u2713 Always compute importance on validation/test data, not training data\n",
    "- \u2713 Report standard deviations to show stability\n",
    "- \u2713 Check for correlated features when interpreting importance\n",
    "- \u2713 Use PDP cautiously when features are highly correlated\n",
    "- \u2713 Conduct segment analysis to find failure modes\n",
    "- \u2713 Be honest about limitations and uncertainties\n",
    "\n",
    "### Remember:\n",
    "\n",
    "> **\"Interpretation is about honest communication, not selling the model.\"**  \n",
    "> Report what you find, including limitations and failure modes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n\n- scikit-learn User Guide: [Inspection Tools](https://scikit-learn.org/stable/inspection.html) (permutation importance, partial dependence)\n- Molnar, C. (2022). *Interpretable Machine Learning*. [Online book](https://christophm.github.io/interpretable-ml-book/)\n- James, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). *An Introduction to Statistical Learning with Python* (ISLP). Springer.\n- Breiman, L. (2001). \"Random Forests.\" *Machine Learning*, 45(1), 5-32.\n\n---\n\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "Thank you!\n",
    "\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}