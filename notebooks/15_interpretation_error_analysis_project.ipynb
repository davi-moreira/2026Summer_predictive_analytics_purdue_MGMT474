{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation - Feature Importance + Partial Dependence + Project Improved Model Delivery\n",
    "\n",
    "<hr>\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/notebooks/figures/mgmt_474_ai_logo_02-modified.png\" width=\"200\"/>\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "# <center><a class=\"tocSkip\"></center>\n",
    "# <center>MGMT47400 Predictive Analytics</center>\n",
    "# <center>Professor: Davi Moreira </center>\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/blob/main/notebooks/15_interpretation_error_analysis_project.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Generate model interpretation artifacts (permutation importance, PDP/ICE)\n",
    "2. Conduct error analysis to find systematic failure segments\n",
    "3. Communicate model behavior honestly (limits, caveats, instability)\n",
    "4. Deliver a project improved model with interpretation and error analysis\n",
    "5. Use Gemini to draft explanation text, then tighten it to evidence\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.inspection import permutation_importance, PartialDependenceDisplay\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport warnings\n\nwarnings.filterwarnings('ignore')\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (12, 6)\n\nRANDOM_SEED = 474\nnp.random.seed(RANDOM_SEED)\n\nprint(\"\u2713 Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data and Train Champion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "california = fetch_california_housing(as_frame=True)\n",
    "df = california.frame\n",
    "\n",
    "X = df.drop(columns=['MedHouseVal'])\n",
    "y = df['MedHouseVal']\n",
    "\n",
    "# Split data\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.20, random_state=RANDOM_SEED)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=RANDOM_SEED)\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train champion model (Random Forest for interpretation)\n",
    "rf_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=RANDOM_SEED, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_val_pred = rf_model.predict(X_val)\n",
    "\n",
    "# Evaluation\n",
    "mae = mean_absolute_error(y_val, y_val_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "print(\"\\n=== CHAMPION MODEL PERFORMANCE ===\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R\u00b2: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Permutation Feature Importance\n",
    "\n",
    "### 3.1 Compute Permutation Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute permutation importance on validation set\n",
    "perm_importance = permutation_importance(\n",
    "    rf_model, X_val, y_val, \n",
    "    n_repeats=10, \n",
    "    random_state=RANDOM_SEED,\n",
    "    scoring='neg_mean_absolute_error'\n",
    ")\n",
    "\n",
    "# Create importance DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance_mean': perm_importance.importances_mean,\n",
    "    'importance_std': perm_importance.importances_std\n",
    "}).sort_values('importance_mean', ascending=False)\n",
    "\n",
    "print(\"\\n=== PERMUTATION FEATURE IMPORTANCE ===\")\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Visualize Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot permutation importance\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.barh(importance_df['feature'], importance_df['importance_mean'], xerr=importance_df['importance_std'])\n",
    "ax.set_xlabel('Permutation Importance (decrease in MAE)')\n",
    "ax.set_title('Feature Importance - Random Forest Model')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd PAUSE-AND-DO Exercise 1 (10 minutes)\n",
    "\n",
    "**Task:** Create permutation importance and write 3 evidence-based bullets about feature importance.\n",
    "\n",
    "**Instructions:**\n",
    "1. Review the permutation importance results above\n",
    "2. Identify the top 3 most important features\n",
    "3. Write 3 evidence-based interpretation bullets\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR INTERPRETATION HERE:\n",
    "\n",
    "**Finding 1:**  \n",
    "[Evidence-based interpretation of most important feature]\n",
    "\n",
    "**Finding 2:**  \n",
    "[Evidence-based interpretation of second feature]\n",
    "\n",
    "**Finding 3:**  \n",
    "[Evidence-based interpretation or pattern]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Partial Dependence Plots (PDP)\n",
    "\n",
    "### 4.1 Create PDP for Top Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top 4 features for PDP\n",
    "top_features = importance_df.head(4)['feature'].tolist()\n",
    "\n",
    "print(f\"Creating PDP for: {top_features}\")\n",
    "\n",
    "# Create partial dependence plots\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "display = PartialDependenceDisplay.from_estimator(\n",
    "    rf_model, X_val, top_features, \n",
    "    ax=ax, \n",
    "    kind=\"average\",\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "plt.suptitle('Partial Dependence Plots - Top 4 Features', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Individual Conditional Expectation (ICE) Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ICE plots for top feature\n",
    "top_feature = top_features[0]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "display = PartialDependenceDisplay.from_estimator(\n",
    "    rf_model, X_val, [top_feature],\n",
    "    kind=\"both\",  # Shows both average PDP and individual ICE curves\n",
    "    ax=ax,\n",
    "    random_state=RANDOM_SEED,\n",
    "    ice_lines_kw={\"alpha\": 0.1}\n",
    ")\n",
    "plt.suptitle(f'ICE Plot for {top_feature}', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\u26a0\ufe0f Interpretation Notes:\")\n",
    "print(f\"  - PDP shows average effect of {top_feature} on predictions\")\n",
    "print(f\"  - ICE curves show effect for individual samples\")\n",
    "print(f\"  - Wide spread in ICE = interactions with other features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Error Analysis\n",
    "\n",
    "### 5.1 Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute residuals\n",
    "residuals = y_val - y_val_pred\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Residual plot\n",
    "axes[0].scatter(y_val_pred, residuals, alpha=0.3)\n",
    "axes[0].axhline(y=0, color='r', linestyle='--')\n",
    "axes[0].set_xlabel('Predicted Values')\n",
    "axes[0].set_ylabel('Residuals')\n",
    "axes[0].set_title('Residual Plot')\n",
    "\n",
    "# Residual histogram\n",
    "axes[1].hist(residuals, bins=50, edgecolor='black')\n",
    "axes[1].set_xlabel('Residuals')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Residual Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nResidual Statistics:\")\n",
    "print(f\"  Mean: {residuals.mean():.4f}\")\n",
    "print(f\"  Std: {residuals.std():.4f}\")\n",
    "print(f\"  Median: {residuals.median():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Segment Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze errors by segments\n",
    "# Create price segments\n",
    "X_val_analysis = X_val.copy()\n",
    "X_val_analysis['y_true'] = y_val.values\n",
    "X_val_analysis['y_pred'] = y_val_pred\n",
    "X_val_analysis['abs_error'] = np.abs(residuals)\n",
    "\n",
    "# Segment by top feature\n",
    "top_feature = importance_df.iloc[0]['feature']\n",
    "X_val_analysis[f'{top_feature}_segment'] = pd.qcut(X_val_analysis[top_feature], q=4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
    "\n",
    "# Error by segment\n",
    "segment_errors = X_val_analysis.groupby(f'{top_feature}_segment').agg({\n",
    "    'abs_error': ['mean', 'median', 'std'],\n",
    "    'y_true': 'count'\n",
    "}).round(4)\n",
    "\n",
    "print(f\"\\n=== ERROR ANALYSIS BY {top_feature} SEGMENT ===\")\n",
    "print(segment_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize errors by segment\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "X_val_analysis.boxplot(column='abs_error', by=f'{top_feature}_segment', ax=ax)\n",
    "ax.set_xlabel(f'{top_feature} Quartile')\n",
    "ax.set_ylabel('Absolute Error')\n",
    "ax.set_title(f'Error Distribution by {top_feature} Segment')\n",
    "plt.suptitle('')  # Remove default title\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd PAUSE-AND-DO Exercise 2 (10 minutes)\n",
    "\n",
    "**Task:** Run segment error analysis and identify one failure segment.\n",
    "\n",
    "**Instructions:**\n",
    "1. Review the segment error analysis above\n",
    "2. Identify which segment has the highest error\n",
    "3. Propose a hypothesis for why this segment performs poorly\n",
    "4. Suggest one potential improvement\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR SEGMENT ANALYSIS HERE:\n",
    "\n",
    "**Highest Error Segment:**  \n",
    "[Which segment and why?]\n",
    "\n",
    "**Hypothesis:**  \n",
    "[Why does this segment have higher errors?]\n",
    "\n",
    "**Potential Improvement:**  \n",
    "[What could help reduce errors in this segment?]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interpretation Narrative Template (Evidence-Based)\n",
    "\n",
    "### 6.1 Model Strengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Strengths (Evidence-Based):**\n",
    "\n",
    "1. **Overall Performance**: The Random Forest model achieves an R\u00b2 of [X.XX] on the validation set, explaining [XX]% of variance in housing prices.\n",
    "\n",
    "2. **Key Drivers**: Feature importance analysis reveals that [Top Feature] is the strongest predictor, with a permutation importance of [X.XX], followed by [Second Feature].\n",
    "\n",
    "3. **Predictive Patterns**: Partial dependence plots show [describe pattern] relationship between [feature] and predictions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Model Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Limitations (Honest Assessment):**\n",
    "\n",
    "1. **Segment Performance Gaps**: Error analysis reveals systematically higher errors in [segment description], with mean absolute error of [X.XX] compared to overall MAE of [X.XX].\n",
    "\n",
    "2. **Feature Interactions**: ICE plots show substantial variation in individual effects, suggesting complex interactions that the model may not fully capture.\n",
    "\n",
    "3. **Residual Patterns**: Residual analysis indicates [describe pattern], suggesting [interpretation].\n",
    "\n",
    "4. **Generalization Concerns**: The model is trained on [timeframe/geography], and may not generalize to [different context].\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recommendations:**\n",
    "\n",
    "1. **Use with Caution**: Apply elevated scrutiny to predictions for [high-error segment].\n",
    "\n",
    "2. **Feature Engineering**: Consider engineering additional features to capture [identified interaction].\n",
    "\n",
    "3. **Model Monitoring**: Track permutation importance stability over time to detect feature drift.\n",
    "\n",
    "4. **Error Bounds**: When communicating predictions, include confidence intervals, especially for [segment].\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Project Milestone 3 Scaffold\n",
    "\n",
    "### 7.1 Deliverable Checklist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Project Milestone 3 Requirements:**\n",
    "\n",
    "- [ ] Updated model comparison table (baseline vs improved models)\n",
    "- [ ] Champion model selection with justification\n",
    "- [ ] Permutation importance plot and interpretation\n",
    "- [ ] PDP/ICE plots for top 3-4 features\n",
    "- [ ] Segment error analysis with findings\n",
    "- [ ] Evidence-based interpretation narrative\n",
    "- [ ] Model limitations section (honest assessment)\n",
    "- [ ] Next steps and recommendations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Wrap-Up: Key Takeaways\n",
    "\n",
    "### What We Learned Today:\n",
    "\n",
    "1. **Permutation Importance**: Model-agnostic method to measure feature importance\n",
    "2. **Partial Dependence**: Visualizing average effect of features on predictions\n",
    "3. **ICE Plots**: Understanding individual-level feature effects and interactions\n",
    "4. **Error Analysis**: Finding systematic failure patterns through segmentation\n",
    "5. **Honest Communication**: Evidence-based interpretation with clear limitations\n",
    "\n",
    "### Interpretation Best Practices:\n",
    "\n",
    "- \u2713 Always compute importance on validation/test data, not training data\n",
    "- \u2713 Report standard deviations to show stability\n",
    "- \u2713 Check for correlated features when interpreting importance\n",
    "- \u2713 Use PDP cautiously when features are highly correlated\n",
    "- \u2713 Conduct segment analysis to find failure modes\n",
    "- \u2713 Be honest about limitations and uncertainties\n",
    "\n",
    "### Remember:\n",
    "\n",
    "> **\"Interpretation is about honest communication, not selling the model.\"**  \n",
    "> Report what you find, including limitations and failure modes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n\n- scikit-learn User Guide: [Inspection Tools](https://scikit-learn.org/stable/inspection.html) (permutation importance, partial dependence)\n- Molnar, C. (2022). *Interpretable Machine Learning*. [Online book](https://christophm.github.io/interpretable-ml-book/)\n- James, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). *An Introduction to Statistical Learning with Python* (ISLP). Springer.\n- Breiman, L. (2001). \"Random Forests.\" *Machine Learning*, 45(1), 5-32.\n\n---\n\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "Thank you!\n",
    "\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}