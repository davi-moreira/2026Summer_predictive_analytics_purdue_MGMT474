{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression That Actually Works - Features, Interactions, Diagnostics\n",
    "\n",
    "<hr>\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/notebooks/figures/mgmt_474_ai_logo_02-modified.png\" width=\"200\"/>\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "# <center><a class=\"tocSkip\"></center>\n",
    "# <center>MGMT47400 Predictive Analytics</center>\n",
    "# <center>Professor: Davi Moreira </center>\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/blob/main/notebooks/04_linear_features_diagnostics_student.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Fit and interpret linear regression in a pipeline\n",
    "2. Create interaction/polynomial features responsibly\n",
    "3. Diagnose underfit/overfit using validation results\n",
    "4. Use residual analysis to spot nonlinearity and heteroskedasticity\n",
    "5. Translate coefficients into business meaning (with caveats)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "> **ðŸ“‹ Participation Reminder:** This notebook contains **2 PAUSE-AND-DO exercises**. You are expected to complete all exercises before submitting your notebook.\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## ðŸ’¼ Why This Matters: What Drives the Price?\n\nYour baseline model at **HomeValue Analytics** beats the average, but the CEO wants more: *\"Can you tell me which features drive prices the most? Why are coastal houses more expensive?\"* Stakeholders don't just want predictions â€” they want explanations.\n\nLinear regression provides both: a prediction and a coefficient for every feature. But you also need to diagnose where the model struggles â€” are residuals random, or do they reveal systematic blind spots?\n\n> **Today's focus:** Building an interpretable linear regression for house prices, reading coefficients as business insights, and diagnosing model weaknesses.\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport warnings\n\nwarnings.filterwarnings('ignore')\npd.set_option('display.precision', 4)\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (10, 6)\nRANDOM_SEED = 474\nnp.random.seed(RANDOM_SEED)\nprint(\"âœ“ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The `Setup complete!` message confirms that all imports loaded without error. This notebook's import list extends the previous one with `PolynomialFeatures` (for automatic feature expansion) from scikit-learn's preprocessing module. The same display settings (`display.precision = 4`, `whitegrid` style, `figure.figsize = (10, 6)`) and **RANDOM_SEED = 474** ensure visual and numerical consistency with earlier notebooks.\n",
    "\n",
    "**Why this matters:** Verifying imports at the top of the notebook catches environment mismatches early. If you are running in Google Colab, all these packages come pre-installed; on a local machine you may need `pip install scikit-learn`.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Create Splits\n",
    "\n",
    "We continue with the California Housing dataset used in the previous notebook. The same 60/20/20 split and `RANDOM_SEED = 474` guarantee that our partitions are identical, making all comparisons across notebooks valid.\n",
    "Having a consistent baseline dataset lets us isolate the effect of new modeling techniques rather than confounding them with different data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Gemini Prompt:** \"Load the California Housing dataset from scikit-learn as a DataFrame. Separate the target column MedHouseVal from the features. Then split the data into train (60%), validation (20%), and test (20%) sets using a two-step train_test_split with random_state=474. Print the size of each partition.\"\n",
    ">\n",
    "> **After running, verify:**\n",
    "> - Train set has approximately 12,384 samples (60% of ~20,640)\n",
    "> - Validation and test sets each have approximately 4,128 samples (20% each)\n",
    "> - Target variable y contains only MedHouseVal values, not included in X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "california = fetch_california_housing(as_frame=True)\n",
    "df = california.frame\n",
    "X = df.drop(columns=['MedHouseVal'])\n",
    "y = df['MedHouseVal']\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.20, random_state=RANDOM_SEED)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=RANDOM_SEED)\n",
    "\n",
    "print(f\"Train: {len(X_train)} | Val: {len(X_val)} | Test: {len(X_test)} (locked)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The printed summary shows the same partition sizes as the previous notebook: roughly **12,384 training**, **4,128 validation**, and **4,128 test** samples, with the test set marked `(locked)`. Because we use the identical `RANDOM_SEED = 474` and the same two-stage `train_test_split` procedure, every row lands in exactly the same partition as before. This reproducibility is essential: if the splits differed, any performance comparison with the previous notebook would be meaningless.\n",
    "\n",
    "**Key takeaway:** Consistent splits across notebooks let you attribute performance changes entirely to modeling choices, not to random variation in the data partition.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline Linear Model with Preprocessing\n",
    "\n",
    "Before adding any feature engineering, we fit the same `StandardScaler` + `LinearRegression` pipeline from the previous notebook. This gives us a clean reference point: the best a plain linear model can do with the original 8 features.\n",
    "We report both train and validation RÂ² as well as the gap between them, which is our first diagnostic for overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Gemini Prompt:** \"Create a scikit-learn Pipeline with two steps: StandardScaler for feature normalization and LinearRegression as the estimator. Fit it on the training data, then compute and print the R-squared score on both the training and validation sets, along with the overfitting gap (train R-squared minus val R-squared).\"\n",
    ">\n",
    "> **After running, verify:**\n",
    "> - Both Train R-squared and Val R-squared are in the range 0.59--0.62\n",
    "> - The overfit gap is very small (close to 0.00), indicating low variance\n",
    "> - Pipeline has exactly two named steps: scaler and regressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple linear regression pipeline\n",
    "baseline_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "baseline_pipeline.fit(X_train, y_train)\n",
    "\n",
    "train_score = baseline_pipeline.score(X_train, y_train)\n",
    "val_score = baseline_pipeline.score(X_val, y_val)\n",
    "\n",
    "print(\"=== BASELINE LINEAR MODEL ===\")\n",
    "print(f\"Train RÂ²: {train_score:.4f}\")\n",
    "print(f\"Val RÂ²: {val_score:.4f}\")\n",
    "print(f\"Overfit gap: {train_score - val_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The baseline linear model reports **Train RÂ²** and **Val RÂ²**, both likely in the range of **0.60--0.61**. The \"Overfit gap\" (train minus validation RÂ²) should be very small (under **0.01**), confirming that a plain 8-feature linear model does not overfit on this dataset. This is expected: ordinary linear regression with few features relative to thousands of training samples has low variance.\n",
    "\n",
    "**Why this matters:** This baseline RÂ² becomes the number to beat. Every feature-engineering step below must be judged by whether it raises validation RÂ² without dramatically increasing the overfit gap.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Coefficient Interpretation\n",
    "\n",
    "### Understanding Linear Regression Coefficients\n",
    "\n",
    "**What coefficients tell you:**\n",
    "- Direction of relationship (positive/negative)\n",
    "- Relative importance (after scaling)\n",
    "- Magnitude of effect (with caveats)\n",
    "\n",
    "**Interpretation caveats:**\n",
    "- Only valid if features are scaled consistently\n",
    "- Assumes linear relationship\n",
    "- Affected by multicollinearity\n",
    "- Correlation â‰  causation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Gemini Prompt:** \"Extract the linear regression coefficients from the fitted baseline pipeline, pair each with its feature name, and sort by absolute value. Display the sorted table, then create a horizontal bar chart showing each coefficient. Also print the intercept and a note explaining that positive coefficients increase predictions while negative ones decrease them.\"\n",
    ">\n",
    "> **After running, verify:**\n",
    "> - MedInc (median income) has the largest absolute coefficient\n",
    "> - Bar chart has a red dashed vertical line at x=0 separating positive from negative effects\n",
    "> - Intercept value is printed below the chart (should be close to the mean of y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coefficients\n",
    "coefficients = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Coefficient': baseline_pipeline.named_steps['regressor'].coef_,\n",
    "    'Abs_Coefficient': np.abs(baseline_pipeline.named_steps['regressor'].coef_)\n",
    "}).sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(\"=== LINEAR REGRESSION COEFFICIENTS ===\")\n",
    "print(coefficients)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(coefficients['Feature'], coefficients['Coefficient'])\n",
    "plt.xlabel('Coefficient Value (after scaling)')\n",
    "plt.title('Feature Importance via Linear Regression Coefficients')\n",
    "plt.axvline(x=0, color='red', linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nIntercept: {baseline_pipeline.named_steps['regressor'].intercept_:.4f}\")\n",
    "print(f\"\\nðŸ’¡ Positive coefficients increase the prediction\")\n",
    "print(f\"ðŸ’¡ Negative coefficients decrease the prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The table lists all 8 features sorted by absolute coefficient magnitude. Because features were standardized first, the coefficient sizes are directly comparable. **MedInc** (median income) almost certainly dominates, reflecting the well-known strong correlation between neighborhood income and house prices. The horizontal bar chart makes this hierarchy visual: long bars indicate influential features, and the direction (positive or negative) shows whether the feature increases or decreases predicted value. The printed intercept is the model's predicted value when all standardized features equal zero (i.e., at the training-set mean of every feature).\n",
    "\n",
    "**Key takeaway:** Standardized coefficients are useful for ranking feature importance, but remember that they measure *association*, not *causation*. A large coefficient on `Latitude` does not mean moving north causes price changes; it reflects geographic price patterns in the data.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering: Interactions\n",
    "\n",
    "### When to Add Interactions\n",
    "\n",
    "Interactions capture **combined effects** of features:\n",
    "- Income Ã— Location might matter more than either alone\n",
    "- Age Ã— Size might have nonlinear effects\n",
    "\n",
    "**Warning:** Interactions increase features exponentially!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Gemini Prompt:** \"Manually create an interaction feature by multiplying MedInc and AveRooms columns in both the training and validation sets. Fit a new StandardScaler + LinearRegression pipeline on the augmented training data (now 9 features), evaluate on both sets, and print the R-squared improvement over the baseline model.\"\n",
    ">\n",
    "> **After running, verify:**\n",
    "> - New column MedInc_x_AveRooms appears in both training and validation DataFrames\n",
    "> - Feature count increased from 8 to 9\n",
    "> - Improvement over baseline is printed (may be positive or near-zero)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a simple interaction manually first\n",
    "# Example: interaction between MedInc and AveRooms\n",
    "X_train_interact = X_train.copy()\n",
    "X_val_interact = X_val.copy()\n",
    "\n",
    "X_train_interact['MedInc_x_AveRooms'] = X_train['MedInc'] * X_train['AveRooms']\n",
    "X_val_interact['MedInc_x_AveRooms'] = X_val['MedInc'] * X_val['AveRooms']\n",
    "\n",
    "# Fit model with interaction\n",
    "interact_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "interact_pipeline.fit(X_train_interact, y_train)\n",
    "\n",
    "train_score_interact = interact_pipeline.score(X_train_interact, y_train)\n",
    "val_score_interact = interact_pipeline.score(X_val_interact, y_val)\n",
    "\n",
    "print(\"=== WITH INTERACTION FEATURE ===\")\n",
    "print(f\"Train RÂ²: {train_score_interact:.4f}\")\n",
    "print(f\"Val RÂ²: {val_score_interact:.4f}\")\n",
    "print(f\"\\nImprovement over baseline: {val_score_interact - val_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "After adding the hand-crafted `MedInc_x_AveRooms` interaction feature, the model now has **9 features** instead of 8. The printed train and validation RÂ² values let you assess whether this single interaction term improved prediction. The \"Improvement over baseline\" line shows the difference in validation RÂ² -- typically a small positive number (a few thousandths) for a single interaction. This modest gain reflects the fact that income-times-rooms captures a joint effect not fully represented by either variable alone.\n",
    "\n",
    "**Why this matters:** Manually crafting interactions forces you to think about domain knowledge: *which* feature combinations have a plausible combined effect? This is more interpretable than blindly generating all possible interactions.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ PAUSE-AND-DO Exercise 1 (5 minutes)\n",
    "\n",
    "**Task:** Add an interaction or polynomial block and measure validation change.\n",
    "\n",
    "**Instructions:**\n",
    "1. Use `PolynomialFeatures` to create degree=2 features\n",
    "2. Fit a new pipeline\n",
    "3. Compare validation scores\n",
    "4. Watch for overfitting!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Gemini Prompt:** \"Build a three-step Pipeline that first applies PolynomialFeatures with degree=2 and include_bias=False, then StandardScaler, then LinearRegression. Fit on the original training data, score on both train and validation, and print the R-squared values, overfit gap, original feature count, expanded feature count, and the feature-explosion multiplier.\"\n",
    ">\n",
    "> **After running, verify:**\n",
    "> - Feature count expands from 8 to 44 (originals + squares + pairwise products)\n",
    "> - Train R-squared is noticeably higher than baseline (likely above 0.65)\n",
    "> - Overfit gap is larger than baseline, signaling increased model complexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Add polynomial features\n",
    "\n",
    "poly_pipeline = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "poly_pipeline.fit(X_train, y_train)\n",
    "\n",
    "train_score_poly = poly_pipeline.score(X_train, y_train)\n",
    "val_score_poly = poly_pipeline.score(X_val, y_val)\n",
    "\n",
    "print(\"=== POLYNOMIAL FEATURES (degree=2) ===\")\n",
    "print(f\"Train RÂ²: {train_score_poly:.4f}\")\n",
    "print(f\"Val RÂ²: {val_score_poly:.4f}\")\n",
    "print(f\"Overfit gap: {train_score_poly - val_score_poly:.4f}\")\n",
    "\n",
    "# Count features\n",
    "n_features_original = X_train.shape[1]\n",
    "n_features_poly = poly_pipeline.named_steps['poly'].transform(X_train[:1]).shape[1]\n",
    "print(f\"\\nOriginal features: {n_features_original}\")\n",
    "print(f\"After polynomial (degree=2): {n_features_poly}\")\n",
    "print(f\"Feature explosion: {n_features_poly / n_features_original:.1f}x increase\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "With `PolynomialFeatures(degree=2)`, the feature count explodes from **8 to 44** (all original features plus their squares and pairwise products). The printed RÂ² values show a noticeable jump in both train and validation scores compared to the baseline. However, the overfit gap (train RÂ² minus validation RÂ²) is now larger, signaling that the added complexity is starting to memorize training-set noise. The \"Feature explosion\" line quantifies the cost: a **5.5x increase** in feature dimensionality.\n",
    "\n",
    "**Key takeaway:** Polynomial features offer a quick accuracy boost, but the curse of dimensionality is real. With 44 features and only ~12,000 training samples, you are approaching the regime where regularization (covered in the next notebook) becomes essential.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANALYSIS:\n",
    "\n",
    "**Observation 1: Performance**  \n",
    "[Did polynomial features improve validation score?]\n",
    "\n",
    "**Observation 2: Overfitting**  \n",
    "[Is the train-val gap larger now?]\n",
    "\n",
    "**Observation 3: Complexity**  \n",
    "[Is the added complexity worth the improvement?]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Residual Diagnostics\n",
    "\n",
    "### What to Look For:\n",
    "\n",
    "1. **Patterns in residual plots** â†’ Model is missing something (nonlinearity, interactions)\n",
    "2. **Funnel shape** â†’ Heteroskedasticity (variance changes with prediction)\n",
    "3. **Non-normal residuals** â†’ Outliers or wrong model family\n",
    "4. **Systematic errors in ranges** â†’ Model struggles in certain regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Gemini Prompt:** \"Generate validation predictions from both the baseline and polynomial pipelines. Compute residuals for each. Create a 2x2 subplot grid: top row shows scatter plots of residuals vs. predicted values (baseline left, polynomial right, each with a red dashed y=0 line), bottom row shows histograms of residual distributions (50 bins). Print MAE and RMSE for both models.\"\n",
    ">\n",
    "> **After running, verify:**\n",
    "> - Top-row scatter plots are centered around the y=0 reference line\n",
    "> - Polynomial model residuals (orange) show a tighter spread than baseline (blue)\n",
    "> - Printed MAE and RMSE for the polynomial model are lower than for the baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "y_pred_baseline = baseline_pipeline.predict(X_val)\n",
    "y_pred_poly = poly_pipeline.predict(X_val)\n",
    "\n",
    "residuals_baseline = y_val - y_pred_baseline\n",
    "residuals_poly = y_val - y_pred_poly\n",
    "\n",
    "# Residual comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Baseline residuals\n",
    "axes[0, 0].scatter(y_pred_baseline, residuals_baseline, alpha=0.5)\n",
    "axes[0, 0].axhline(y=0, color='r', linestyle='--')\n",
    "axes[0, 0].set_xlabel('Predicted')\n",
    "axes[0, 0].set_ylabel('Residuals')\n",
    "axes[0, 0].set_title('Baseline Model Residuals')\n",
    "\n",
    "# Polynomial residuals\n",
    "axes[0, 1].scatter(y_pred_poly, residuals_poly, alpha=0.5, color='orange')\n",
    "axes[0, 1].axhline(y=0, color='r', linestyle='--')\n",
    "axes[0, 1].set_xlabel('Predicted')\n",
    "axes[0, 1].set_ylabel('Residuals')\n",
    "axes[0, 1].set_title('Polynomial Model Residuals')\n",
    "\n",
    "# Baseline distribution\n",
    "axes[1, 0].hist(residuals_baseline, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].axvline(x=0, color='r', linestyle='--')\n",
    "axes[1, 0].set_xlabel('Residual')\n",
    "axes[1, 0].set_title('Baseline Residual Distribution')\n",
    "\n",
    "# Polynomial distribution\n",
    "axes[1, 1].hist(residuals_poly, bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[1, 1].axvline(x=0, color='r', linestyle='--')\n",
    "axes[1, 1].set_xlabel('Residual')\n",
    "axes[1, 1].set_title('Polynomial Residual Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=== RESIDUAL STATISTICS ===\")\n",
    "print(f\"Baseline MAE: {mean_absolute_error(y_val, y_pred_baseline):.4f}\")\n",
    "print(f\"Polynomial MAE: {mean_absolute_error(y_val, y_pred_poly):.4f}\")\n",
    "print(f\"\\nBaseline RMSE: {np.sqrt(mean_squared_error(y_val, y_pred_baseline)):.4f}\")\n",
    "print(f\"Polynomial RMSE: {np.sqrt(mean_squared_error(y_val, y_pred_poly)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The 2x2 grid compares residual behavior for the baseline linear model (left column) and the polynomial model (right column). **Top row** (residuals vs. predicted): look for patterns. A funnel shape means the model's error variance changes with the prediction level (heteroskedasticity). A curve means the model misses a nonlinear trend. **Bottom row** (residual histograms): a symmetric, bell-shaped distribution centered at zero is ideal. Heavy tails or skew indicate outliers or systematic mis-specification. The printed MAE and RMSE values quantify the improvement: the polynomial model should show lower error on both metrics.\n",
    "\n",
    "**Why this matters:** Residual plots reveal *what kind* of error your model makes, not just *how much*. Two models can have similar RÂ² but very different residual patterns. A model with random residuals is trustworthy; a model with patterned residuals is hiding systematic bias that could be fixed with better features or a different model family.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ PAUSE-AND-DO Exercise 2 (5 minutes)\n",
    "\n",
    "**Task:** Write a short diagnostic conclusion (what error patterns suggest).\n",
    "\n",
    "Look at the residual plots above and answer:\n",
    "1. Do you see any patterns (non-random scatter)?\n",
    "2. Is there a funnel shape (heteroskedasticity)?\n",
    "3. Does the polynomial model fix any issues?\n",
    "4. What would you try next?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR DIAGNOSTIC CONCLUSION:\n",
    "\n",
    "**Pattern Analysis:**  \n",
    "[Describe any patterns you see in residuals]\n",
    "\n",
    "**Heteroskedasticity:**  \n",
    "[Is variance constant across predictions?]\n",
    "\n",
    "**Model Comparison:**  \n",
    "[Which model has better residual behavior?]\n",
    "\n",
    "**Next Steps:**  \n",
    "[What would you try to improve the model?]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison Table\n",
    "\n",
    "With three models trained (baseline linear, manual interaction, and degree-2 polynomial), we consolidate their performance into a single DataFrame. This side-by-side view makes it easy to compare RÂ², MAE, and the overfit gap in one glance.\n",
    "The table also records the number of input features for each model, highlighting the cost of polynomial expansion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **Gemini Prompt:** \"Create a pandas DataFrame comparing three models (Baseline Linear, With Interaction, Polynomial deg=2) with columns for model name, feature count, Train R-squared, Val R-squared, overfit gap, and validation MAE. Print the table without the index. Then identify and print which model achieved the best validation R-squared.\"\n",
    ">\n",
    "> **After running, verify:**\n",
    "> - Table has exactly 3 rows and 6 columns (Model, Features, Train_R2, Val_R2, Overfit_Gap, Val_MAE)\n",
    "> - Feature counts are 8, 9, and 44 respectively\n",
    "> - Best model line names the model with the highest Val_R2 and prints its score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison = pd.DataFrame([\n",
    "    {\n",
    "        'Model': 'Baseline Linear',\n",
    "        'Features': X_train.shape[1],\n",
    "        'Train_R2': train_score,\n",
    "        'Val_R2': val_score,\n",
    "        'Overfit_Gap': train_score - val_score,\n",
    "        'Val_MAE': mean_absolute_error(y_val, y_pred_baseline)\n",
    "    },\n",
    "    {\n",
    "        'Model': 'With Interaction',\n",
    "        'Features': X_train_interact.shape[1],\n",
    "        'Train_R2': train_score_interact,\n",
    "        'Val_R2': val_score_interact,\n",
    "        'Overfit_Gap': train_score_interact - val_score_interact,\n",
    "        'Val_MAE': mean_absolute_error(y_val, interact_pipeline.predict(X_val_interact))\n",
    "    },\n",
    "    {\n",
    "        'Model': 'Polynomial (deg=2)',\n",
    "        'Features': n_features_poly,\n",
    "        'Train_R2': train_score_poly,\n",
    "        'Val_R2': val_score_poly,\n",
    "        'Overfit_Gap': train_score_poly - val_score_poly,\n",
    "        'Val_MAE': mean_absolute_error(y_val, y_pred_poly)\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"=== MODEL COMPARISON ===\")\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "# Best model\n",
    "best_idx = comparison['Val_R2'].idxmax()\n",
    "print(f\"\\nâœ“ Best validation RÂ²: {comparison.loc[best_idx, 'Model']} ({comparison.loc[best_idx, 'Val_R2']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The final comparison table shows three rows: **Baseline Linear** (8 features), **With Interaction** (9 features), and **Polynomial deg=2** (44 features). The columns include train RÂ², validation RÂ², the overfit gap, and validation MAE. The polynomial model likely wins on validation RÂ², but its overfit gap is the largest. The \"Best validation RÂ²\" line at the bottom names the winner. Notice the cost-benefit tradeoff: going from 8 to 44 features may yield only a few percentage points of RÂ² improvement while substantially increasing model complexity and the risk of overfitting.\n",
    "\n",
    "**Key takeaway:** Always weigh accuracy gains against complexity costs. A simpler model that performs almost as well is often preferable in production because it is easier to explain, faster to train, and less likely to degrade on new data.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Wrap-Up: Key Takeaways\n\n### What We Learned Today:\n\n1. **Coefficient Interpretation**: Requires scaling, caution about multicollinearity\n2. **Feature Engineering**: Interactions and polynomials can help, but watch overfitting\n3. **Residual Diagnostics**: Visual patterns reveal model inadequacies\n4. **Complexity Tradeoff**: More features â‰  better performance\n5. **Validation Discipline**: Always check validation scores, not just training\n\n### Critical Rules:\n\n> **\"Scale features before interpreting coefficients\"**\n\n> **\"Polynomial features explode exponentially - use sparingly\"**\n\n> **\"Residual plots never lie\"**\n\n### Next Steps:\n\n- Next notebook: Regularization (Ridge/Lasso) + Project proposal\n- Regularization will help control overfitting from complex features\n- Start thinking about your project dataset and target\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Participation Assignment Submission Instructions\n",
    "\n",
    "### To Submit This Notebook:\n",
    "\n",
    "1. **Complete all exercises**: Fill in both PAUSE-AND-DO exercise cells with your findings\n",
    "2. **Run All Cells**: Execute `Runtime â†’ Run all` to ensure everything works\n",
    "3. **Save a Copy**: `File â†’ Save a copy in Drive or Download the .ipynb extension`\n",
    "4. **Submit**: Upload your `.ipynb` file in the participation assignment you find in the course Brightspace page.\n",
    "\n",
    "### Before Submitting, Check:\n",
    "\n",
    "- [ ] All cells execute without errors\n",
    "- [ ] All outputs are visible\n",
    "- [ ] Both exercise responses are complete\n",
    "- [ ] Notebook is shared with correct permissions\n",
    "- [ ] You can explain every line of code you wrote\n",
    "\n",
    "### Next Step:\n",
    "\n",
    "Complete the **Quiz** in Brightspace (auto-graded)\n",
    "\n",
    "---\n",
    "\n",
    "## Bibliography\n\n- James, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). *An Introduction to Statistical Learning with Python* - Linear Regression chapter\n- Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning* - Linear methods\n- scikit-learn User Guide: [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n- scikit-learn User Guide: [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)\n\n---\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "Thank you!\n",
    "\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
