{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 11: Decision Trees - Interpretable Models with Sharp Edges\n",
    "\n",
    "**MGMT 47400 - Predictive Analytics**  \n",
    "**4-Week Online Course**  \n",
    "**Day 11 - Tuesday June 1, 2027**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/blob/main/notebooks/11_decision_trees.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Fit decision trees for regression/classification\n",
    "2. Control complexity (depth, min samples) to manage overfitting\n",
    "3. Interpret tree structure and failure modes\n",
    "4. Compare tree vs linear/logistic baselines under CV\n",
    "5. Document \"when a tree is the right tool\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer, fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.precision', 4)\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "print(\"âœ“ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Decision Tree Intuition\n",
    "\n",
    "### How Trees Make Decisions\n",
    "\n",
    "**Algorithm (CART - Classification and Regression Trees):**\n",
    "1. Start with all data at root\n",
    "2. Find best feature + threshold to split\n",
    "   - \"Best\" = maximize information gain (classification) or minimize MSE (regression)\n",
    "3. Create two child nodes\n",
    "4. Repeat recursively until stopping criteria\n",
    "\n",
    "**Example Decision Path:**\n",
    "```\n",
    "Is income > $50k?\n",
    "  â”œâ”€ No: Is age > 30?\n",
    "  â”‚   â”œâ”€ No: Predict class 0 (high risk)\n",
    "  â”‚   â””â”€ Yes: Predict class 1 (low risk)\n",
    "  â””â”€ Yes: Predict class 1 (low risk)\n",
    "```\n",
    "\n",
    "### Key Hyperparameters\n",
    "\n",
    "**Complexity control:**\n",
    "- `max_depth`: Maximum tree depth (prevents overfitting)\n",
    "- `min_samples_split`: Minimum samples to split a node\n",
    "- `min_samples_leaf`: Minimum samples in leaf node\n",
    "- `max_features`: Number of features to consider per split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification Tree Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = load_breast_cancer(as_frame=True)\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RANDOM_SEED, stratify=y)\n",
    "\n",
    "print(f\"Train: {len(X_train)} | Test: {len(X_test)}\")\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "\n",
    "# Fit a simple tree\n",
    "tree_clf = DecisionTreeClassifier(max_depth=3, random_state=RANDOM_SEED)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "train_score = tree_clf.score(X_train, y_train)\n",
    "test_score = tree_clf.score(X_test, y_test)\n",
    "\n",
    "print(f\"\\n=== DECISION TREE (max_depth=3) ===\")\n",
    "print(f\"Train accuracy: {train_score:.4f}\")\n",
    "print(f\"Test accuracy: {test_score:.4f}\")\n",
    "print(f\"Overfit gap: {train_score - test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the tree\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(\n",
    "    tree_clf,\n",
    "    feature_names=X.columns,\n",
    "    class_names=data.target_names,\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    fontsize=10\n",
    ")\n",
    "plt.title(\"Decision Tree Visualization (max_depth=3)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ’¡ Follow a path from root to leaf to see decision logic\")\n",
    "print(\"ðŸ’¡ Darker colors = more samples, purity of class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Overfitting Problem\n",
    "\n",
    "### Trees Without Constraints = Memorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different depths\n",
    "depths = [1, 2, 3, 5, 10, 20, None]  # None = unlimited\n",
    "results = []\n",
    "\n",
    "for depth in depths:\n",
    "    tree = DecisionTreeClassifier(max_depth=depth, random_state=RANDOM_SEED)\n",
    "    tree.fit(X_train, y_train)\n",
    "    \n",
    "    train_acc = tree.score(X_train, y_train)\n",
    "    test_acc = tree.score(X_test, y_test)\n",
    "    \n",
    "    results.append({\n",
    "        'max_depth': str(depth),\n",
    "        'train_acc': train_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'gap': train_acc - test_acc,\n",
    "        'n_leaves': tree.get_n_leaves()\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"=== DEPTH SWEEP ===\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy vs depth\n",
    "axes[0].plot(range(len(depths)), results_df['train_acc'], marker='o', label='Train', linewidth=2)\n",
    "axes[0].plot(range(len(depths)), results_df['test_acc'], marker='s', label='Test', linewidth=2)\n",
    "axes[0].set_xticks(range(len(depths)))\n",
    "axes[0].set_xticklabels(results_df['max_depth'])\n",
    "axes[0].set_xlabel('Max Depth')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Accuracy vs Tree Depth')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Overfit gap\n",
    "axes[1].bar(range(len(depths)), results_df['gap'], alpha=0.7, edgecolor='black')\n",
    "axes[1].set_xticks(range(len(depths)))\n",
    "axes[1].set_xticklabels(results_df['max_depth'])\n",
    "axes[1].set_xlabel('Max Depth')\n",
    "axes[1].set_ylabel('Train - Test Gap')\n",
    "axes[1].set_title('Overfitting vs Tree Depth')\n",
    "axes[1].axhline(y=0, color='r', linestyle='--')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Unrestricted trees achieve 100% training accuracy (pure overfitting!)\")\n",
    "print(\"ðŸ’¡ Best test performance is at moderate depth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ PAUSE-AND-DO Exercise 1 (10 minutes)\n",
    "\n",
    "**Task:** Run a depth sweep and choose depth based on CV.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE: Depth sweep with cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "depths_to_test = [2, 3, 4, 5, 6, 7, 8, 10, 15]\n",
    "\n",
    "cv_results = []\n",
    "for depth in depths_to_test:\n",
    "    tree = DecisionTreeClassifier(max_depth=depth, random_state=RANDOM_SEED)\n",
    "    scores = cross_val_score(tree, X_train, y_train, cv=cv, scoring='roc_auc')\n",
    "    \n",
    "    cv_results.append({\n",
    "        'max_depth': depth,\n",
    "        'cv_mean': scores.mean(),\n",
    "        'cv_std': scores.std()\n",
    "    })\n",
    "\n",
    "cv_df = pd.DataFrame(cv_results)\n",
    "print(\"=== CV DEPTH SWEEP ===\")\n",
    "print(cv_df.to_string(index=False))\n",
    "\n",
    "best_depth = cv_df.loc[cv_df['cv_mean'].idxmax(), 'max_depth']\n",
    "best_score = cv_df.loc[cv_df['cv_mean'].idxmax(), 'cv_mean']\n",
    "\n",
    "print(f\"\\nâœ“ Best depth by CV: {best_depth} (CV ROC-AUC = {best_score:.4f})\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(cv_df['max_depth'], cv_df['cv_mean'], yerr=cv_df['cv_std'], \n",
    "             marker='o', capsize=5, linewidth=2)\n",
    "plt.axvline(x=best_depth, color='r', linestyle='--', label=f'Best depth = {best_depth}')\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('CV ROC-AUC')\n",
    "plt.title('Cross-Validation: Depth Selection')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tree vs Linear Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare tree vs logistic regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "models = {\n",
    "    'Decision Tree (best)': DecisionTreeClassifier(max_depth=int(best_depth), random_state=RANDOM_SEED),\n",
    "    'Logistic Regression': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', LogisticRegression(random_state=RANDOM_SEED, max_iter=1000))\n",
    "    ])\n",
    "}\n",
    "\n",
    "comparison = []\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='roc_auc')\n",
    "    \n",
    "    # Fit on full train for test evaluation\n",
    "    model.fit(X_train, y_train)\n",
    "    test_score = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "    \n",
    "    comparison.append({\n",
    "        'Model': name,\n",
    "        'CV_Mean': scores.mean(),\n",
    "        'CV_Std': scores.std(),\n",
    "        'Test_Score': test_score\n",
    "    })\n",
    "\n",
    "comp_df = pd.DataFrame(comparison)\n",
    "print(\"=== TREE VS LINEAR COMPARISON ===\")\n",
    "print(comp_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nðŸ’¡ Which performs better depends on data structure\")\n",
    "print(\"ðŸ’¡ Trees handle non-linear relationships naturally\")\n",
    "print(\"ðŸ’¡ Linear models need manual feature engineering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ PAUSE-AND-DO Exercise 2 (10 minutes)\n",
    "\n",
    "**Task:** Write 3 observed tree failure modes (with evidence).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANALYSIS: Tree Failure Modes\n",
    "\n",
    "**Failure Mode 1: Overfitting**  \n",
    "[Evidence from depth sweep - what happened with unlimited depth?]\n",
    "\n",
    "**Failure Mode 2: Instability**  \n",
    "[Evidence from CV std - how much do scores vary across folds?]\n",
    "\n",
    "**Failure Mode 3: Extrapolation**  \n",
    "[Trees can only predict values seen in training - what's the implication?]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Regression Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load regression dataset\n",
    "california = fetch_california_housing(as_frame=True)\n",
    "X_reg = california.data\n",
    "y_reg = california.target\n",
    "\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.3, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Depth sweep for regression\n",
    "depths_reg = [2, 3, 5, 7, 10, 15, None]\n",
    "reg_results = []\n",
    "\n",
    "for depth in depths_reg:\n",
    "    tree_reg = DecisionTreeRegressor(max_depth=depth, random_state=RANDOM_SEED)\n",
    "    tree_reg.fit(X_train_reg, y_train_reg)\n",
    "    \n",
    "    train_r2 = tree_reg.score(X_train_reg, y_train_reg)\n",
    "    test_r2 = tree_reg.score(X_test_reg, y_test_reg)\n",
    "    \n",
    "    reg_results.append({\n",
    "        'max_depth': str(depth),\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'gap': train_r2 - test_r2\n",
    "    })\n",
    "\n",
    "reg_df = pd.DataFrame(reg_results)\n",
    "print(\"=== REGRESSION TREE DEPTH SWEEP ===\")\n",
    "print(reg_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nðŸ’¡ Same overfitting pattern as classification\")\n",
    "print(\"ðŸ’¡ Unrestricted tree memorizes training data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. When to Use Decision Trees\n",
    "\n",
    "### Strengths\n",
    "- âœ“ Interpretable (can visualize and explain)\n",
    "- âœ“ Handle non-linear relationships naturally\n",
    "- âœ“ No feature scaling needed\n",
    "- âœ“ Handle mixed data types (numeric + categorical)\n",
    "- âœ“ Capture interactions automatically\n",
    "- âœ“ Fast to train and predict\n",
    "\n",
    "### Weaknesses\n",
    "- âœ— High variance (unstable - small data changes â†’ different tree)\n",
    "- âœ— Easy to overfit\n",
    "- âœ— Poor extrapolation (can't predict outside training range)\n",
    "- âœ— Biased toward features with many values\n",
    "- âœ— Step-function boundaries (not smooth)\n",
    "\n",
    "### Use Decision Trees When:\n",
    "1. Interpretability is critical\n",
    "2. You need quick baseline\n",
    "3. Relationships are highly non-linear\n",
    "4. You'll use ensemble methods (Random Forests, Boosting)\n",
    "\n",
    "### Avoid Decision Trees When:\n",
    "1. High-dimensional sparse data\n",
    "2. Need smooth decision boundaries\n",
    "3. Small datasets (unstable)\n",
    "4. Extrapolation required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Wrap-Up: Key Takeaways\n",
    "\n",
    "### What We Learned Today:\n",
    "\n",
    "1. **Tree Mechanics**: Recursive partitioning with greedy splits\n",
    "2. **Overfitting Risk**: Unrestricted trees memorize perfectly\n",
    "3. **Complexity Control**: depth, min_samples_split, min_samples_leaf\n",
    "4. **Interpretability**: Can visualize exact decision logic\n",
    "5. **Limitations**: High variance, poor extrapolation\n",
    "\n",
    "### Critical Rules:\n",
    "\n",
    "> **\"Never use unrestricted trees in production\"**\n",
    "\n",
    "> **\"Always tune depth with cross-validation\"**\n",
    "\n",
    "> **\"Trees are building blocks for ensembles\"**\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Day 12: Random Forests (ensemble of trees)\n",
    "- We'll fix tree instability with bagging\n",
    "- Learn feature importance from forests\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "- James, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). *An Introduction to Statistical Learning with Python* - Tree-Based Methods (trees, pruning)\n",
    "- Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning* - CART foundations and complexity control\n",
    "- Breiman, L., Friedman, J., Stone, C. J., & Olshen, R. A. (1984). *Classification and Regression Trees*\n",
    "- scikit-learn User Guide: [Decision Trees](https://scikit-learn.org/stable/modules/tree.html)\n",
    "\n",
    "---\n",
    "\n",
    "**End of Day 11 Notebook**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
