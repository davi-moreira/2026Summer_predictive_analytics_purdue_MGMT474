{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment Thinking - Reproducibility, Monitoring, and Don't Ship a Notebook\n",
    "\n",
    "<hr>\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/notebooks/figures/mgmt_474_ai_logo_02-modified.png\" width=\"200\"/>\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "# <center><a class=\"tocSkip\"></center>\n",
    "# <center>MGMT47400 Predictive Analytics</center>\n",
    "# <center>Professor: Davi Moreira </center>\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/blob/main/notebooks/18_reproducibility_monitoring.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Package a model pipeline reproducibly (single function, fixed preprocessing)\n",
    "2. Save/load model artifacts and ensure consistent inference\n",
    "3. Define monitoring signals (data drift, performance drift, calibration drift)\n",
    "4. Create a minimal production checklist and risk log\n",
    "5. Prepare the project notebook for executive-facing reproducibility\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup: Installs, Imports, Seeds, Display Settings\n",
    "\n",
    "First, let's set up our environment with all necessary packages and configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n# !pip install pandas numpy matplotlib seaborn scikit-learn joblib --quiet\n\n# Core imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nimport joblib\nimport warnings\nfrom datetime import datetime\nimport json\n\n# Display settings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', None)\npd.set_option('display.precision', 3)\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (10, 6)\n\n# Set random seed for reproducibility\nRANDOM_SEED = 474\nnp.random.seed(RANDOM_SEED)\n\nprint(\"\u2713 Setup complete!\")\nprint(f\"Random seed: {RANDOM_SEED}\")\nprint(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The output confirms `Setup complete!` alongside the locked **random seed\n",
    "(474)** and a timestamp. The timestamp is informational only -- it lets you\n",
    "compare runs across sessions. All the heavy imports (`joblib`, `Pipeline`,\n",
    "`StandardScaler`, `json`) are in place, which means we are ready to build,\n",
    "save, load, and evaluate a reproducible pipeline.\n",
    "\n",
    "**Why this matters:** Recording the seed and timestamp at the top of the\n",
    "notebook is a simple but powerful reproducibility habit.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. Refactor into Functions: train_model(), predict(), evaluate()\n",
    "\n",
    "### Why Refactor?\n",
    "\n",
    "> **\"Notebooks are for exploration. Functions are for production.\"**  \n",
    "> Reproducibility requires separating configuration from code.\n",
    "\n",
    "**Key principles:**\n",
    "- Separate configuration (hyperparameters, paths, seeds) from logic\n",
    "- Wrap training logic in a single function that returns a fitted pipeline\n",
    "- Create prediction and evaluation functions that work with the saved pipeline\n",
    "- Make everything reproducible with fixed seeds and saved artifacts\n",
    "\n",
    "### 2.1 Configuration Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration dictionary - all settings in one place\n",
    "CONFIG = {\n",
    "    'data': {\n",
    "        'test_size': 0.2,\n",
    "        'val_size': 0.25,  # 0.25 of remaining 0.8 = 0.2 overall\n",
    "        'random_seed': RANDOM_SEED\n",
    "    },\n",
    "    'preprocessing': {\n",
    "        'scaler': 'standard',  # 'standard', 'minmax', or None\n",
    "    },\n",
    "    'model': {\n",
    "        'type': 'logistic_regression',  # 'logistic_regression' or 'random_forest'\n",
    "        'hyperparameters': {\n",
    "            'C': 1.0,\n",
    "            'max_iter': 1000,\n",
    "            'random_state': RANDOM_SEED\n",
    "        }\n",
    "    },\n",
    "    'paths': {\n",
    "        'model_artifact': 'model_pipeline.joblib',\n",
    "        'config_artifact': 'model_config.json',\n",
    "        'metrics_artifact': 'training_metrics.json'\n",
    "    },\n",
    "    'metadata': {\n",
    "        'project_name': 'Predictive Analytics Project',\n",
    "        'author': 'Your Name',\n",
    "        'created_date': datetime.now().strftime('%Y-%m-%d')\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\u2713 Configuration loaded\")\n",
    "print(json.dumps(CONFIG, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The full `CONFIG` dictionary is printed as pretty-printed JSON. Check that\n",
    "the **data** section matches the 60/20/20 split ratios, the\n",
    "**preprocessing** section specifies `standard` scaling, and the **model**\n",
    "section names `logistic_regression` with `C=1.0` and the correct seed.\n",
    "The **paths** section lists the three artifact filenames that will be\n",
    "written to disk later.\n",
    "\n",
    "**Key takeaway:** Externalising every tuneable setting into a single config\n",
    "dictionary means you can reproduce *or modify* any experiment by changing\n",
    "one JSON block instead of hunting through scattered code cells.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "### 2.2 Load Sample Data\n",
    "\n",
    "We use scikit-learn's built-in breast-cancer dataset for demonstration\n",
    "because it loads instantly in any Colab environment with no file downloads.\n",
    "The dataset has 569 samples and 30 numeric features describing cell-nucleus\n",
    "measurements. After loading, we print shape and target counts so we can\n",
    "verify the data before splitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demonstration, we'll use the breast cancer dataset\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load data\n",
    "data = load_breast_cancer(as_frame=True)\n",
    "df = data.frame\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=['target'])\n",
    "y = df['target']\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"Target distribution: {y.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The dataset has **569 samples** and **30 features**. The target distribution\n",
    "shows the counts of malignant (0) and benign (1) cases. Because the classes\n",
    "are not perfectly balanced, the upcoming split will use stratification to\n",
    "keep proportions consistent across train, validation, and test sets.\n",
    "\n",
    "**Why this matters:** Printing shape and target counts immediately after\n",
    "loading is a basic but essential data-quality checkpoint.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "### 2.3 Create Splits\n",
    "\n",
    "We apply the standard 60/20/20 train-validation-test split using the seed\n",
    "stored in `CONFIG`. Stratification on the target ensures that class\n",
    "proportions are preserved in every fold. Printing split sizes and\n",
    "percentages immediately after splitting serves as a sanity check that the\n",
    "ratios are correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/val/test splits\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=CONFIG['data']['test_size'], \n",
    "    random_state=CONFIG['data']['random_seed'],\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=CONFIG['data']['val_size'], \n",
    "    random_state=CONFIG['data']['random_seed'],\n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"=== SPLIT SIZES ===\")\n",
    "print(f\"Train: {len(X_train)} samples ({len(X_train)/len(df)*100:.1f}%)\")\n",
    "print(f\"Validation: {len(X_val)} samples ({len(X_val)/len(df)*100:.1f}%)\")\n",
    "print(f\"Test: {len(X_test)} samples ({len(X_test)/len(df)*100:.1f}%)\")\n",
    "print(f\"\\n\u2713 Splits created with seed {CONFIG['data']['random_seed']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The split summary shows the number of samples and percentage for **Train**,\n",
    "**Validation**, and **Test** sets, which should approximate 60 %, 20 %, and\n",
    "20 % of the full 569 samples. The confirmation line reprints the seed used.\n",
    "If any percentage is notably off, it may indicate an incorrect `test_size`\n",
    "or `val_size` in the config.\n",
    "\n",
    "**Key takeaway:** Printing split sizes right after creation is a fast way\n",
    "to catch configuration errors before they propagate into model training.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "### 2.4 Train Function: Fit Once, Run Anywhere\n",
    "\n",
    "Wrapping the model inside a scikit-learn `Pipeline` ensures that\n",
    "preprocessing (e.g., `StandardScaler`) and the estimator travel together as\n",
    "a single artifact. The `train_model()` function reads all settings from the\n",
    "`CONFIG` dictionary, so changing the model type or hyper-parameters never\n",
    "requires editing the function body -- only the config.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, y_train, config):\n",
    "    \"\"\"\n",
    "    Train a model pipeline from scratch.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : pd.DataFrame\n",
    "        Training features\n",
    "    y_train : pd.Series\n",
    "        Training target\n",
    "    config : dict\n",
    "        Configuration dictionary with preprocessing and model settings\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pipeline : sklearn.pipeline.Pipeline\n",
    "        Fitted pipeline ready for prediction\n",
    "    \"\"\"\n",
    "    # Build pipeline steps\n",
    "    steps = []\n",
    "    \n",
    "    # Add scaler if specified\n",
    "    if config['preprocessing']['scaler'] == 'standard':\n",
    "        steps.append(('scaler', StandardScaler()))\n",
    "    \n",
    "    # Add model\n",
    "    if config['model']['type'] == 'logistic_regression':\n",
    "        model = LogisticRegression(**config['model']['hyperparameters'])\n",
    "    elif config['model']['type'] == 'random_forest':\n",
    "        model = RandomForestClassifier(**config['model']['hyperparameters'])\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {config['model']['type']}\")\n",
    "    \n",
    "    steps.append(('model', model))\n",
    "    \n",
    "    # Create and fit pipeline\n",
    "    pipeline = Pipeline(steps)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"\u2713 Pipeline trained: {len(steps)} steps\")\n",
    "    for step_name, step_obj in pipeline.steps:\n",
    "        print(f\"  - {step_name}: {type(step_obj).__name__}\")\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "No output is produced here because we are *defining* the function, not\n",
    "calling it yet. When `train_model()` is later invoked, it will print the\n",
    "number of pipeline steps and the class name of each step (e.g.,\n",
    "`StandardScaler`, `LogisticRegression`). This printout acts as a quick\n",
    "audit: you can confirm that scaling and the correct estimator are both\n",
    "present in the pipeline.\n",
    "\n",
    "**Why this matters:** Defining training logic inside a function -- rather\n",
    "than in loose notebook cells -- is the first step toward production-grade\n",
    "code.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "### 2.5 Predict Function\n",
    "\n",
    "The `predict()` function takes a fitted pipeline and a feature matrix, then\n",
    "returns both hard predictions and probability estimates (when available).\n",
    "Separating prediction from training makes it easy to swap in a loaded\n",
    "artifact later without duplicating code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(pipeline, X):\n",
    "    \"\"\"\n",
    "    Make predictions using a fitted pipeline.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pipeline : sklearn.pipeline.Pipeline\n",
    "        Fitted pipeline\n",
    "    X : pd.DataFrame\n",
    "        Features to predict on\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    predictions : np.ndarray\n",
    "        Predicted class labels\n",
    "    probabilities : np.ndarray\n",
    "        Predicted probabilities (if available)\n",
    "    \"\"\"\n",
    "    predictions = pipeline.predict(X)\n",
    "    \n",
    "    # Get probabilities if available\n",
    "    if hasattr(pipeline, 'predict_proba'):\n",
    "        probabilities = pipeline.predict_proba(X)\n",
    "    else:\n",
    "        probabilities = None\n",
    "    \n",
    "    return predictions, probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "Again, this cell only *defines* the `predict()` function -- no output yet.\n",
    "When called, it returns both hard labels and probability estimates. The\n",
    "`hasattr` check for `predict_proba` makes the function safe to use with\n",
    "estimators that do not natively produce probabilities (e.g., some SVMs).\n",
    "\n",
    "**Key takeaway:** Defensive checks like `hasattr` prevent runtime errors\n",
    "when you swap model types in the config.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "### 2.6 Evaluate Function\n",
    "\n",
    "The `evaluate()` function calls `predict()` internally and then computes a\n",
    "standard set of metrics: accuracy, precision, recall, F1, and ROC-AUC.\n",
    "Returning the results as a dictionary makes it straightforward to log them\n",
    "to a JSON file for reproducibility tracking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(pipeline, X, y, split_name='Test'):\n",
    "    \"\"\"\n",
    "    Evaluate a fitted pipeline.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pipeline : sklearn.pipeline.Pipeline\n",
    "        Fitted pipeline\n",
    "    X : pd.DataFrame\n",
    "        Features\n",
    "    y : pd.Series\n",
    "        True labels\n",
    "    split_name : str\n",
    "        Name of the split for reporting\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    metrics : dict\n",
    "        Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    # Get predictions\n",
    "    y_pred, y_proba = predict(pipeline, X)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'split': split_name,\n",
    "        'n_samples': len(y),\n",
    "        'accuracy': accuracy_score(y, y_pred),\n",
    "        'precision': precision_score(y, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y, y_pred, zero_division=0)\n",
    "    }\n",
    "    \n",
    "    # Add AUC if probabilities available\n",
    "    if y_proba is not None:\n",
    "        metrics['roc_auc'] = roc_auc_score(y, y_proba[:, 1])\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n=== {split_name} Metrics ===\")\n",
    "    for key, value in metrics.items():\n",
    "        if key not in ['split', 'n_samples']:\n",
    "            print(f\"{key:>12s}: {value:.4f}\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "This cell defines `evaluate()` -- it will print a table of metrics when\n",
    "called. The function returns a dictionary containing accuracy, precision,\n",
    "recall, F1, and (when available) ROC-AUC, tagged with the split name and\n",
    "sample count. Storing results in a dictionary rather than just printing\n",
    "them makes it easy to serialise the metrics to JSON later.\n",
    "\n",
    "**Key takeaway:** Always return metrics programmatically so they can be\n",
    "saved, compared, and audited -- do not rely solely on printed text.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "### 2.7 Train and Evaluate\n",
    "\n",
    "Now we run the full workflow: train the pipeline on the training set, then\n",
    "evaluate on train, validation, and test sets in sequence. Comparing\n",
    "train-set performance to validation and test performance immediately reveals\n",
    "whether the model is overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the pipeline\n",
    "pipeline = train_model(X_train, y_train, CONFIG)\n",
    "\n",
    "# Evaluate on all splits\n",
    "train_metrics = evaluate(pipeline, X_train, y_train, 'Train')\n",
    "val_metrics = evaluate(pipeline, X_val, y_val, 'Validation')\n",
    "test_metrics = evaluate(pipeline, X_test, y_test, 'Test')\n",
    "\n",
    "print(\"\\n\u2713 Model trained and evaluated on all splits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "First you see the pipeline confirmation (`Pipeline trained: 2 steps --\n",
    "StandardScaler, LogisticRegression`). Then three metric blocks appear for\n",
    "**Train**, **Validation**, and **Test**. Compare them: if train metrics are\n",
    "much higher than validation/test, the model may be overfitting. Because we\n",
    "are using a simple logistic regression with default regularisation on a\n",
    "well-behaved dataset, all three sets should show similar performance.\n",
    "\n",
    "**Why this matters:** Evaluating on all three splits in a single cell gives\n",
    "you an instant overfitting/underfitting diagnostic.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd PAUSE-AND-DO Exercise 1 (10 minutes)\n",
    "\n",
    "**Task:** Implement `train_model(config)` returning pipeline + metrics.\n",
    "\n",
    "**Instructions:**\n",
    "1. Review the `train_model()` function above\n",
    "2. Modify the CONFIG dictionary to try different settings:\n",
    "   - Change the model type to 'random_forest'\n",
    "   - Adjust hyperparameters (e.g., n_estimators, max_depth)\n",
    "   - Compare performance metrics\n",
    "3. Document your findings below\n",
    "\n",
    "**What to try:**\n",
    "- Different model types\n",
    "- Different preprocessing approaches\n",
    "- Different hyperparameter values\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "### YOUR EXPERIMENT HERE:\n",
    "\n",
    "**Configuration tested:**  \n",
    "[Describe your configuration changes]\n",
    "\n",
    "**Results:**  \n",
    "[Report performance metrics]\n",
    "\n",
    "**Key finding:**  \n",
    "[What did you learn?]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 3. Save/Load Model Artifacts (using joblib)\n",
    "\n",
    "### Why Save Artifacts?\n",
    "\n",
    "> **\"If you can't load it, you can't deploy it.\"**  \n",
    "> Model persistence is the bridge between development and production.\n",
    "\n",
    "**What to save:**\n",
    "- Fitted pipeline (includes preprocessing + model)\n",
    "- Configuration used to train\n",
    "- Training metrics and metadata\n",
    "- Feature names and types\n",
    "\n",
    "### 3.1 Save Model Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_artifacts(pipeline, config, metrics, feature_names):\n",
    "    \"\"\"\n",
    "    Save all model artifacts for reproducibility.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pipeline : sklearn.pipeline.Pipeline\n",
    "        Fitted pipeline\n",
    "    config : dict\n",
    "        Configuration dictionary\n",
    "    metrics : dict\n",
    "        Training metrics\n",
    "    feature_names : list\n",
    "        List of feature names\n",
    "    \"\"\"\n",
    "    # Save pipeline\n",
    "    joblib.dump(pipeline, config['paths']['model_artifact'])\n",
    "    print(f\"\u2713 Saved pipeline to {config['paths']['model_artifact']}\")\n",
    "    \n",
    "    # Save config\n",
    "    with open(config['paths']['config_artifact'], 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    print(f\"\u2713 Saved config to {config['paths']['config_artifact']}\")\n",
    "    \n",
    "    # Save metrics with metadata\n",
    "    artifact_metadata = {\n",
    "        'train_metrics': train_metrics,\n",
    "        'val_metrics': val_metrics,\n",
    "        'test_metrics': test_metrics,\n",
    "        'feature_names': feature_names,\n",
    "        'n_features': len(feature_names),\n",
    "        'saved_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "    \n",
    "    with open(config['paths']['metrics_artifact'], 'w') as f:\n",
    "        json.dump(artifact_metadata, f, indent=2)\n",
    "    print(f\"\u2713 Saved metrics to {config['paths']['metrics_artifact']}\")\n",
    "    \n",
    "    print(\"\\n=== Artifact Summary ===\")\n",
    "    print(f\"Pipeline size: {joblib.dump(pipeline, '/tmp/temp.joblib')} bytes\")\n",
    "    print(f\"Features: {len(feature_names)}\")\n",
    "    print(f\"Pipeline steps: {len(pipeline.steps)}\")\n",
    "\n",
    "# Save artifacts\n",
    "save_model_artifacts(\n",
    "    pipeline=pipeline,\n",
    "    config=CONFIG,\n",
    "    metrics={'train': train_metrics, 'val': val_metrics, 'test': test_metrics},\n",
    "    feature_names=list(X_train.columns)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "Three confirmation lines show that the **pipeline** (`.joblib`), the\n",
    "**config** (`.json`), and the **metrics** (`.json`) were written to disk.\n",
    "The artifact summary reports the pipeline file size and the number of\n",
    "features and steps. A small file size is expected for logistic regression;\n",
    "tree ensembles will be larger. These artifacts are everything someone needs\n",
    "to reproduce your predictions without re-running training.\n",
    "\n",
    "**Key takeaway:** `joblib` serialises the entire fitted pipeline --\n",
    "including the scaler's learned mean and variance -- so inference always\n",
    "applies the same preprocessing.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "### 3.2 Load Model Artifacts\n",
    "\n",
    "The `load_model_artifacts()` function reverses the save process: it reads\n",
    "the pipeline with `joblib.load`, then reads back the JSON config and\n",
    "metrics files. We verify reproducibility by comparing predictions from the\n",
    "loaded pipeline to those from the original in-memory pipeline -- they must\n",
    "match exactly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_artifacts(config):\n",
    "    \"\"\"\n",
    "    Load saved model artifacts.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    config : dict\n",
    "        Configuration dictionary with artifact paths\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pipeline : sklearn.pipeline.Pipeline\n",
    "        Loaded pipeline\n",
    "    config_loaded : dict\n",
    "        Loaded configuration\n",
    "    metrics_loaded : dict\n",
    "        Loaded metrics\n",
    "    \"\"\"\n",
    "    # Load pipeline\n",
    "    pipeline = joblib.load(config['paths']['model_artifact'])\n",
    "    print(f\"\u2713 Loaded pipeline from {config['paths']['model_artifact']}\")\n",
    "    \n",
    "    # Load config\n",
    "    with open(config['paths']['config_artifact'], 'r') as f:\n",
    "        config_loaded = json.load(f)\n",
    "    print(f\"\u2713 Loaded config from {config['paths']['config_artifact']}\")\n",
    "    \n",
    "    # Load metrics\n",
    "    with open(config['paths']['metrics_artifact'], 'r') as f:\n",
    "        metrics_loaded = json.load(f)\n",
    "    print(f\"\u2713 Loaded metrics from {config['paths']['metrics_artifact']}\")\n",
    "    \n",
    "    return pipeline, config_loaded, metrics_loaded\n",
    "\n",
    "# Test loading\n",
    "print(\"=== Testing Model Loading ===\")\n",
    "loaded_pipeline, loaded_config, loaded_metrics = load_model_artifacts(CONFIG)\n",
    "\n",
    "# Verify predictions match\n",
    "original_preds = pipeline.predict(X_test[:5])\n",
    "loaded_preds = loaded_pipeline.predict(X_test[:5])\n",
    "\n",
    "print(\"\\n=== Verification ===\")\n",
    "print(f\"Original predictions: {original_preds}\")\n",
    "print(f\"Loaded predictions:   {loaded_preds}\")\n",
    "print(f\"Predictions match: {np.array_equal(original_preds, loaded_preds)}\")\n",
    "print(\"\\n\u2713 Model artifacts save/load verified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "After loading, the verification section prints predictions from the\n",
    "**original** in-memory pipeline and the **loaded** pipeline side by side.\n",
    "The final line, `Predictions match: True`, is the critical check -- if it\n",
    "said `False`, something went wrong during serialisation (e.g., a\n",
    "preprocessing step was fitted outside the pipeline and therefore not saved).\n",
    "\n",
    "**Why this matters:** This is the ultimate reproducibility test. If the\n",
    "loaded model cannot reproduce the original predictions exactly, the saved\n",
    "artifact is useless for deployment.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "### 3.3 Reproducibility Checklist\n",
    "\n",
    "**Before deploying, verify:**\n",
    "\n",
    "- [ ] Pipeline includes all preprocessing steps\n",
    "- [ ] Random seeds are fixed and documented\n",
    "- [ ] Feature names and types are recorded\n",
    "- [ ] Model can be loaded and produces identical predictions\n",
    "- [ ] Configuration is saved separately from code\n",
    "- [ ] Training metrics are documented\n",
    "- [ ] All dependencies (package versions) are recorded\n",
    "\n",
    "**\u26a0\ufe0f Common reproducibility failures:**\n",
    "- Preprocessing done outside the pipeline (won't load correctly)\n",
    "- Missing random seeds\n",
    "- Feature engineering not included in pipeline\n",
    "- Package version mismatches between training and inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## 4. Monitoring Plan Template\n",
    "\n",
    "### Why Monitor?\n",
    "\n",
    "> **\"Models decay. The world changes. Monitoring is not optional.\"**  \n",
    "> Without monitoring, you won't know when your model stops working.\n",
    "\n",
    "### 4.1 Three Types of Drift\n",
    "\n",
    "**1. Data Drift (Covariate Shift)**\n",
    "- Feature distributions change over time\n",
    "- Example: Average transaction amount increases\n",
    "- Detection: Compare feature distributions (KS test, PSI)\n",
    "\n",
    "**2. Performance Drift (Concept Drift)**\n",
    "- Model accuracy degrades over time\n",
    "- Example: Precision drops from 0.85 to 0.70\n",
    "- Detection: Track accuracy, precision, recall on new data\n",
    "\n",
    "**3. Calibration Drift**\n",
    "- Predicted probabilities become unreliable\n",
    "- Example: Model says 80% confidence but only right 60% of the time\n",
    "- Detection: Compare predicted probabilities to observed frequencies\n",
    "\n",
    "### 4.2 Monitoring Signals Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create monitoring plan table\n",
    "monitoring_plan = pd.DataFrame([\n",
    "    {\n",
    "        'Signal': 'Prediction Volume',\n",
    "        'Type': 'System Health',\n",
    "        'Metric': 'Daily prediction count',\n",
    "        'Warning Threshold': '< 80% of baseline',\n",
    "        'Critical Threshold': '< 50% of baseline',\n",
    "        'Check Frequency': 'Daily',\n",
    "        'Owner': 'Data Engineering'\n",
    "    },\n",
    "    {\n",
    "        'Signal': 'Feature Availability',\n",
    "        'Type': 'Data Quality',\n",
    "        'Metric': '% missing values per feature',\n",
    "        'Warning Threshold': '> 5% missing',\n",
    "        'Critical Threshold': '> 20% missing',\n",
    "        'Check Frequency': 'Daily',\n",
    "        'Owner': 'Data Engineering'\n",
    "    },\n",
    "    {\n",
    "        'Signal': 'Feature Distribution',\n",
    "        'Type': 'Data Drift',\n",
    "        'Metric': 'Population Stability Index (PSI)',\n",
    "        'Warning Threshold': 'PSI > 0.1',\n",
    "        'Critical Threshold': 'PSI > 0.25',\n",
    "        'Check Frequency': 'Weekly',\n",
    "        'Owner': 'ML Engineering'\n",
    "    },\n",
    "    {\n",
    "        'Signal': 'Prediction Distribution',\n",
    "        'Type': 'Data Drift',\n",
    "        'Metric': 'Predicted class proportions',\n",
    "        'Warning Threshold': '> 10% shift',\n",
    "        'Critical Threshold': '> 25% shift',\n",
    "        'Check Frequency': 'Weekly',\n",
    "        'Owner': 'ML Engineering'\n",
    "    },\n",
    "    {\n",
    "        'Signal': 'Model Accuracy',\n",
    "        'Type': 'Performance Drift',\n",
    "        'Metric': 'Accuracy on labeled subset',\n",
    "        'Warning Threshold': '< 90% of baseline',\n",
    "        'Critical Threshold': '< 80% of baseline',\n",
    "        'Check Frequency': 'Weekly',\n",
    "        'Owner': 'ML Engineering'\n",
    "    },\n",
    "    {\n",
    "        'Signal': 'Precision/Recall',\n",
    "        'Type': 'Performance Drift',\n",
    "        'Metric': 'Precision and recall on labeled subset',\n",
    "        'Warning Threshold': '> 5% drop',\n",
    "        'Critical Threshold': '> 15% drop',\n",
    "        'Check Frequency': 'Weekly',\n",
    "        'Owner': 'ML Engineering'\n",
    "    },\n",
    "    {\n",
    "        'Signal': 'Calibration',\n",
    "        'Type': 'Calibration Drift',\n",
    "        'Metric': 'Brier score / calibration error',\n",
    "        'Warning Threshold': '> 20% degradation',\n",
    "        'Critical Threshold': '> 50% degradation',\n",
    "        'Check Frequency': 'Bi-weekly',\n",
    "        'Owner': 'ML Engineering'\n",
    "    },\n",
    "    {\n",
    "        'Signal': 'Business Metric',\n",
    "        'Type': 'Business Impact',\n",
    "        'Metric': 'Conversion rate / ROI',\n",
    "        'Warning Threshold': 'Per business rules',\n",
    "        'Critical Threshold': 'Per business rules',\n",
    "        'Check Frequency': 'Weekly',\n",
    "        'Owner': 'Business Team'\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"=== Monitoring Plan ===\")\n",
    "print(monitoring_plan.to_string(index=False))\n",
    "\n",
    "# Save monitoring plan\n",
    "monitoring_plan.to_csv('monitoring_plan.csv', index=False)\n",
    "print(\"\\n\u2713 Monitoring plan saved to monitoring_plan.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The monitoring plan table lists **eight signals** across four categories:\n",
    "System Health, Data Quality, Data Drift, Performance Drift, Calibration\n",
    "Drift, and Business Impact. Each row specifies the metric, warning and\n",
    "critical thresholds, check frequency, and responsible owner. Notice the\n",
    "escalation pattern: most signals start with a **weekly** check but\n",
    "prediction volume is watched **daily** because a sudden drop may signal a\n",
    "pipeline failure.\n",
    "\n",
    "**Key takeaway:** A monitoring plan is only as good as its thresholds. The\n",
    "numbers here (PSI > 0.1, accuracy < 90 % of baseline) are industry\n",
    "conventions -- tailor them to your project's tolerance for degradation.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "### 4.3 Monitoring Implementation Checklist\n",
    "\n",
    "**Setup:**\n",
    "- [ ] Define baseline distributions (training data)\n",
    "- [ ] Set up logging infrastructure for predictions\n",
    "- [ ] Create dashboards for key metrics\n",
    "- [ ] Define alert thresholds and escalation paths\n",
    "\n",
    "**Ongoing:**\n",
    "- [ ] Collect labeled data for ground truth\n",
    "- [ ] Run scheduled monitoring jobs\n",
    "- [ ] Review alerts and investigate anomalies\n",
    "- [ ] Retrain model when drift detected\n",
    "\n",
    "**Documentation:**\n",
    "- [ ] Document baseline metrics\n",
    "- [ ] Record all retraining events\n",
    "- [ ] Maintain incident log\n",
    "- [ ] Update monitoring plan as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd PAUSE-AND-DO Exercise 2 (10 minutes)\n",
    "\n",
    "**Task:** Draft a monitoring plan with 5-8 signals and owners.\n",
    "\n",
    "**Instructions:**\n",
    "1. Review the monitoring plan table above\n",
    "2. Customize it for your project:\n",
    "   - What features are most important to monitor?\n",
    "   - What business metrics matter most?\n",
    "   - What are realistic thresholds for your use case?\n",
    "3. Add at least 2 project-specific signals\n",
    "4. Document your plan below\n",
    "\n",
    "**What to include:**\n",
    "- Signal name and type\n",
    "- Specific metric to track\n",
    "- Warning and critical thresholds\n",
    "- Check frequency\n",
    "- Responsible owner\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "### YOUR MONITORING PLAN HERE:\n",
    "\n",
    "**Project-specific signals:**\n",
    "\n",
    "1. **[Signal Name]**  \n",
    "   - Type: [Data Drift / Performance / Calibration / Business]\n",
    "   - Metric: [What to measure]\n",
    "   - Thresholds: [Warning / Critical]\n",
    "   - Frequency: [How often]\n",
    "   - Owner: [Who is responsible]\n",
    "\n",
    "2. **[Signal Name]**  \n",
    "   - Type:\n",
    "   - Metric:\n",
    "   - Thresholds:\n",
    "   - Frequency:\n",
    "   - Owner:\n",
    "\n",
    "**Rationale:**  \n",
    "[Why did you choose these signals?]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## 5. Ready-to-Share Notebook Hygiene Checklist\n",
    "\n",
    "### Before Sharing Your Notebook\n",
    "\n",
    "> **\"Your notebook is your reputation.\"**  \n",
    "> A well-organized notebook shows rigor and professionalism.\n",
    "\n",
    "### 5.1 Technical Hygiene"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "**Run-All Test:**\n",
    "- [ ] Restart kernel and \"Run All\" completes without errors\n",
    "- [ ] No deprecated warnings (or they're documented)\n",
    "- [ ] Outputs are visible and formatted properly\n",
    "- [ ] Random seeds produce consistent results\n",
    "\n",
    "**Code Quality:**\n",
    "- [ ] Imports organized at top (standard, third-party, custom)\n",
    "- [ ] No unused imports or variables\n",
    "- [ ] Functions have clear docstrings\n",
    "- [ ] Variable names are descriptive\n",
    "- [ ] Magic numbers replaced with named constants\n",
    "\n",
    "**Data Quality:**\n",
    "- [ ] Data source is documented\n",
    "- [ ] Missing value handling is explicit\n",
    "- [ ] Train/val/test splits are clearly labeled\n",
    "- [ ] No data leakage between splits\n",
    "- [ ] Feature engineering is reproducible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "### 5.2 Communication Hygiene"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "**Structure:**\n",
    "- [ ] Clear title and introduction\n",
    "- [ ] Learning objectives stated upfront\n",
    "- [ ] Logical section flow\n",
    "- [ ] Summary/conclusion at end\n",
    "\n",
    "**Documentation:**\n",
    "- [ ] Markdown cells explain the \"why\" before code\n",
    "- [ ] Key findings are highlighted\n",
    "- [ ] Visualizations have titles and labels\n",
    "- [ ] Tables are formatted and readable\n",
    "- [ ] Assumptions are stated explicitly\n",
    "\n",
    "**Professionalism:**\n",
    "- [ ] No debug cells or commented-out code\n",
    "- [ ] No placeholder text (\"TODO\", \"FIXME\")\n",
    "- [ ] Consistent formatting throughout\n",
    "- [ ] Bibliography and citations included\n",
    "- [ ] Author and date documented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "### 5.3 Reproducibility Hygiene"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "**Environment:**\n",
    "- [ ] Package versions documented (requirements.txt or in notebook)\n",
    "- [ ] Random seeds set and documented\n",
    "- [ ] Data sources with URLs/paths\n",
    "- [ ] Instructions for obtaining data\n",
    "\n",
    "**Artifacts:**\n",
    "- [ ] Model saved and loadable\n",
    "- [ ] Configuration saved separately\n",
    "- [ ] Feature names preserved\n",
    "- [ ] Preprocessing steps documented\n",
    "\n",
    "**Validation:**\n",
    "- [ ] Test set untouched until final evaluation\n",
    "- [ ] Cross-validation procedure documented\n",
    "- [ ] Baseline model included for comparison\n",
    "- [ ] Performance metrics clearly reported"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "### 5.4 Production Readiness Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production readiness assessment\nproduction_checklist = pd.DataFrame([\n    {'Category': 'Reproducibility', 'Item': 'Pipeline save/load tested', 'Status': '\u2713', 'Notes': 'Verified with test data'},\n    {'Category': 'Reproducibility', 'Item': 'Random seeds documented', 'Status': '\u2713', 'Notes': 'RANDOM_SEED = 474'},\n    {'Category': 'Reproducibility', 'Item': 'Configuration externalized', 'Status': '\u2713', 'Notes': 'CONFIG dictionary'},\n    {'Category': 'Testing', 'Item': 'Input validation', 'Status': '\u25cb', 'Notes': 'Need to add schema validation'},\n    {'Category': 'Testing', 'Item': 'Error handling', 'Status': '\u25cb', 'Notes': 'Need try/except blocks'},\n    {'Category': 'Testing', 'Item': 'Unit tests', 'Status': '\u25cb', 'Notes': 'Need test suite'},\n    {'Category': 'Monitoring', 'Item': 'Monitoring plan defined', 'Status': '\u2713', 'Notes': '8 signals identified'},\n    {'Category': 'Monitoring', 'Item': 'Logging infrastructure', 'Status': '\u25cb', 'Notes': 'Need to implement'},\n    {'Category': 'Documentation', 'Item': 'Model card', 'Status': '\u25cb', 'Notes': 'Draft in progress'},\n    {'Category': 'Documentation', 'Item': 'API documentation', 'Status': '\u25cb', 'Notes': 'Need to create'},\n    {'Category': 'Security', 'Item': 'No hardcoded credentials', 'Status': '\u2713', 'Notes': 'N/A for this example'},\n    {'Category': 'Security', 'Item': 'Input sanitization', 'Status': '\u25cb', 'Notes': 'Need to add'},\n])\n\nprint(\"=== Production Readiness Assessment ===\")\nprint(production_checklist.to_string(index=False))\n\n# Summary\nready_count = (production_checklist['Status'] == '\u2713').sum()\ntotal_count = len(production_checklist)\nprint(f\"\\nReadiness: {ready_count}/{total_count} items complete ({ready_count/total_count*100:.0f}%)\")\n\n# Save checklist\nproduction_checklist.to_csv('production_readiness.csv', index=False)\nprint(\"\u2713 Checklist saved to production_readiness.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The production-readiness table shows **12 items** grouped into\n",
    "Reproducibility, Testing, Monitoring, Documentation, and Security. Items\n",
    "marked `\u2713` are already complete; items marked `\u25cb` still need work. The\n",
    "summary line prints the overall readiness percentage. For a course project,\n",
    "achieving 100 % is not expected -- the exercise is to *identify* the gaps,\n",
    "not necessarily close all of them.\n",
    "\n",
    "**Why this matters:** In industry, a checklist like this gates the\n",
    "transition from 'model works in a notebook' to 'model runs in production'.\n",
    "Knowing what remains undone is itself a sign of professional maturity.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-37",
   "metadata": {},
   "source": [
    "### 5.5 Final Pre-Submission Checklist\n",
    "\n",
    "This checklist consolidates every quality gate into one place. Running this\n",
    "cell before submitting your project gives you a printable list you can tick\n",
    "off: technical correctness, reproducibility, communication quality, and\n",
    "production readiness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell before submitting\n",
    "print(\"=== PRE-SUBMISSION CHECKLIST ===\")\n",
    "print(\"\\n1. TECHNICAL\")\n",
    "print(\"   [ ] Kernel restarted and Run All completed\")\n",
    "print(\"   [ ] No errors or warnings\")\n",
    "print(\"   [ ] All outputs visible\")\n",
    "print(\"\\n2. REPRODUCIBILITY\")\n",
    "print(\"   [ ] Seeds fixed and documented\")\n",
    "print(\"   [ ] Model artifacts saved\")\n",
    "print(\"   [ ] Configuration externalized\")\n",
    "print(\"\\n3. COMMUNICATION\")\n",
    "print(\"   [ ] Clear narrative structure\")\n",
    "print(\"   [ ] Key findings highlighted\")\n",
    "print(\"   [ ] Visualizations labeled\")\n",
    "print(\"\\n4. PROFESSIONALISM\")\n",
    "print(\"   [ ] No debug code or TODOs\")\n",
    "print(\"   [ ] Consistent formatting\")\n",
    "print(\"   [ ] Bibliography included\")\n",
    "print(\"\\n5. PRODUCTION READINESS\")\n",
    "print(\"   [ ] Monitoring plan defined\")\n",
    "print(\"   [ ] Risk assessment completed\")\n",
    "print(\"   [ ] Deployment checklist reviewed\")\n",
    "print(\"\\n\u26a0\ufe0f Review each item before submitting your project!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the output:**\n",
    "\n",
    "The printed checklist organises final review into five categories:\n",
    "**Technical** (kernel, errors, outputs), **Reproducibility** (seeds,\n",
    "artifacts, config), **Communication** (narrative, findings, visuals),\n",
    "**Professionalism** (no debug code, formatting, bibliography), and\n",
    "**Production Readiness** (monitoring, risk, deployment). Running this cell\n",
    "right before submission gives you a compact, scannable to-do list.\n",
    "\n",
    "**Key takeaway:** Treat this checklist as a pre-flight ritual: it only\n",
    "takes two minutes but catches the most common submission mistakes.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-39",
   "metadata": {},
   "source": [
    "## 6. Wrap-Up: Key Takeaways\n\n### What We Learned Today:\n\n1. **Refactoring for Production**: Separate configuration from code, wrap logic in functions\n2. **Model Persistence**: Save/load pipelines with joblib, verify reproducibility\n3. **Monitoring Strategy**: Track data drift, performance drift, and calibration drift\n4. **Production Checklist**: Comprehensive review before deployment\n5. **Notebook Hygiene**: Make your work shareable and reproducible\n\n### Next-Day Readiness:\n\n- \u2713 You can package a model pipeline for deployment\n- \u2713 You can save and load model artifacts\n- \u2713 You can define a monitoring plan\n- \u2713 You understand production readiness requirements\n- \u2713 You're ready for the next notebook: Executive Narrative\n\n### Remember:\n\n> **\"Deployment is not the end. It's the beginning of model maintenance.\"**  \n> Without monitoring and maintenance, even the best models decay.\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-40",
   "metadata": {},
   "source": [
    "## 7. Submission Instructions\n\n### To Submit This Notebook:\n\n1. **Run All Cells**: Execute `Runtime \u2192 Run all` to ensure everything works\n2. **Save a Copy**: `File \u2192 Save a copy in Drive`\n3. **Get Shareable Link**: Click `Share` and set to \"Anyone with the link can view\"\n4. **Submit Link**: Paste the link in the LMS assignment\n\n### Before Submitting, Check:\n\n- [ ] All cells execute without errors\n- [ ] All outputs are visible\n- [ ] Exercise responses are complete\n- [ ] Monitoring plan is documented\n- [ ] Notebook is shared with correct permissions\n\n---\n\n## Bibliography\n\n- Huyen, C. (2022). *Designing Machine Learning Systems*. O'Reilly Media.\n- Lakshmanan, V., Robinson, S., & Munn, M. (2020). *Machine Learning Design Patterns*. O'Reilly Media.\n- Quionero-Candela, J., Sugiyama, M., Schwaighofer, A., & Lawrence, N. D. (2009). *Dataset Shift in Machine Learning*. MIT Press.\n- Rabanser, S., G\u00fcnnemann, S., & Lipton, Z. (2019). Failing Loudly: An Empirical Study of Methods for Detecting Dataset Shift. *NeurIPS 2019*.\n- scikit-learn User Guide: [Model persistence](https://scikit-learn.org/stable/model_persistence.html)\n- scikit-learn User Guide: [Pipelines and composite estimators](https://scikit-learn.org/stable/modules/compose.html)\n\n---\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "Thank you!\n",
    "\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}