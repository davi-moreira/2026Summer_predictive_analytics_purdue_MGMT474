{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 13: Gradient Boosting - Performance with Discipline (and Leakage Avoidance)\n",
    "\n",
    "**MGMT 47400 - Predictive Analytics**  \n",
    "**4-Week Online Course**  \n",
    "**Day 13 - Thursday June 3, 2027**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/blob/main/notebooks/13_gradient_boosting.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Explain boosting vs bagging at a high level\n",
    "2. Train a gradient boosting model with sensible defaults\n",
    "3. Tune learning rate / depth / estimators with runtime controls\n",
    "4. Compare boosted model vs forest under consistent CV\n",
    "5. Identify and control overfitting in boosting\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.stats import randint, uniform\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.precision', 4)\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "print(\"âœ“ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Boosting vs Bagging\n",
    "\n",
    "### Bagging (Random Forest)\n",
    "- **Strategy**: Train many trees independently on bootstrap samples\n",
    "- **Combine**: Average (or vote) predictions\n",
    "- **Effect**: Reduces variance\n",
    "- **Trees**: Can be deep (low bias, high variance)\n",
    "- **Parallelizable**: Yes\n",
    "\n",
    "### Boosting (Gradient Boosting)\n",
    "- **Strategy**: Train trees sequentially, each correcting previous errors\n",
    "- **Combine**: Weighted sum of predictions\n",
    "- **Effect**: Reduces bias (and some variance)\n",
    "- **Trees**: Should be shallow (reduce variance of individual trees)\n",
    "- **Parallelizable**: Not really (sequential by nature)\n",
    "\n",
    "### Gradient Boosting Algorithm (Simplified)\n",
    "\n",
    "```\n",
    "1. Start with initial prediction (e.g., mean)\n",
    "2. For iteration m = 1 to M:\n",
    "   a. Calculate residuals (errors) from current model\n",
    "   b. Fit shallow tree to predict residuals\n",
    "   c. Add tree to model with learning rate Î·:\n",
    "      F_m(x) = F_(m-1)(x) + Î· * tree_m(x)\n",
    "3. Final model: F_M(x) = sum of all trees\n",
    "```\n",
    "\n",
    "**Key insight**: Each tree tries to fix mistakes of previous trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = load_breast_cancer(as_frame=True)\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RANDOM_SEED, stratify=y)\n",
    "\n",
    "print(f\"Train: {len(X_train)} | Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple GBM with defaults\n",
    "gbm = GradientBoostingClassifier(random_state=RANDOM_SEED)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "gbm_scores = cross_val_score(gbm, X_train, y_train, cv=cv, scoring='roc_auc')\n",
    "\n",
    "print(\"=== GRADIENT BOOSTING (defaults) ===\")\n",
    "print(f\"Default params:\")\n",
    "print(f\"  n_estimators: 100\")\n",
    "print(f\"  learning_rate: 0.1\")\n",
    "print(f\"  max_depth: 3\")\n",
    "print(f\"\\nCV ROC-AUC: {gbm_scores.mean():.4f} Â± {gbm_scores.std():.4f}\")\n",
    "\n",
    "# Compare to Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=RANDOM_SEED, n_jobs=-1)\n",
    "rf_scores = cross_val_score(rf, X_train, y_train, cv=cv, scoring='roc_auc')\n",
    "\n",
    "print(f\"\\n=== RANDOM FOREST (100 trees) ===\")\n",
    "print(f\"CV ROC-AUC: {rf_scores.mean():.4f} Â± {rf_scores.std():.4f}\")\n",
    "\n",
    "print(f\"\\n=== COMPARISON ===\")\n",
    "print(f\"GBM advantage: {(gbm_scores.mean() - rf_scores.mean()):.4f}\")\n",
    "print(\"\\nðŸ’¡ GBM often outperforms RF (but needs more tuning)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Learning Rate Tradeoff\n",
    "\n",
    "### Learning Rate (Î·, or `learning_rate`)\n",
    "\n",
    "**High learning rate (e.g., 0.3)**\n",
    "- âœ“ Faster training (need fewer trees)\n",
    "- âœ— Risk of overfitting\n",
    "- âœ— May miss optimal solution\n",
    "\n",
    "**Low learning rate (e.g., 0.01)**\n",
    "- âœ“ Better final performance (usually)\n",
    "- âœ“ More stable\n",
    "- âœ— Needs many more trees\n",
    "- âœ— Slower training\n",
    "\n",
    "**Rule of thumb**: Lower learning rate + more trees = better results (but longer training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate sweep\n",
    "learning_rates = [0.01, 0.05, 0.1, 0.2, 0.5]\n",
    "lr_results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    gbm_lr = GradientBoostingClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=lr,\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    scores = cross_val_score(gbm_lr, X_train, y_train, cv=cv, scoring='roc_auc')\n",
    "    lr_results.append({\n",
    "        'learning_rate': lr,\n",
    "        'cv_mean': scores.mean(),\n",
    "        'cv_std': scores.std()\n",
    "    })\n",
    "\n",
    "lr_df = pd.DataFrame(lr_results)\n",
    "print(\"=== LEARNING RATE SWEEP (100 trees) ===\")\n",
    "print(lr_df.to_string(index=False))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(lr_df['learning_rate'], lr_df['cv_mean'], \n",
    "             yerr=lr_df['cv_std'], marker='o', capsize=5, linewidth=2)\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('CV ROC-AUC')\n",
    "plt.title('Effect of Learning Rate (100 trees)')\n",
    "plt.xscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Lower learning rate often performs better\")\n",
    "print(\"ðŸ’¡ But needs to compensate with more trees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Number of Trees vs Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: low LR with more trees\n",
    "configs = [\n",
    "    {'n_estimators': 50, 'learning_rate': 0.1},\n",
    "    {'n_estimators': 100, 'learning_rate': 0.1},\n",
    "    {'n_estimators': 200, 'learning_rate': 0.05},\n",
    "    {'n_estimators': 500, 'learning_rate': 0.01},\n",
    "]\n",
    "\n",
    "config_results = []\n",
    "for config in configs:\n",
    "    gbm = GradientBoostingClassifier(random_state=RANDOM_SEED, **config)\n",
    "    scores = cross_val_score(gbm, X_train, y_train, cv=cv, scoring='roc_auc')\n",
    "    config_results.append({\n",
    "        'n_estimators': config['n_estimators'],\n",
    "        'learning_rate': config['learning_rate'],\n",
    "        'cv_mean': scores.mean(),\n",
    "        'cv_std': scores.std()\n",
    "    })\n",
    "\n",
    "config_df = pd.DataFrame(config_results)\n",
    "print(\"=== LEARNING RATE + N_ESTIMATORS COMBINATIONS ===\")\n",
    "print(config_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nðŸ’¡ More trees with lower learning rate often wins\")\n",
    "print(\"ðŸ’¡ But training time increases significantly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ PAUSE-AND-DO Exercise 1 (10 minutes)\n",
    "\n",
    "**Task:** Train baseline GBM and compare against RF under CV.\n",
    "\n",
    "Already done above! Now analyze:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR ANALYSIS:\n",
    "\n",
    "**Performance Comparison:**  \n",
    "[Which performs better: GBM or RF? By how much?]\n",
    "\n",
    "**Training Time:**  \n",
    "[Estimate: which is faster to train?]\n",
    "\n",
    "**When to use each:**  \n",
    "[When would you choose GBM over RF?]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tuning Gradient Boosting\n",
    "\n",
    "### Key Hyperparameters\n",
    "\n",
    "1. **n_estimators**: Number of boosting stages\n",
    "2. **learning_rate**: Shrinkage parameter (Î·)\n",
    "3. **max_depth**: Maximum tree depth (keep low: 3-5)\n",
    "4. **min_samples_split**: Minimum samples to split\n",
    "5. **subsample**: Fraction of samples for fitting each tree (stochastic GB)\n",
    "\n",
    "### Tuning Strategy\n",
    "\n",
    "**Use RandomizedSearchCV with constraints:**\n",
    "- Limit search space to reasonable values\n",
    "- Use early stopping to save time\n",
    "- Monitor overfitting carefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constrained randomized search\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(100, 300),\n",
    "    'learning_rate': uniform(0.01, 0.19),  # 0.01 to 0.2\n",
    "    'max_depth': randint(3, 7),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'subsample': uniform(0.7, 0.3)  # 0.7 to 1.0\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    GradientBoostingClassifier(random_state=RANDOM_SEED),\n",
    "    param_distributions,\n",
    "    n_iter=20,  # Try 20 combinations\n",
    "    cv=cv,\n",
    "    scoring='roc_auc',\n",
    "    random_state=RANDOM_SEED,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"=== RANDOMIZED SEARCH (20 iterations) ===\")\n",
    "print(\"Running...\")\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters: {random_search.best_params_}\")\n",
    "print(f\"Best CV score: {random_search.best_score_:.4f}\")\n",
    "\n",
    "# Test performance\n",
    "test_score = roc_auc_score(y_test, random_search.predict_proba(X_test)[:, 1])\n",
    "print(f\"Test score: {test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ PAUSE-AND-DO Exercise 2 (10 minutes)\n",
    "\n",
    "**Task:** Run constrained tuning and report best params + score.\n",
    "\n",
    "Already done! Now document your tuning strategy:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR TUNING STRATEGY:\n",
    "\n",
    "**Best Parameters Found:**  \n",
    "[List them]\n",
    "\n",
    "**Performance Improvement:**  \n",
    "[Compare to baseline GBM]\n",
    "\n",
    "**What would you try next:**  \n",
    "[If you had more time/compute, what would you tune?]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Overfitting in Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track train vs validation performance over iterations\n",
    "from sklearn.model_selection import train_test_split as split2\n",
    "\n",
    "X_t, X_v, y_t, y_v = split2(X_train, y_train, test_size=0.2, random_state=RANDOM_SEED, stratify=y_train)\n",
    "\n",
    "gbm_monitor = GradientBoostingClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=3,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "gbm_monitor.fit(X_t, y_t)\n",
    "\n",
    "# Compute staged predictions\n",
    "train_scores = []\n",
    "val_scores = []\n",
    "\n",
    "for i, (train_pred, val_pred) in enumerate(zip(\n",
    "    gbm_monitor.staged_predict_proba(X_t),\n",
    "    gbm_monitor.staged_predict_proba(X_v)\n",
    ")):\n",
    "    train_scores.append(roc_auc_score(y_t, train_pred[:, 1]))\n",
    "    val_scores.append(roc_auc_score(y_v, val_pred[:, 1]))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_scores)+1), train_scores, label='Train', linewidth=2)\n",
    "plt.plot(range(1, len(val_scores)+1), val_scores, label='Validation', linewidth=2)\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('ROC-AUC')\n",
    "plt.title('Training Progress: Monitoring Overfitting')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal number of trees\n",
    "optimal_n = np.argmax(val_scores) + 1\n",
    "print(f\"\\n=== OPTIMAL NUMBER OF TREES ===\")\n",
    "print(f\"Optimal n_estimators: {optimal_n}\")\n",
    "print(f\"Validation ROC-AUC: {val_scores[optimal_n-1]:.4f}\")\n",
    "print(f\"\\nðŸ’¡ Validation performance peaks, then plateaus (slight overfit after ~{optimal_n} trees)\")\n",
    "print(f\"ðŸ’¡ Could use early stopping in production\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Comparison: All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comparison\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "final_models = {\n",
    "    'Logistic Regression': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', LogisticRegression(random_state=RANDOM_SEED, max_iter=1000))\n",
    "    ]),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=RANDOM_SEED),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=200, random_state=RANDOM_SEED, n_jobs=-1),\n",
    "    'Gradient Boosting (default)': GradientBoostingClassifier(random_state=RANDOM_SEED),\n",
    "    'Gradient Boosting (tuned)': random_search.best_estimator_\n",
    "}\n",
    "\n",
    "final_results = []\n",
    "for name, model in final_models.items():\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='roc_auc')\n",
    "    \n",
    "    # Test score\n",
    "    if not hasattr(model, 'predict_proba'):\n",
    "        model.fit(X_train, y_train)\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        if name not in ['Gradient Boosting (tuned)']:  # Already fitted\n",
    "            model.fit(X_train, y_train)\n",
    "        test_pred = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        test_pred = model.predict(X_test)\n",
    "    \n",
    "    test_sc = roc_auc_score(y_test, test_pred)\n",
    "    \n",
    "    final_results.append({\n",
    "        'Model': name,\n",
    "        'CV_Mean': scores.mean(),\n",
    "        'CV_Std': scores.std(),\n",
    "        'Test_Score': test_sc\n",
    "    })\n",
    "\n",
    "final_df = pd.DataFrame(final_results).sort_values('CV_Mean', ascending=False)\n",
    "print(\"=== FINAL MODEL COMPARISON ===\")\n",
    "print(final_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nâœ“ Champion: {final_df.iloc[0]['Model']} (CV: {final_df.iloc[0]['CV_Mean']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Wrap-Up: Key Takeaways\n",
    "\n",
    "### What We Learned Today:\n",
    "\n",
    "1. **Boosting vs Bagging**: Sequential vs parallel, bias vs variance reduction\n",
    "2. **Gradient Boosting**: Each tree corrects previous errors\n",
    "3. **Learning Rate**: Lower rates + more trees = better performance\n",
    "4. **Tuning Strategy**: Use constrained search, monitor overfitting\n",
    "5. **Performance**: GBM often beats RF, but needs more careful tuning\n",
    "\n",
    "### Critical Rules:\n",
    "\n",
    "> **\"Lower learning rate + more trees = better (but slower)\"**\n",
    "\n",
    "> **\"Keep trees shallow (depth 3-5) for boosting\"**\n",
    "\n",
    "> **\"Monitor overfitting closely with boosting\"**\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Day 14: Model selection and comparison protocol\n",
    "- We'll formalize how to choose between models\n",
    "- Create reproducible experiment logs\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "- Friedman, J. H. (2001). \"Greedy Function Approximation: A Gradient Boosting Machine.\" *Annals of Statistics*.\n",
    "- James, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). *An Introduction to Statistical Learning with Python* - Tree-Based Methods (boosting overview)\n",
    "- Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning* - Boosting methods\n",
    "- scikit-learn User Guide: [Gradient boosting](https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting)\n",
    "\n",
    "---\n",
    "\n",
    "**End of Day 13 Notebook**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
