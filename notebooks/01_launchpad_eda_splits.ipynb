{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Day 1: Launchpad - Welcome, Predictive Analytics Fundamentals, and Data Workflow\n\n<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/images/mgmt_474_ai_logo_02-modified.png\" width=\"150\" align=\"right\" style=\"margin-left: 20px; margin-bottom: 10px;\">\n\n**MGMT 47400 - Predictive Analytics**  \n**4-Week Online Course**  \n**Day 1**\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/blob/main/notebooks/01_launchpad_eda_splits.ipynb)\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Welcome and Introductions\n",
    "\n",
    "### 1.1 Your Instructor: Professor Davi Moreira\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/images/davi_moreira_photo.JPG\" width=\"200\" align=\"left\" style=\"margin-right: 20px; margin-bottom: 10px;\">\n",
    "\n",
    "**Professor Davi Moreira** is a Clinical Assistant Professor at Purdue University's Daniels School of Business. His research focuses on computational social science, machine learning applications in business, and data-driven decision making. He teaches predictive analytics, machine learning, and data science courses at both undergraduate and graduate levels.\n",
    "\n",
    "**Why this course matters:** In today's data-driven business environment, the ability to build, evaluate, and deploy predictive models is essential. Whether you're working in marketing, finance, operations, or strategy, predictive analytics helps you make better decisions, optimize processes, and create competitive advantages.\n",
    "\n",
    "**What makes this course different:**\n",
    "- **Hands-on from Day 1:** Every concept is immediately applied in Google Colab notebooks\n",
    "- **Real business cases:** We use industry-relevant datasets and scenarios\n",
    "- **AI-assisted learning:** You'll learn to use Google Gemini responsibly as a coding partner\n",
    "- **End-to-end workflow:** From raw data to deployed model to business recommendation\n",
    "\n",
    "<br clear=\"all\">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Your Turn: Student Introductions\n",
    "\n",
    "**PAUSE-AND-THINK (3 minutes):**\n",
    "\n",
    "In the markdown cell below, introduce yourself by answering these questions:\n",
    "\n",
    "1. **Name and background:** Where are you from? What program are you in?\n",
    "2. **Data experience:** Have you worked with Python before? Any machine learning exposure?\n",
    "3. **Goals:** What do you hope to learn or build by the end of this course?\n",
    "4. **Predictive analytics in your field:** Can you think of one way predictive analytics could be applied in your area of interest? (e.g., predicting customer churn in retail, forecasting stock prices in finance, optimizing supply chains in operations)\n",
    "\n",
    "**Why this matters:** Understanding your classmates' backgrounds helps build a collaborative learning environment. You'll find study partners, get different perspectives on business problems, and see how predictive analytics applies across industries.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### YOUR INTRODUCTION (edit this cell):\n\n**Name and background:**  \n[Your answer here]\n\n**Data experience:**  \n[Your answer here]\n\n**Goals:**  \n[Your answer here]\n\n**Predictive analytics application:**  \n[Your answer here - For example: \"I work in real estate and want to use predictive models to estimate house prices based on property characteristics and location features.\"]\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Course Syllabus and Logistics\n",
    "\n",
    "**Note:** Detailed syllabus, grading policies, and schedule are available on the course website:\n",
    "\n",
    "üìò **Course Website:** [https://davi-moreira.github.io/2026Summer_predictive_analytics_purdue_MGMT474/](https://davi-moreira.github.io/2026Summer_predictive_analytics_purdue_MGMT474/)\n",
    "\n",
    "**Key points covered in the welcome video:**\n",
    "- **Format:** 4-week intensive, 20 business days, 112.5 min/day commitment\n",
    "- **Structure:** Micro-videos (‚â§12 min) + Google Colab notebooks + exercises + daily quiz\n",
    "- **Grading:** Daily quizzes (20%), Midterm (15%), Project milestones (55%), Peer review (10%)\n",
    "- **Project:** Single capstone with 4 milestones (Proposal Day 5, Baseline Day 10, Improved Day 15, Final Day 20)\n",
    "- **Technology:** Google Colab (primary), Google Gemini (AI assistant), Brightspace (LMS)\n",
    "- **Textbooks:** ISLP (Introduction to Statistical Learning with Python) + supplementary readings\n",
    "\n",
    "**Daily workflow:**\n",
    "1. Watch micro-videos (~54 min total)\n",
    "2. Work through notebook with PAUSE-AND-DO exercises (~40 min)\n",
    "3. Complete daily auto-graded quiz (~8.5 min)\n",
    "4. Post questions in discussion forum\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Google Colab Setup and Notebook Conventions\n",
    "\n",
    "### 3.1 Why Google Colab?\n",
    "\n",
    "**Google Colab** (Colaboratory) is a free, cloud-based Jupyter notebook environment that requires no local setup. It's perfect for this course because:\n",
    "\n",
    "- ‚úì **Zero installation:** No Python setup, no package management, no environment conflicts\n",
    "- ‚úì **Free GPU access:** Useful for deep learning later in the course\n",
    "- ‚úì **Cloud storage:** Work from anywhere, automatic saves to Google Drive\n",
    "- ‚úì **Built-in AI:** Google Gemini integration for coding assistance\n",
    "- ‚úì **Reproducible:** Share notebooks via links, ensure everyone has the same environment\n",
    "\n",
    "**How to open this notebook in Colab:**\n",
    "1. Click the \"Open in Colab\" badge at the top of this notebook\n",
    "2. Or visit: [https://colab.research.google.com/](https://colab.research.google.com/)\n",
    "3. Select \"GitHub\" tab and paste the course repo URL\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Colab Navigation and Workflow\n",
    "\n",
    "**Notebook structure:**\n",
    "- **Markdown cells:** Text explanations (like this one). Double-click to edit, Shift+Enter to render.\n",
    "- **Code cells:** Python code that you can run. Click the play button or Shift+Enter to execute.\n",
    "\n",
    "**Essential keyboard shortcuts:**\n",
    "- **Shift + Enter:** Run cell and move to next\n",
    "- **Ctrl + Enter (Cmd + Enter on Mac):** Run cell and stay\n",
    "- **Ctrl + M B (Cmd + M B):** Insert cell below\n",
    "- **Ctrl + M A (Cmd + M A):** Insert cell above\n",
    "- **Ctrl + M D (Cmd + M D):** Delete cell\n",
    "\n",
    "**Menu options:**\n",
    "- **Runtime ‚Üí Run all:** Execute all cells in order (use this to test reproducibility)\n",
    "- **Runtime ‚Üí Restart runtime:** Clear all variables and outputs, start fresh\n",
    "- **File ‚Üí Save a copy in Drive:** Create your own editable version\n",
    "\n",
    "**‚ö†Ô∏è Important:** Colab notebooks are **temporary** unless saved to your Drive. Always use \"File ‚Üí Save a copy in Drive\" to preserve your work.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.3 Course Notebook Conventions\n\nThroughout this course, notebooks follow consistent patterns:\n\n**1. Setup Cell (always first code cell):**\nEvery notebook starts with imports, random seed, and display settings.\n\n**2. RANDOM_SEED = 474:**\nWe use seed 474 for all random operations to ensure reproducibility. This means every time you run the notebook, you'll get the same results.\n\n**3. PAUSE-AND-DO Exercises:**\nThese are 10-minute guided practice sections where you'll apply concepts immediately. Look for this format:\n\n> ## üìù PAUSE-AND-DO Exercise N (10 minutes)\n> **Task:** [What you need to do]  \n> **Instructions:** [Step-by-step guidance]\n\n**4. Blockquotes for Critical Rules:**\nImportant warnings and best practices appear like this:\n\n> ‚ö†Ô∏è **Critical Rule:** Always split data BEFORE preprocessing to avoid leakage.\n\n**5. ‚úì Checkmarks for Confirmations:**\nWhen code completes successfully, you'll see: `‚úì Setup complete!`\n\n**6. Comments in Code:**\nAll code includes explanatory comments starting with `#` to help you understand each step.\n\n**7. Google Gemini Integration:**\nWe'll use Gemini following the **\"Ask ‚Üí Verify ‚Üí Document\"** pattern (explained in Section 3.4).\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.4 Using Google Gemini Responsibly: The \"Ask ‚Üí Verify ‚Üí Document\" Pattern\n\n**Google Gemini in Colab** is an AI coding assistant integrated directly into your notebooks. It can help you write code, debug errors, and understand concepts. However, **you are responsible for all code you submit**.\n\n**The Three-Step Pattern:**\n\n**1. ASK Gemini to draft code or explain concepts**\n\nExample prompts:\n- \"Create a histogram of the MedHouseVal variable\"\n- \"Explain what this error message means: KeyError: 'MedHouseVal'\"\n- \"Write code to calculate the correlation between MedInc and MedHouseVal\"\n- \"Generate a scatter plot showing the relationship between Latitude and house values\"\n\n**2. VERIFY the code works and you understand it**\n- Run the generated code in a cell\n- Check the output matches what you expected\n- Read through the code line by line\n- Ask yourself: \"Could I explain what each line does?\"\n\n**3. DOCUMENT with your own comments**\n- Add `#` comments explaining the logic in your own words\n- Annotate any parts that were unclear\n- Note any modifications you made\n\n**‚úì What's allowed:**\n- Using Gemini to generate boilerplate code (imports, data loading, standard plots)\n- Asking Gemini to explain error messages or debugging\n- Getting suggestions for visualizations or analyses\n- Learning new pandas/numpy/sklearn functions\n\n**‚ö†Ô∏è What's required:**\n- You must understand **every line** of code you submit\n- You must verify that generated code works correctly on your data\n- You must add your own comments and documentation\n- You must be able to explain your code and methodology\n\n**‚ùå Accountability:**\n- You are responsible for all code you submit, even if Gemini generated it\n- If you can't explain your code in the project presentation, you don't understand it well enough\n- Gemini can make mistakes - you need to catch them\n\n**Example of proper use with California Housing data:**\n\n```python\n# PROMPT TO GEMINI: \"Calculate the mean MedHouseVal for coastal vs inland areas\"\n\n# Original Gemini output (verified and commented by me):\n# Create a binary coastal indicator (Latitude > 37 and Longitude < -120)\ndf['IsCoastal'] = ((df['Latitude'] > 37) & (df['Longitude'] < -120)).astype(int)\n\n# Group by coastal status and calculate mean house value\nmean_price_by_location = df.groupby('IsCoastal')['MedHouseVal'].mean()\n\n# Add my own descriptive print statement\nprint(\"Average median house value by location:\")\nprint(f\"  Inland (0): ${mean_price_by_location[0]:.2f}00,000\")\nprint(f\"  Coastal (1): ${mean_price_by_location[1]:.2f}00,000\")\n\n# My interpretation: Coastal areas have significantly higher median house values,\n# which makes sense given proximity to ocean and desirable California locations.\n```\n\n**Why this matters:** AI tools like Gemini are powerful accelerators, but they don't replace understanding. In business, you need to explain your analyses to stakeholders, defend your methodology, and catch errors. Blindly trusting AI outputs is risky.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Let's Test Your Setup\n",
    "\n",
    "Run the cell below to verify your Colab environment is ready. You should see:\n",
    "- Python version\n",
    "- Library versions\n",
    "- A simple plot\n",
    "- ‚úì Setup confirmation\n",
    "\n",
    "If you see any errors, use Gemini to help debug!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Day 1 Setup Test Cell\n# This cell verifies your Colab environment is properly configured\n\n# Import essential libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nimport sklearn\nimport sys\nimport warnings\n\n# Suppress warnings for cleaner output\nwarnings.filterwarnings('ignore')\n\n# Set random seed for reproducibility\nRANDOM_SEED = 474\nnp.random.seed(RANDOM_SEED)\n\n# Display settings\npd.set_option('display.max_columns', None)\npd.set_option('display.precision', 3)\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (10, 6)\n\n# Version check\nprint(\"=== ENVIRONMENT CHECK ===\")\nprint(f\"Python version: {sys.version.split()[0]}\")\nprint(f\"pandas: {pd.__version__}\")\nprint(f\"numpy: {np.__version__}\")\nprint(f\"matplotlib: {plt.matplotlib.__version__}\")\nprint(f\"seaborn: {sns.__version__}\")\nprint(f\"scikit-learn: {sklearn.__version__}\")\n\n# Create a simple test plot\nx = np.linspace(0, 10, 100)\ny = np.sin(x) + np.random.normal(0, 0.1, 100)\n\nplt.figure(figsize=(8, 4))\nplt.plot(x, y, 'o', alpha=0.5, label='Noisy sine wave')\nplt.plot(x, np.sin(x), 'r-', linewidth=2, label='True function')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Test Plot: If you see this, your environment works!')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(\"\\n‚úì Setup complete! Your Colab environment is ready.\")\nprint(f\"‚úì Random seed set to {RANDOM_SEED} for reproducibility\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Introduction to Predictive Analytics\n\n### 4.1 What is Predictive Analytics? Motivation Examples\n\n**Predictive analytics** is the practice of extracting patterns from historical data to make predictions about future or unknown outcomes. It's used everywhere in modern business and society:\n\n**Example 1: Email Spam Detection**\n- **Business problem:** Users receive hundreds of unwanted emails daily, wasting time and risking security\n- **Predictive task:** Classify incoming emails as \"spam\" or \"ham\" (legitimate)\n- **How it works:** Train a model on historical emails labeled as spam/ham; model learns patterns (keywords, sender characteristics, formatting) and predicts labels for new emails\n- **Business impact:** Gmail blocks 99.9% of spam, saving billions of user hours annually\n\n**Example 2: Handwritten Digit Recognition (Zip Codes)**\n- **Business problem:** US Postal Service processes millions of handwritten envelopes daily; manual sorting is slow and expensive\n- **Predictive task:** Read handwritten zip codes from envelope images (0-9 classification)\n- **How it works:** Convolutional neural networks trained on thousands of handwritten digit examples\n- **Business impact:** Automated mail sorting reduced delivery time and costs by 40%\n\n<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/lecture_slides/01_introduction/figs/zip_code.png\" width=\"400\">\n\n*Figure: Handwritten zip code recognition example from USPS automated sorting system.*\n\n**Example 3: Netflix Prize (Movie Recommendations)**\n- **Business problem:** Netflix wants to recommend movies users will enjoy to increase engagement and retention\n- **Predictive task:** Predict how a user would rate a movie they haven't watched yet\n- **How it works:** Collaborative filtering using millions of past user ratings\n- **Business impact:** $1M prize competition (2006-2009) improved recommendation accuracy by 10%, resulting in billions in retained subscriptions\n\n<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/lecture_slides/01_introduction/figs/netflix_prize.png\" width=\"600\">\n\n*Figure: Netflix Prize leaderboard showing the competition that advanced recommendation systems.*\n\n**Common thread:** All three examples follow the same workflow:\n1. Collect historical data with known outcomes\n2. Extract relevant features\n3. Train a predictive model\n4. Deploy the model to make predictions on new data\n5. Measure business impact\n\n**This course:** You'll learn the entire workflow, from raw data to deployed model, with emphasis on doing it **correctly** (avoiding leakage, overfitting) and **responsibly** (fairness, transparency).\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.2 The Statistical Learning Framework\n\nAt the core of predictive analytics is **statistical learning:** the set of tools for understanding and modeling relationships between variables.\n\n**The fundamental equation:**\n\n$$Y = f(X) + \\epsilon$$\n\nWhere:\n- **Y:** The outcome variable (target, response, dependent variable) we want to predict\n  - Examples: spam/ham label, digit 0-9, movie rating, **house price**\n- **X:** The input variables (features, predictors, independent variables) we use to make predictions\n  - Examples: email text, pixel intensities, user demographics, **median income, location, house age**\n- **f:** The unknown function relating X to Y (this is what we're trying to learn from data)\n- **Œµ (epsilon):** Random error/noise that cannot be predicted (irreducible error)\n\n**Visual intuition using California Housing data:**\n\nLet's look at the relationship between median income (MedInc) and median house value (MedHouseVal):\n\n<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/lecture_slides/01_introduction/figs/2_1-1.png\" width=\"700\">\n\n*Figure: Original ISLP example showing Sales vs. advertising spending. The same concept applies to our housing data below.*\n\n---\n\n### California Housing Example: Income vs. House Value\n\nNow let's see this with our actual dataset - the relationship between median income and house values:"
  },
  {
   "cell_type": "code",
   "source": "# Generate California Housing visualization for Statistical Learning Framework\n# This demonstrates Y = f(X) + Œµ using real data\n\n# Load a sample for visualization (use first 5000 points for clearer plot)\nfrom sklearn.datasets import fetch_california_housing\ncalifornia_demo = fetch_california_housing(as_frame=True)\ndf_demo = california_demo.frame.sample(n=5000, random_state=474)\n\n# Create scatter plot: MedInc (X) vs MedHouseVal (Y)\nplt.figure(figsize=(10, 6))\nplt.scatter(df_demo['MedInc'], df_demo['MedHouseVal'], alpha=0.4, s=20, color='steelblue')\nplt.xlabel('Median Income (in $10,000s)', fontsize=12)\nplt.ylabel('Median House Value (in $100,000s)', fontsize=12)\nplt.title('California Housing: Income vs. House Value\\n(Demonstrating Y = f(X) + Œµ)', fontsize=14)\nplt.grid(True, alpha=0.3)\n\n# Add a trend line to show f(X)\nfrom scipy.stats import linregress\nslope, intercept, r_value, p_value, std_err = linregress(df_demo['MedInc'], df_demo['MedHouseVal'])\nline_x = np.array([df_demo['MedInc'].min(), df_demo['MedInc'].max()])\nline_y = slope * line_x + intercept\nplt.plot(line_x, line_y, 'r-', linewidth=2, label=f'Estimated f(X) (R¬≤={r_value**2:.3f})')\n\nplt.legend()\nplt.tight_layout()\nplt.show()\n\nprint(f\"üí° Notice the clear positive relationship: higher income ‚Üí higher house values\")\nprint(f\"üí° But there's scatter around the red line - that's the 'noise' (Œµ) we can't eliminate\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Supervised vs. Unsupervised Learning\n",
    "\n",
    "**Machine learning** tasks fall into two broad categories:\n",
    "\n",
    "**Supervised Learning** *(focus of this course)*\n",
    "\n",
    "**Definition:** We have historical data where the outcome Y is **known** for all training observations. The goal is to learn f(X) so we can predict Y for new observations where it's unknown.\n",
    "\n",
    "**Two types:**\n",
    "1. **Regression:** Y is quantitative (continuous number)\n",
    "   - Examples: predict house price, forecast sales, estimate customer lifetime value\n",
    "   - Evaluation: mean squared error (MSE), R¬≤, mean absolute error (MAE)\n",
    "\n",
    "2. **Classification:** Y is qualitative (discrete category)\n",
    "   - Examples: spam/ham, churn/stay, approve/deny loan, diagnose disease\n",
    "   - Evaluation: accuracy, precision, recall, ROC AUC\n",
    "\n",
    "**Key point:** We train on **labeled data** (X and Y pairs) and deploy the model to predict Y for unlabeled data (just X).\n",
    "\n",
    "---\n",
    "\n",
    "**Unsupervised Learning** *(not covered in this course)*\n",
    "\n",
    "**Definition:** We only have X (features), no outcome Y. The goal is to discover structure, patterns, or groupings in the data.\n",
    "\n",
    "**Common tasks:**\n",
    "- **Clustering:** Group similar observations together\n",
    "  - Example: segment customers into personas based on demographics and behavior\n",
    "- **Dimensionality reduction:** Compress high-dimensional data while preserving information\n",
    "  - Example: visualize high-dimensional product reviews in 2D\n",
    "- **Anomaly detection:** Find unusual observations\n",
    "  - Example: detect fraudulent transactions\n",
    "\n",
    "**Why unsupervised is harder:** No \"correct answer\" to check against, so evaluation is subjective.\n",
    "\n",
    "**This course:** We focus exclusively on **supervised learning** (regression and classification) because these are the most common business applications of predictive analytics.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.4 End-to-End Predictive Modeling Workflow\n\n**Real-world predictive analytics projects** follow a structured workflow with 9 core steps. Let's preview the journey using a **customer churn prediction** example.\n\n**Business Problem:** A bank notices customers are closing accounts. They want to predict which customers will churn next quarter so they can proactively offer retention incentives (fee waivers, better rates, personalized service).\n\n**Target Variable (Y):** `Exited` (binary: 0 = stayed, 1 = closed account)  \n**Features (X):** Customer demographics (age, gender, geography), account info (balance, # products, credit score), and behavior (tenure, active member status)\n\n---\n\n**Step 0: Setup and Configuration**\n- Import libraries (pandas, numpy, matplotlib, scikit-learn)\n- Set random seed for reproducibility (`RANDOM_SEED = 474`)\n- Configure display settings\n\n**Step 1: Data Loading and Sanity Checks**\n- Load train and test CSVs\n- Validate data types and shapes\n- Identify and exclude non-feature columns (IDs, names)\n- Check target variable is binary and present in train, absent in test\n\n**Step 2: Exploratory Data Analysis (EDA)**\n- Examine target distribution (class balance)\n- Analyze univariate distributions (histograms, boxplots)\n- Explore bivariate relationships (features vs. target)\n- Compute correlations and mutual information\n- **Business insight:** Older customers, inactive members, and those with 3-4 products have higher churn rates\n\n**Step 3: Data Preparation and Feature Engineering**\n- Handle missing values (imputation)\n- Encode categorical variables (one-hot encoding)\n- Scale numeric features (standardization)\n- Create new features if needed (interactions, ratios)\n- Build preprocessing pipeline to ensure reproducibility\n\n**Step 4: Baseline Model**\n- Train simple logistic regression (interpretable, fast)\n- Evaluate with cross-validation (ROC AUC, accuracy)\n- Establish performance benchmark\n- **Baseline AUC:** ~0.75\n\n**Step 5: Advanced Models**\n- Try more complex algorithms (LASSO, tree-based methods, ensembles)\n- Tune hyperparameters\n- Compare performance vs. baseline\n- **Best model AUC:** ~0.82 (LASSO with feature selection)\n\n**Step 6: Model Comparison and Visualization**\n- Plot performance metrics with confidence intervals\n- Create comparison table (AUC, error rates, sparsity)\n\n**Step 7: Model Selection and Rationale**\n- Choose best model based on performance, stability, and parsimony\n- Document decision criteria transparently\n\n**Step 8: Final Training**\n- Retrain selected model on **full training data**\n- Save model artifact (`.joblib` file)\n- Record metadata (timestamp, features, hyperparameters)\n\n**Step 9: Test Set Predictions and Submission**\n- Load test data\n- Generate predictions (probabilities)\n- Export submission CSV with IDs and predictions\n- Validate format and alignment\n\n**Step 99: Reporting and Reproducibility**\n- Write `requirements.txt` for package versions\n- Document assumptions and decisions\n- Create executive summary of findings\n\n---\n\n**Key principles throughout:**\n- ‚úì **Split first, preprocess second:** Avoid leakage by separating data before any transformations\n- ‚úì **Use cross-validation:** Don't trust a single train/test split; average performance across multiple folds\n- ‚úì **Lock the test set:** Don't peek at test data until final evaluation (one-time use only)\n- ‚úì **Document everything:** Future you (and your stakeholders) need to understand your choices\n- ‚úì **Reproducibility matters:** Set seeds (RANDOM_SEED = 474), version packages, save pipelines\n\n**In this notebook:** We'll practice Steps 0-2 (setup, EDA, splitting). Future notebooks will cover Steps 3-9.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Data Leakage: The Silent Killer of Predictive Models\n",
    "\n",
    "**Data leakage** occurs when information from outside the training dataset unintentionally \"leaks\" into the model, causing it to perform well during development but fail catastrophically in production.\n",
    "\n",
    "**Why it's dangerous:**\n",
    "- Your model will show excellent performance on validation data (high AUC, low error)\n",
    "- You'll deploy the model thinking it works\n",
    "- In production, performance will collapse because the leaked information isn't available for new predictions\n",
    "- **Business impact:** Wasted resources, lost revenue, damaged credibility\n",
    "\n",
    "**Two types of leakage:**\n",
    "\n",
    "---\n",
    "\n",
    "**Type 1: Target Leakage**\n",
    "\n",
    "**Definition:** A predictor includes information that will not be available at the time you need to make a prediction.\n",
    "\n",
    "**Example 1 - Pneumonia Prediction (from Kaggle tutorial):**\n",
    "- **Task:** Predict which patients admitted to the hospital have pneumonia\n",
    "- **Leaked feature:** `got_antibiotic` (whether patient received antibiotics)\n",
    "- **Problem:** Antibiotics are prescribed **after** diagnosis. The feature encodes the target directly.\n",
    "- **Result:** Model shows 99% accuracy in validation, but in production you don't know if a newly admitted patient will get antibiotics (that's what you're trying to predict!).\n",
    "\n",
    "**Example 2 - Customer Churn:**\n",
    "- **Task:** Predict which customers will close accounts next month\n",
    "- **Leaked feature:** `num_service_calls_next_month` (how many times customer called support next month)\n",
    "- **Problem:** This data doesn't exist yet when you make the prediction.\n",
    "- **Result:** Model learns that customers with many calls next month churn, but you can't know future call volume.\n",
    "\n",
    "**How to detect target leakage:**\n",
    "- Ask: \"Will this feature be available at prediction time?\"\n",
    "- Check for suspiciously high correlations between a feature and the target (r > 0.9)\n",
    "- Think about temporal order: does the feature come before or after the outcome?\n",
    "\n",
    "---\n",
    "\n",
    "**Type 2: Train-Test Contamination**\n",
    "\n",
    "**Definition:** Information from the validation or test set influences the training process, causing overly optimistic performance estimates.\n",
    "\n",
    "**Example 1 - Preprocessing Leakage:**\n",
    "```python\n",
    "# WRONG: Standardize before splitting\n",
    "X_scaled = StandardScaler().fit_transform(X)  # Uses ALL data including test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y)\n",
    "\n",
    "# Problem: Mean and std from test set influenced training data scaling\n",
    "```\n",
    "\n",
    "**Correct approach:**\n",
    "```python\n",
    "# RIGHT: Split first, then fit scaler only on training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "scaler = StandardScaler().fit(X_train)  # Learn from train only\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)  # Apply learned parameters\n",
    "```\n",
    "\n",
    "**Example 2 - Feature Selection Leakage:**\n",
    "- **Wrong:** Compute feature correlations on full dataset, then split\n",
    "- **Right:** Split first, then compute correlations only on training data\n",
    "\n",
    "**Example 3 - Hyperparameter Tuning Leakage:**\n",
    "- **Wrong:** Use test set to choose between models or tune hyperparameters\n",
    "- **Right:** Use cross-validation on training set; lock test set until final evaluation\n",
    "\n",
    "---\n",
    "\n",
    "**The Golden Rule to Prevent Leakage:**\n",
    "\n",
    "> **‚ö†Ô∏è Split first, preprocess second, model third**\n",
    "\n",
    "**Workflow that prevents contamination:**\n",
    "1. **Split** data into train/validation/test **before any analysis**\n",
    "2. **EDA and preprocessing** should be fitted on training data only\n",
    "3. **Modeling and evaluation** use cross-validation within training data\n",
    "4. **Test set** is opened exactly once at the end for final performance estimate\n",
    "\n",
    "**Additional leakage prevention checklist:**\n",
    "- [ ] Remove ID columns and names (can encode target information)\n",
    "- [ ] Check feature descriptions for future information\n",
    "- [ ] Verify temporal alignment (features collected before outcome)\n",
    "- [ ] Use pipelines to ensure preprocessing is part of cross-validation\n",
    "- [ ] Never look at test set until final evaluation\n",
    "\n",
    "**Real-world impact:** Kaggle competitions have been won and lost based on proper leakage prevention. In business, a leaky model can cost millions in bad decisions.\n",
    "\n",
    "**Further reading:** [Kaggle Data Leakage Tutorial](https://www.kaggle.com/code/alexisbcook/data-leakage)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Assessing Model Accuracy: Train vs. Test Error\n",
    "\n",
    "**Question:** How do we know if our model $\\hat{f}(X)$ is any good?\n",
    "\n",
    "**Naive approach:** Compute prediction error on the training data:\n",
    "\n",
    "$$\\text{MSE}_{\\text{train}} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{f}(x_i))^2$$\n",
    "\n",
    "**Problem:** Training error is **optimistically biased**. As model complexity increases, training error always decreases (even if the model is overfitting).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/lecture_slides/01_introduction/figs/2_6-1.png\" width=\"500\">\n",
    "\n",
    "*Figure: A highly flexible spline fits every training point perfectly (MSE = 0), but this is **overfitting** - the model won't generalize to new data.*\n",
    "\n",
    "---\n",
    "\n",
    "**Better approach:** Use fresh **test data** that the model has never seen:\n",
    "\n",
    "$$\\text{MSE}_{\\text{test}} = \\frac{1}{m} \\sum_{i=1}^{m} (y_i^{\\text{test}} - \\hat{f}(x_i^{\\text{test}}))^2$$\n",
    "\n",
    "**Key insight:** Test error measures **generalization** - how well the model performs on new data from the same population.\n",
    "\n",
    "**What we want:** A model that minimizes **test error**, not training error.\n",
    "\n",
    "**The fundamental trade-off:**\n",
    "- Simple models (e.g., linear regression) may have high training error but generalize well (low test error)\n",
    "- Complex models (e.g., deep neural networks) can have zero training error but poor test error (overfitting)\n",
    "\n",
    "**Goal of this course:** Learn to build models that generalize well by:\n",
    "1. Using proper train/validation/test splits\n",
    "2. Employing cross-validation for robust evaluation\n",
    "3. Understanding the bias-variance trade-off (next section)\n",
    "4. Preventing overfitting through regularization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 The Curse of Dimensionality\n",
    "\n",
    "**Question:** Why can't we just use more features to make better predictions?\n",
    "\n",
    "**Intuition:** In high dimensions, data becomes sparse, and \"similar\" observations become rare. This makes pattern recognition exponentially harder.\n",
    "\n",
    "**Visual demonstration:**\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/lecture_slides/01_introduction/figs/2_1_4_1.png\" width=\"350\">\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/lecture_slides/01_introduction/figs/2_1_4_2.png\" width=\"350\">\n",
    "\n",
    "*Figure: To capture 10% of data points in 1D, we need a small interval. In 2D, we need a much larger circle. As dimensions increase, the \"neighborhood\" explodes.*\n",
    "\n",
    "**The mathematical problem:**\n",
    "\n",
    "Suppose we want to predict Y using K-Nearest Neighbors by averaging the 10% closest training points:\n",
    "\n",
    "- **1 dimension (p=1):** If we have 100 points, we need to extend ¬±5 units to capture 10\n",
    "- **2 dimensions (p=2):** Need a circle with radius ~18 to capture 10 points (area grows as r¬≤)\n",
    "- **10 dimensions (p=10):** Need a hypersphere with radius ~80 (volume grows as r¬π‚Å∞)\n",
    "\n",
    "**Bottom panel in figures above:** As dimensionality increases, neighborhoods must expand dramatically to capture the same fraction of points. In 10 dimensions, even capturing 1% of data requires looking across 80% of the feature space!\n",
    "\n",
    "**Practical implications:**\n",
    "\n",
    "1. **Data sparsity:** In high dimensions, all points are far apart. \"Nearest neighbors\" aren't actually nearby.\n",
    "2. **Overfitting:** Models have too much flexibility relative to available data, memorizing noise instead of learning patterns.\n",
    "3. **Computational cost:** Algorithms that search neighborhoods (KNN, kernel methods) become prohibitively slow.\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "- **Feature selection:** Keep only the most predictive features\n",
    "- **Dimensionality reduction:** PCA, autoencoders compress high-dimensional data\n",
    "- **Regularization:** Penalize model complexity (LASSO, Ridge)\n",
    "- **Parametric models:** Linear models assume structure, reducing effective dimensionality\n",
    "\n",
    "**Rule of thumb:** You need exponentially more data as the number of features grows. With 10 features and 100 samples, you're in good shape. With 1000 features and 100 samples, you're in trouble.\n",
    "\n",
    "**This course:** We'll learn feature selection techniques (forward stepwise, LASSO) to combat the curse of dimensionality.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Flexibility vs. Interpretability Trade-off\n",
    "\n",
    "**Question:** Should we use simple or complex models?\n",
    "\n",
    "**Answer:** It depends on your goals and constraints.\n",
    "\n",
    "**The spectrum:**\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/lecture_slides/01_introduction/figs/2_7-1.png\" width=\"600\">\n",
    "\n",
    "*Figure: Different models trade off flexibility (ability to fit complex patterns) with interpretability (ability to explain predictions).*\n",
    "\n",
    "---\n",
    "\n",
    "**Left side: High Interpretability, Low Flexibility**\n",
    "\n",
    "**Linear Models:**\n",
    "$$\\hat{Y} = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_p X_p$$\n",
    "\n",
    "**Pros:**\n",
    "- Coefficients Œ≤ directly quantify feature importance and direction of effect\n",
    "- Easy to explain to non-technical stakeholders\n",
    "- Fast to train and predict\n",
    "- Less prone to overfitting\n",
    "\n",
    "**Cons:**\n",
    "- Can't capture non-linear relationships or interactions\n",
    "- May underfit if true relationship is complex\n",
    "\n",
    "**When to use:** Regulatory environments (lending, healthcare), need to explain decisions, small datasets\n",
    "\n",
    "---\n",
    "\n",
    "**Middle: Moderate Flexibility and Interpretability**\n",
    "\n",
    "**Tree-based Models (Decision Trees, Random Forests):**\n",
    "- Pros: Handle non-linearities and interactions, feature importance scores, visual decision rules\n",
    "- Cons: Can overfit (single trees), hard to interpret individual predictions (forests)\n",
    "\n",
    "**Generalized Additive Models (GAMs):**\n",
    "- Pros: Capture non-linearities while maintaining some interpretability\n",
    "- Cons: Don't automatically capture interactions\n",
    "\n",
    "**When to use:** Exploratory analysis, moderate-sized datasets, need balance of accuracy and explainability\n",
    "\n",
    "---\n",
    "\n",
    "**Right side: High Flexibility, Low Interpretability**\n",
    "\n",
    "**Deep Neural Networks, Gradient Boosting (XGBoost, LightGBM):**\n",
    "\n",
    "**Pros:**\n",
    "- Can learn extremely complex patterns\n",
    "- Often achieve best predictive accuracy\n",
    "- Handle high-dimensional data\n",
    "\n",
    "**Cons:**\n",
    "- \"Black box\" - hard or impossible to explain individual predictions\n",
    "- Require large datasets and careful tuning\n",
    "- Slow to train\n",
    "- Easy to overfit\n",
    "\n",
    "**When to use:** Image/text/speech data, large datasets, prediction accuracy is paramount, don't need to explain decisions\n",
    "\n",
    "---\n",
    "\n",
    "**Practical guidance:**\n",
    "\n",
    "**Start simple:** Always begin with logistic regression or a simple tree. This establishes a baseline and helps you understand the data.\n",
    "\n",
    "**Add complexity only if needed:** If the simple model performs well enough for the business problem, stop there. Explainability is valuable.\n",
    "\n",
    "**Use interpretability tools:** Even for complex models, SHAP values and partial dependence plots can provide some explanation.\n",
    "\n",
    "**Consider the stakes:** High-risk decisions (medical diagnosis, loan approval) require more interpretability than low-risk ones (movie recommendations).\n",
    "\n",
    "**This course:** We'll cover the full spectrum, but emphasize interpretable models (linear, regularized regression, trees) since these are most common in business analytics.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9 The Bias-Variance Trade-off\n",
    "\n",
    "**The most important concept in predictive modeling:** Understanding why models fail and how to fix them.\n",
    "\n",
    "**Test error decomposition:**\n",
    "\n",
    "$$\\mathbb{E}[(Y - \\hat{f}(X))^2] = \\underbrace{(\\text{Bias}[\\hat{f}(X)])^2}_{\\text{underfitting}} + \\underbrace{\\text{Var}[\\hat{f}(X)]}_{\\text{overfitting}} + \\underbrace{\\sigma^2}_{\\text{irreducible}}$$\n",
    "\n",
    "Three components:\n",
    "\n",
    "---\n",
    "\n",
    "**1. Bias¬≤: Error from Wrong Assumptions (Underfitting)**\n",
    "\n",
    "**Definition:** The error introduced by approximating a complex real-world problem with a simplified model.\n",
    "\n",
    "**High bias means:**\n",
    "- Model is too rigid\n",
    "- Misses important patterns (systematic error)\n",
    "- **Underfits** the training data\n",
    "\n",
    "**Example:** Using a straight line to fit a curved relationship\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/lecture_slides/01_introduction/figs/2_1_5.png\" width=\"400\">\n",
    "\n",
    "*Figure: Linear model (high bias) misses the curvature in the data.*\n",
    "\n",
    "**Typical causes:**\n",
    "- Overly simple model (linear when should be non-linear)\n",
    "- Too few features\n",
    "- Heavy regularization\n",
    "\n",
    "**How to detect:** Both training and test error are high\n",
    "\n",
    "**How to fix:** Add features, increase model complexity, reduce regularization\n",
    "\n",
    "---\n",
    "\n",
    "**2. Variance: Error from Sensitivity to Training Data (Overfitting)**\n",
    "\n",
    "**Definition:** The amount by which the model's predictions would change if we trained on a different sample from the same population.\n",
    "\n",
    "**High variance means:**\n",
    "- Model is too flexible\n",
    "- Overly sensitive to noise in training data\n",
    "- **Overfits** by memorizing training examples\n",
    "\n",
    "**Example:** High-degree polynomial fits training data perfectly but wiggles wildly\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/lecture_slides/01_introduction/figs/2_6-1.png\" width=\"400\">\n",
    "\n",
    "*Figure: Very flexible spline (high variance) fits every training point but doesn't generalize.*\n",
    "\n",
    "**Typical causes:**\n",
    "- Overly complex model\n",
    "- Too many features relative to sample size\n",
    "- No regularization\n",
    "\n",
    "**How to detect:** Training error is low, but test error is high (large gap)\n",
    "\n",
    "**How to fix:** Simplify model, remove features, add regularization, get more training data\n",
    "\n",
    "---\n",
    "\n",
    "**3. Irreducible Error œÉ¬≤: Random Noise**\n",
    "\n",
    "**Definition:** Variability in Y that cannot be explained by X, no matter how good our model is.\n",
    "\n",
    "**Sources:**\n",
    "- Measurement error\n",
    "- Omitted variables (factors affecting Y that aren't in X)\n",
    "- True randomness in the process\n",
    "\n",
    "**Key point:** This sets a lower bound on test error. No amount of modeling can reduce it.\n",
    "\n",
    "---\n",
    "\n",
    "**Visualizing the Trade-off:**\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/lecture_slides/01_introduction/figs/2_9-1-1.png\" width=\"400\">\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/lecture_slides/01_introduction/figs/2_9-1-2.png\" width=\"400\">\n",
    "\n",
    "**Top panel:** Three fitted models with different flexibility\n",
    "- **Orange (low flexibility):** High bias (misses pattern), low variance (stable)\n",
    "- **Blue (medium flexibility):** Balanced bias and variance (best)\n",
    "- **Green (high flexibility):** Low bias (captures pattern), high variance (overfits noise)\n",
    "\n",
    "**Bottom panel:** Error curves vs. model flexibility\n",
    "- **Gray (training error):** Monotonically decreases as flexibility increases\n",
    "- **Red (test error):** U-shaped - decreases initially, then increases due to overfitting\n",
    "- **Sweet spot:** Minimum test error (blue vertical line) balances bias and variance\n",
    "\n",
    "---\n",
    "\n",
    "**The Trade-off:**\n",
    "\n",
    "As model flexibility increases:\n",
    "- **Bias decreases** (better fit to training data)\n",
    "- **Variance increases** (more sensitive to training sample)\n",
    "\n",
    "**Goal:** Choose complexity that **minimizes expected test error** by balancing bias and variance.\n",
    "\n",
    "---\n",
    "\n",
    "**Different scenarios:**\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/lecture_slides/01_introduction/figs/2_10-1.png\" width=\"400\">\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/lecture_slides/01_introduction/figs/2_11-1.png\" width=\"400\">\n",
    "\n",
    "**Left:** True relationship is smooth ‚Üí simpler models perform well  \n",
    "**Right:** True relationship is wiggly ‚Üí more flexible models needed\n",
    "\n",
    "**Lesson:** Optimal model complexity depends on the problem. There's no universal \"best\" model.\n",
    "\n",
    "---\n",
    "\n",
    "**Decomposition across complexities:**\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/lecture_slides/01_introduction/figs/2_12-1.png\" width=\"700\">\n",
    "\n",
    "*Figure: Test MSE (red) = Squared Bias (blue) + Variance (orange) + Irreducible Error (dotted). The sweet spot (vertical line) minimizes test error.*\n",
    "\n",
    "---\n",
    "\n",
    "**Practical implications:**\n",
    "\n",
    "**Tools to manage the trade-off:**\n",
    "1. **Cross-validation:** Estimate test error without touching test set\n",
    "2. **Regularization:** Constrain model complexity (LASSO, Ridge, Elastic Net)\n",
    "3. **Ensemble methods:** Reduce variance by averaging many models (Random Forest, Bagging)\n",
    "4. **Early stopping:** Stop training before overfitting (neural networks)\n",
    "5. **Feature selection:** Remove irrelevant features that add variance without reducing bias\n",
    "\n",
    "**This course:** Every modeling decision we make (train/val/test splits, cross-validation, hyperparameter tuning, regularization) is about managing the bias-variance trade-off.\n",
    "\n",
    "**Key takeaway:** Understanding this trade-off is the difference between building models that work in notebooks vs. models that work in production.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Hands-On Practice: California Housing Price Prediction EDA and Splitting\n\nNow that we understand the theory, let's apply it to a **real regression problem**.\n\n### 5.1 Business Case: California Housing Price Prediction\n\n**Business Context:**\n\nYou are a data analyst at a California real estate investment firm. The firm makes money by identifying undervalued properties, purchasing them, and either renting them out or reselling them at a profit. Accurate price predictions are crucial for two key decisions:\n\n1. **Investment decisions:** Which properties should the firm buy? (those priced below predicted value)\n2. **Portfolio valuation:** What is the current market value of the firm's existing properties?\n\nCurrently, the firm relies on manual appraisals that are expensive ($500-$1000 per property) and slow (2-3 weeks turnaround). Your task is to build a **predictive model** that can estimate house prices instantly based on property characteristics and location.\n\n**Your task:** Build a regression model that predicts median house values for California census block groups.\n\n**Target Variable:** `MedHouseVal` (continuous, measured in $100,000s)\n- **Range:** Typically $0.15 to $5.0 (representing $15,000 to $500,000)\n- **Interpretation:** Median house value for houses within a census block group\n\n**Available Features:**\n- **Location:** Latitude, Longitude (geographic coordinates)\n- **Demographics:** MedInc (median income in block group, in $10,000s), Population, AveOccup (average occupancy)\n- **Housing Stock:** HouseAge (median age of houses), AveRooms (average rooms per household), AveBedrms (average bedrooms per household)\n\n**Success Metrics:**\n- **Primary:** MAE (Mean Absolute Error) - average prediction error in $100,000s\n  - **Why MAE?** Directly interpretable in business terms (\"on average, we're off by $X\")\n- **Secondary:** RMSE (Root Mean Squared Error) - penalizes large errors more heavily\n  - **Why RMSE?** Buying a house at 2√ó overpayment is worse than 2√ó small errors\n\n**Business Constraint:** The firm can tolerate prediction errors up to $50,000 (MAE < 0.5) for most properties, but large errors ($100,000+) on expensive properties could be financially disastrous. Model must balance overall accuracy with avoiding catastrophic mispredictions.\n\n---\n\n### 5.2 Load the Data from sklearn and Save as CSV\n\nWe'll use the California Housing dataset from sklearn, which is a classic regression benchmark. We'll demonstrate how to save and reload it as a CSV file for reproducibility."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 1: Load California Housing dataset from sklearn\nfrom sklearn.datasets import fetch_california_housing\n\n# Fetch the dataset as a pandas DataFrame\ncalifornia = fetch_california_housing(as_frame=True)\ndf_full = california.frame\n\nprint(\"=== DATASET LOADED FROM SKLEARN ===\")\nprint(f\"Shape: {df_full.shape[0]} rows √ó {df_full.shape[1]} columns\")\nprint(f\"\\nColumn names:\")\nprint(df_full.columns.tolist())\nprint(f\"\\nFirst 5 rows:\")\ndisplay(df_full.head())\n\n# Step 2: Save to CSV (demonstrating data persistence)\ncsv_path = 'california_housing.csv'\ndf_full.to_csv(csv_path, index=False)\nprint(f\"\\n‚úì Dataset saved to: {csv_path}\")\n\n# Step 3: Reload from CSV (simulating real-world workflow)\ndf = pd.read_csv(csv_path)\nprint(f\"\\n‚úì Dataset reloaded from CSV\")\nprint(f\"Reloaded shape: {df.shape}\")\n\n# Verify target variable\nprint(f\"\\n=== TARGET VARIABLE ===\")\nprint(f\"Target: 'MedHouseVal' (Median House Value in $100,000s)\")\nprint(f\"Type: Continuous (Regression problem)\")\nprint(f\"Range: {df['MedHouseVal'].min():.2f} to {df['MedHouseVal'].max():.2f}\")\nprint(f\"Mean: ${df['MedHouseVal'].mean():.2f}00,000\")\nprint(f\"Median: ${df['MedHouseVal'].median():.2f}00,000\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 5.3 Identify Features and Target\n\n**Critical step:** Before EDA, we must clearly identify our target variable and feature set.\n\n**Target Variable:** `MedHouseVal` - This is what we want to predict\n\n**Features:** All other columns represent potential predictors:\n- **MedInc:** Median income in block group\n- **HouseAge:** Median house age in block group\n- **AveRooms:** Average number of rooms per household\n- **AveBedrms:** Average number of bedrooms per household\n- **Population:** Block group population\n- **AveOccup:** Average household occupancy\n- **Latitude:** Latitude coordinate\n- **Longitude:** Longitude coordinate\n\n**No ID columns to remove:** Unlike many business datasets, California Housing is already clean with no row numbers or customer IDs that need to be excluded.\n\nLet's verify the data structure:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verify dataset structure\nprint(\"=== DATASET OVERVIEW ===\")\nprint(f\"Shape: {df.shape}\")\nprint(f\"\\nAll columns:\")\nfor idx, col in enumerate(df.columns, 1):\n    print(f\"  {idx}. {col}\")\n\n# Separate target from features\ntarget_col = 'MedHouseVal'\nfeature_cols = [col for col in df.columns if col != target_col]\n\nprint(f\"\\n=== MODELING SETUP ===\")\nprint(f\"Target variable: {target_col}\")\nprint(f\"Number of features: {len(feature_cols)}\")\nprint(f\"Feature list: {feature_cols}\")\n\n# Basic statistics on target\nprint(f\"\\n=== TARGET DISTRIBUTION ===\")\nprint(df[target_col].describe())\nprint(f\"\\nInterpretation:\")\nprint(f\"  - Median house values range from ${df[target_col].min():.2f}00,000 to ${df[target_col].max():.2f}00,000\")\nprint(f\"  - Average median value: ${df[target_col].mean():.2f}00,000\")\nprint(f\"  - Standard deviation: ${df[target_col].std():.2f}00,000\")\nprint(f\"\\n‚úì This is a REGRESSION problem (predicting continuous values)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. EDA Checklist: Types, Missingness, Target Distribution\n",
    "\n",
    "### 6.1 Data Types Audit\n",
    "\n",
    "First, let's understand what types of data we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check data types\nprint(\"=== DATA TYPES ===\")\nprint(df.dtypes)\nprint(\"\\n=== DATA INFO ===\")\ndf.info()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Missingness Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check for missing values\nmissing = df.isnull().sum()\nmissing_pct = (df.isnull().sum() / len(df)) * 100\n\nmissing_df = pd.DataFrame({\n    'Missing_Count': missing,\n    'Missing_Percentage': missing_pct\n})\n\nprint(\"=== MISSING VALUES ===\")\nif missing.sum() == 0:\n    print(\"‚úì No missing values detected!\")\nelse:\n    print(missing_df[missing_df['Missing_Count'] > 0])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Basic Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Descriptive statistics\nprint(\"=== DESCRIPTIVE STATISTICS ===\")\ndf.describe()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 6.4 Target Distribution (Regression)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize target distribution for regression\ntarget_col = 'MedHouseVal'\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Histogram\naxes[0].hist(df[target_col], bins=50, edgecolor='black', alpha=0.7, color='steelblue')\naxes[0].axvline(df[target_col].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: ${df[target_col].mean():.2f}00k')\naxes[0].axvline(df[target_col].median(), color='green', linestyle='--', linewidth=2, label=f'Median: ${df[target_col].median():.2f}00k')\naxes[0].set_xlabel('Median House Value ($100,000s)')\naxes[0].set_ylabel('Frequency')\naxes[0].set_title('Target Distribution: Median House Value')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Box plot\naxes[1].boxplot(df[target_col], vert=True, patch_artist=True,\n                boxprops=dict(facecolor='lightblue', alpha=0.7))\naxes[1].set_ylabel('Median House Value ($100,000s)')\naxes[1].set_title('Target Distribution: Box Plot')\naxes[1].grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"=== TARGET STATISTICS ===\")\nprint(f\"Count: {df[target_col].count()}\")\nprint(f\"Mean: ${df[target_col].mean():.2f}00,000\")\nprint(f\"Median: ${df[target_col].median():.2f}00,000\")\nprint(f\"Std Dev: ${df[target_col].std():.2f}00,000\")\nprint(f\"Min: ${df[target_col].min():.2f}00,000\")\nprint(f\"Max: ${df[target_col].max():.2f}00,000\")\n\nprint(f\"\\nüí° Key observations:\")\nprint(f\"  - Distribution is right-skewed (mean > median)\")\nprint(f\"  - Some high-value outliers visible in box plot\")\nprint(f\"  - This is typical for real estate prices\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize feature distributions\n# All features are numeric in California Housing dataset\nnumeric_cols = feature_cols\n\n# Plot histograms for all numeric features\nn_cols = 3\nn_rows = (len(numeric_cols) + n_cols - 1) // n_cols\n\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))\naxes = axes.flatten()\n\nfor idx, col in enumerate(numeric_cols):\n    axes[idx].hist(df[col], bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n    axes[idx].set_xlabel(col)\n    axes[idx].set_ylabel('Frequency')\n    axes[idx].set_title(f'{col} Distribution')\n    axes[idx].grid(True, alpha=0.3)\n    \n    # Add mean line\n    mean_val = df[col].mean()\n    axes[idx].axvline(mean_val, color='red', linestyle='--', linewidth=1.5, \n                      label=f'Mean: {mean_val:.2f}')\n    axes[idx].legend(fontsize=8)\n\n# Hide extra subplots\nfor idx in range(len(numeric_cols), len(axes)):\n    axes[idx].axis('off')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"üí° Feature distribution observations:\")\nprint(\"  - MedInc: Right-skewed (few very high income areas)\")\nprint(\"  - HouseAge: Fairly uniform distribution\")\nprint(\"  - AveRooms: Right-skewed (some very large houses)\")\nprint(\"  - Latitude/Longitude: Reflect California's geography\")\nprint(\"  - Population: Heavy right skew (some very dense areas)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Correlation matrix\ncorr_matrix = df.corr()\n\nplt.figure(figsize=(12, 10))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f', \n            square=True, linewidths=1)\nplt.title('Feature Correlation Matrix')\nplt.tight_layout()\nplt.show()\n\n# Correlations with target\nprint(\"\\n=== CORRELATIONS WITH TARGET ===\")\ntarget_corr = corr_matrix[target_col].sort_values(ascending=False)\nprint(target_corr)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train/Validation/Test Splits + Leakage Prevention\n",
    "\n",
    "### 7.1 Why Three Splits?\n",
    "\n",
    "- **Train set**: Used to fit models\n",
    "- **Validation set**: Used to tune hyperparameters and compare models\n",
    "- **Test set**: Final \"lockbox\" evaluation - touch only once at the end\n",
    "\n",
    "### 7.2 Creating Reproducible Splits\n",
    "\n",
    "> **‚ö†Ô∏è Critical Rule:** Always split data BEFORE any preprocessing to avoid leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Separate features and target\nX = df.drop(columns=[target_col])\ny = df[target_col]\n\nprint(f\"Features shape: {X.shape}\")\nprint(f\"Target shape: {y.shape}\")\n\n# Split: 60% train, 20% validation, 20% test\n# For regression, we don't need stratification (that's for classification)\nX_temp, X_test, y_temp, y_test = train_test_split(\n    X, y, test_size=0.20, random_state=RANDOM_SEED\n)\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X_temp, y_temp, test_size=0.25, random_state=RANDOM_SEED  # 0.25 * 0.80 = 0.20\n)\n\nprint(\"\\n=== SPLIT SIZES ===\")\nprint(f\"Train: {len(X_train)} samples ({len(X_train)/len(df)*100:.1f}%)\")\nprint(f\"Validation: {len(X_val)} samples ({len(X_val)/len(df)*100:.1f}%)\")\nprint(f\"Test: {len(X_test)} samples ({len(X_test)/len(df)*100:.1f}%)\")\nprint(f\"Total: {len(X_train) + len(X_val) + len(X_test)} samples\")\n\n# Check target distribution across splits\nprint(\"\\n=== TARGET DISTRIBUTION ACROSS SPLITS ===\")\nprint(f\"Train - Mean: ${y_train.mean():.2f}00k, Std: ${y_train.std():.2f}00k\")\nprint(f\"Val   - Mean: ${y_val.mean():.2f}00k, Std: ${y_val.std():.2f}00k\")\nprint(f\"Test  - Mean: ${y_test.mean():.2f}00k, Std: ${y_test.std():.2f}00k\")\n\nprint(\"\\n‚úì Data successfully split into train/validation/test sets\")\nprint(f\"‚úì Using RANDOM_SEED = {RANDOM_SEED} for reproducibility\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Split Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize target distribution across splits (regression)\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\nfor idx, (split_name, split_y) in enumerate([('Train', y_train), ('Validation', y_val), ('Test', y_test)]):\n    axes[idx].hist(split_y, bins=30, alpha=0.7, edgecolor='black', color='steelblue')\n    axes[idx].axvline(split_y.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: ${split_y.mean():.2f}00k')\n    axes[idx].axvline(split_y.median(), color='green', linestyle='--', linewidth=2, label=f'Median: ${split_y.median():.2f}00k')\n    axes[idx].set_xlabel('Median House Value ($100,000s)')\n    axes[idx].set_ylabel('Frequency')\n    axes[idx].set_title(f'{split_name} Set (n={len(split_y)})')\n    axes[idx].legend(fontsize=9)\n    axes[idx].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"=== DISTRIBUTION CONSISTENCY CHECK ===\")\nprint(f\"Train - Mean: ${y_train.mean():.3f}00k, Median: ${y_train.median():.3f}00k\")\nprint(f\"Val   - Mean: ${y_val.mean():.3f}00k, Median: ${y_val.median():.3f}00k\")\nprint(f\"Test  - Mean: ${y_test.mean():.3f}00k, Median: ${y_test.median():.3f}00k\")\nprint(\"\\n‚úì Target distributions are similar across splits\")\nprint(\"‚úì Random splitting preserves overall data characteristics\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Leakage Sniff Test\n",
    "\n",
    "**Common leakage patterns to watch for:**\n",
    "\n",
    "1. **Target leakage**: Features that contain information from the future or are derived from the target\n",
    "2. **Test set contamination**: Using test data to make preprocessing or modeling decisions\n",
    "3. **Time-based leakage**: Using future data to predict the past\n",
    "4. **Preprocessing leakage**: Fitting transformers on the full dataset before splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Leakage checklist\nprint(\"=== LEAKAGE SNIFF TEST ===\")\nprint(\"\\n‚úì Checklist:\")\nprint(\"  [‚úì] Split data before any preprocessing\")\nprint(\"  [‚úì] No overlap between train/val/test indices\")\nprint(\"  [‚úì] Random splitting ensures fair distribution for regression\")\nprint(\"  [‚úì] Check feature descriptions for suspicious variables\")\nprint(\"  [‚úì] Verify no future information in features\")\nprint(\"  [‚úì] Confirm target is not derived from features\")\n\nprint(\"\\n=== FEATURE NAMES ===\")\nprint(\"Review these carefully for potential leakage:\")\nfor col in X.columns:\n    print(f\"  - {col}\")\n\nprint(\"\\n‚ö†Ô∏è Red flags to watch for:\")\nprint(\"  - Features with 'target', 'outcome', 'label', 'price', 'value' in name\")\nprint(\"  - Perfect correlations (r > 0.99)\")\nprint(\"  - Features that would not be available at prediction time\")\nprint(\"  - ID columns that encode information about the target\")\n\n# Check for suspiciously high correlations\nhigh_corr = []\nfor col in X.columns:\n    if col in corr_matrix.columns and target_col in corr_matrix.columns:\n        corr_val = abs(corr_matrix.loc[col, target_col])\n        if corr_val > 0.95:\n            high_corr.append((col, corr_val))\n\nif high_corr:\n    print(\"\\n‚ö†Ô∏è WARNING: Suspiciously high correlations detected:\")\n    for col, corr_val in high_corr:\n        print(f\"  - {col}: r = {corr_val:.3f}\")\n        print(f\"    ‚Üí This could indicate leakage - investigate further!\")\nelse:\n    print(\"\\n‚úì No suspiciously high correlations (r > 0.95) detected\")\n    print(f\"\\nüí° Strongest correlation with target: {corr_matrix[target_col].drop(target_col).abs().idxmax()} (r = {corr_matrix[target_col].drop(target_col).abs().max():.3f})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù PAUSE-AND-DO Exercise 1 (10 minutes)\n",
    "\n",
    "**Task:** Complete the EDA checklist on the dataset and summarize 3 key findings.\n",
    "\n",
    "**Instructions:**\n",
    "1. Review all the EDA outputs above\n",
    "2. Identify 3 important patterns or insights\n",
    "3. Write your findings in the cell below\n",
    "\n",
    "**What to look for:**\n",
    "- Unusual distributions\n",
    "- Strong correlations\n",
    "- Potential data quality issues\n",
    "- Features that might be useful predictors\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### YOUR FINDINGS HERE:\n\n**Finding 1:**  \n[Your observation about the data - e.g., target distribution, feature ranges, correlations with price]\n\n**Finding 2:**  \n[Your second observation - e.g., relationship between income and house value, geographic patterns, outliers]\n\n**Finding 3:**  \n[Your third observation - e.g., feature correlations, missing values (there are none!), data quality]\n\n**Example observations to look for:**\n- Is MedInc strongly correlated with MedHouseVal?\n- Do coastal areas (certain Latitude/Longitude combinations) have higher prices?\n- Are there outliers in AveRooms or AveOccup that might need handling?\n- Is the target variable skewed?\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üìù PAUSE-AND-DO Exercise 2 (10 minutes)\n\n**Task:** Review the dataset and identify 3 potential leakage risks for real estate prediction.\n\n**Instructions:**\n1. Think about the business context (California housing price prediction)\n2. Review the feature names and their correlations with the target\n3. Identify 3 scenarios or features that could cause leakage\n4. Explain why each would be problematic\n\n**Consider:**\n- Would this feature be available at prediction time?\n- Could this feature encode information about the outcome?\n- Is this feature derived from or influenced by the target?\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### YOUR LEAKAGE ANALYSIS HERE:\n\n**Potential Risk 1:**  \n[Describe a feature or scenario that could cause leakage in a real estate prediction context]\n\n**Potential Risk 2:**  \n[Describe another leakage risk - think about temporal ordering, feature availability]\n\n**Potential Risk 3:**  \n[Describe a preprocessing or data handling issue that could cause train-test contamination]\n\n**Examples to consider for real estate prediction:**\n- Would \"recent sale price\" be available at prediction time? (NO - that's the target!)\n- Would \"current listing price\" be available? (Maybe, but that would leak target information)\n- Could \"neighborhood average sale price\" leak information about specific property values?\n- What about \"appraised value\"? (Definitely leakage - that's essentially the target)\n- Time-based features: Are we using data from the future to predict the past?\n- Preprocessing: Did we normalize using statistics from the full dataset before splitting?\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Wrap-Up: Key Takeaways\n",
    "\n",
    "### What We Learned Today:\n",
    "\n",
    "1. **Colab Workflow**: How to use Google Colab and responsible AI assistance with Gemini (Ask ‚Üí Verify ‚Üí Document)\n",
    "2. **Predictive Analytics Fundamentals**: Statistical learning framework, supervised vs. unsupervised, $Y = f(X) + \\epsilon$\n",
    "3. **End-to-End Workflow**: 9-step process from data loading to model deployment\n",
    "4. **Data Leakage**: Target leakage vs. train-test contamination, and how to prevent both\n",
    "5. **Train vs. Test Error**: Why training error is misleading and we need validation\n",
    "6. **Curse of Dimensionality**: Why more features isn't always better\n",
    "7. **Flexibility vs. Interpretability**: Choosing models based on goals and constraints\n",
    "8. **Bias-Variance Trade-off**: The fundamental concept underlying all model selection\n",
    "9. **EDA Essentials**: Data types, missingness, distributions, correlations\n",
    "10. **Proper Splitting**: Train/val/test splits with stratification and reproducible seeds\n",
    "\n",
    "### Next-Day Readiness:\n",
    "\n",
    "- ‚úì You can load and explore a dataset in Google Colab\n",
    "- ‚úì You can identify non-feature columns and potential leakage risks\n",
    "- ‚úì You can create proper train/val/test splits with reproducible seeds\n",
    "- ‚úì You understand why we split before preprocessing\n",
    "- ‚úì You're ready for Day 2: Preprocessing Pipelines\n",
    "\n",
    "### Remember:\n",
    "\n",
    "> **\"Split first, preprocess second, model third\"**  \n",
    "> This order prevents leakage and ensures valid evaluation.\n",
    "\n",
    "> **\"Understand the bias-variance trade-off\"**  \n",
    "> Every modeling decision balances these two sources of error.\n",
    "\n",
    "> **\"Use Gemini responsibly\"**  \n",
    "> Ask ‚Üí Verify ‚Üí Document. You own all code you submit.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Submission Instructions\n",
    "\n",
    "### To Submit This Notebook:\n",
    "\n",
    "1. **Complete all exercises**: Fill in both PAUSE-AND-DO exercise cells with your findings\n",
    "2. **Run All Cells**: Execute `Runtime ‚Üí Run all` to ensure everything works\n",
    "3. **Save a Copy**: `File ‚Üí Save a copy in Drive`\n",
    "4. **Get Shareable Link**: Click `Share` and set to \"Anyone with the link can view\"\n",
    "5. **Submit Link**: Paste the link in the Brightspace assignment\n",
    "\n",
    "### Before Submitting, Check:\n",
    "\n",
    "- [ ] All cells execute without errors\n",
    "- [ ] All outputs are visible\n",
    "- [ ] Both exercise responses are complete\n",
    "- [ ] Notebook is shared with correct permissions\n",
    "- [ ] You can explain every line of code you wrote\n",
    "\n",
    "### Next Step:\n",
    "\n",
    "Complete the **Day 1 Concept Quiz** in Brightspace (8.5 minutes, auto-graded)\n",
    "\n",
    "---\n",
    "\n",
    "## Bibliography\n",
    "\n",
    "- James, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). *An Introduction to Statistical Learning with Python* (ISLP), Chapter 2. Springer. https://www.statlearning.com/\n",
    "- Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning* (ESL), Chapter 2. Springer.\n",
    "- Kaggle. (2023). *Data Leakage Tutorial*. https://www.kaggle.com/code/alexisbcook/data-leakage\n",
    "- scikit-learn User Guide: [Cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html)\n",
    "- scikit-learn User Guide: [Common pitfalls and recommended practices](https://scikit-learn.org/stable/common_pitfalls.html)\n",
    "- Google Colab Documentation: https://colab.research.google.com/\n",
    "\n",
    "---\n",
    "\n",
    "**End of Day 1 Notebook** üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}