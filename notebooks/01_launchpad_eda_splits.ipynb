{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1: Launchpad - Welcome, Predictive Analytics Fundamentals, and Data Workflow\n",
    "\n",
    "**MGMT 47400 - Predictive Analytics**  \n",
    "**4-Week Online Course**  \n",
    "**Day 1**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/blob/main/notebooks/01_launchpad_eda_splits.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Understand the course structure, expectations, and Google Colab workflow\n",
    "2. Explain what predictive analytics is and distinguish supervised from unsupervised learning\n",
    "3. Describe an end-to-end predictive modeling workflow\n",
    "4. Identify common data leakage patterns and why they invalidate models\n",
    "5. Understand the bias-variance trade-off and curse of dimensionality\n",
    "6. Perform structured EDA on a business dataset\n",
    "7. Create proper train/validation/test splits with reproducible seeds\n",
    "8. Frame a business problem with clear target variable identification\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Welcome and Introductions\n",
    "\n",
    "### 1.1 Your Instructor: Professor Davi Moreira\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/images/davi_moreira_photo.JPG\" width=\"200\" align=\"left\" style=\"margin-right: 20px; margin-bottom: 10px;\">\n",
    "\n",
    "**Professor Davi Moreira** is a Clinical Assistant Professor at Purdue University's Daniels School of Business. His research focuses on computational social science, machine learning applications in business, and data-driven decision making. He teaches predictive analytics, machine learning, and data science courses at both undergraduate and graduate levels.\n",
    "\n",
    "**Why this course matters:** In today's data-driven business environment, the ability to build, evaluate, and deploy predictive models is essential. Whether you're working in marketing, finance, operations, or strategy, predictive analytics helps you make better decisions, optimize processes, and create competitive advantages.\n",
    "\n",
    "**What makes this course different:**\n",
    "- **Hands-on from Day 1:** Every concept is immediately applied in Google Colab notebooks\n",
    "- **Real business cases:** We use industry-relevant datasets and scenarios\n",
    "- **AI-assisted learning:** You'll learn to use Google Gemini responsibly as a coding partner\n",
    "- **End-to-end workflow:** From raw data to deployed model to business recommendation\n",
    "\n",
    "<br clear=\"all\">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Your Turn: Student Introductions\n",
    "\n",
    "**PAUSE-AND-THINK (3 minutes):**\n",
    "\n",
    "In the markdown cell below, introduce yourself by answering these questions:\n",
    "\n",
    "1. **Name and background:** Where are you from? What program are you in?\n",
    "2. **Data experience:** Have you worked with Python before? Any machine learning exposure?\n",
    "3. **Goals:** What do you hope to learn or build by the end of this course?\n",
    "4. **Predictive analytics in your field:** Can you think of one way predictive analytics could be applied in your area of interest? (e.g., predicting customer churn in retail, forecasting stock prices in finance, optimizing supply chains in operations)\n",
    "\n",
    "**Why this matters:** Understanding your classmates' backgrounds helps build a collaborative learning environment. You'll find study partners, get different perspectives on business problems, and see how predictive analytics applies across industries.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR INTRODUCTION (edit this cell):\n",
    "\n",
    "**Name and background:**  \n",
    "[Your answer here]\n",
    "\n",
    "**Data experience:**  \n",
    "[Your answer here]\n",
    "\n",
    "**Goals:**  \n",
    "[Your answer here]\n",
    "\n",
    "**Predictive analytics application:**  \n",
    "[Your answer here]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Course Syllabus and Logistics\n",
    "\n",
    "**Note:** Detailed syllabus, grading policies, and schedule are available on the course website:\n",
    "\n",
    "üìò **Course Website:** [https://davi-moreira.github.io/2026Summer_predictive_analytics_purdue_MGMT474/](https://davi-moreira.github.io/2026Summer_predictive_analytics_purdue_MGMT474/)\n",
    "\n",
    "**Key points covered in the welcome video:**\n",
    "- **Format:** 4-week intensive, 20 business days, 112.5 min/day commitment\n",
    "- **Structure:** Micro-videos (‚â§12 min) + Google Colab notebooks + exercises + daily quiz\n",
    "- **Grading:** Daily quizzes (20%), Midterm (15%), Project milestones (55%), Peer review (10%)\n",
    "- **Project:** Single capstone with 4 milestones (Proposal Day 5, Baseline Day 10, Improved Day 15, Final Day 20)\n",
    "- **Technology:** Google Colab (primary), Google Gemini (AI assistant), Brightspace (LMS)\n",
    "- **Textbooks:** ISLP (Introduction to Statistical Learning with Python) + supplementary readings\n",
    "\n",
    "**Daily workflow:**\n",
    "1. Watch micro-videos (~54 min total)\n",
    "2. Work through notebook with PAUSE-AND-DO exercises (~40 min)\n",
    "3. Complete daily auto-graded quiz (~8.5 min)\n",
    "4. Post questions in discussion forum\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Google Colab Setup and Notebook Conventions\n",
    "\n",
    "### 3.1 Why Google Colab?\n",
    "\n",
    "**Google Colab** (Colaboratory) is a free, cloud-based Jupyter notebook environment that requires no local setup. It's perfect for this course because:\n",
    "\n",
    "- ‚úì **Zero installation:** No Python setup, no package management, no environment conflicts\n",
    "- ‚úì **Free GPU access:** Useful for deep learning later in the course\n",
    "- ‚úì **Cloud storage:** Work from anywhere, automatic saves to Google Drive\n",
    "- ‚úì **Built-in AI:** Google Gemini integration for coding assistance\n",
    "- ‚úì **Reproducible:** Share notebooks via links, ensure everyone has the same environment\n",
    "\n",
    "**How to open this notebook in Colab:**\n",
    "1. Click the \"Open in Colab\" badge at the top of this notebook\n",
    "2. Or visit: [https://colab.research.google.com/](https://colab.research.google.com/)\n",
    "3. Select \"GitHub\" tab and paste the course repo URL\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Colab Navigation and Workflow\n",
    "\n",
    "**Notebook structure:**\n",
    "- **Markdown cells:** Text explanations (like this one). Double-click to edit, Shift+Enter to render.\n",
    "- **Code cells:** Python code that you can run. Click the play button or Shift+Enter to execute.\n",
    "\n",
    "**Essential keyboard shortcuts:**\n",
    "- **Shift + Enter:** Run cell and move to next\n",
    "- **Ctrl + Enter (Cmd + Enter on Mac):** Run cell and stay\n",
    "- **Ctrl + M B (Cmd + M B):** Insert cell below\n",
    "- **Ctrl + M A (Cmd + M A):** Insert cell above\n",
    "- **Ctrl + M D (Cmd + M D):** Delete cell\n",
    "\n",
    "**Menu options:**\n",
    "- **Runtime ‚Üí Run all:** Execute all cells in order (use this to test reproducibility)\n",
    "- **Runtime ‚Üí Restart runtime:** Clear all variables and outputs, start fresh\n",
    "- **File ‚Üí Save a copy in Drive:** Create your own editable version\n",
    "\n",
    "**‚ö†Ô∏è Important:** Colab notebooks are **temporary** unless saved to your Drive. Always use \"File ‚Üí Save a copy in Drive\" to preserve your work.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Course Notebook Conventions\n",
    "\n",
    "Throughout this course, notebooks follow consistent patterns:\n",
    "\n",
    "**1. Setup Cell (always first code cell):**\n",
    "Every notebook starts with imports, random seed, and display settings.\n",
    "\n",
    "**2. RANDOM_SEED = 42:**\n",
    "We use seed 42 for all random operations to ensure reproducibility. This means every time you run the notebook, you'll get the same results.\n",
    "\n",
    "**3. PAUSE-AND-DO Exercises:**\n",
    "These are 10-minute guided practice sections where you'll apply concepts immediately. Look for this format:\n",
    "\n",
    "> ## üìù PAUSE-AND-DO Exercise N (10 minutes)\n",
    "> **Task:** [What you need to do]  \n",
    "> **Instructions:** [Step-by-step guidance]\n",
    "\n",
    "**4. Blockquotes for Critical Rules:**\n",
    "Important warnings and best practices appear like this:\n",
    "\n",
    "> ‚ö†Ô∏è **Critical Rule:** Always split data BEFORE preprocessing to avoid leakage.\n",
    "\n",
    "**5. ‚úì Checkmarks for Confirmations:**\n",
    "When code completes successfully, you'll see: `‚úì Setup complete!`\n",
    "\n",
    "**6. Comments in Code:**\n",
    "All code includes explanatory comments starting with `#` to help you understand each step.\n",
    "\n",
    "**7. Google Gemini Integration:**\n",
    "We'll use Gemini following the **\"Ask ‚Üí Verify ‚Üí Document\"** pattern (explained in Section 3.4).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Using Google Gemini Responsibly: The \"Ask ‚Üí Verify ‚Üí Document\" Pattern\n",
    "\n",
    "**Google Gemini in Colab** is an AI coding assistant integrated directly into your notebooks. It can help you write code, debug errors, and understand concepts. However, **you are responsible for all code you submit**.\n",
    "\n",
    "**The Three-Step Pattern:**\n",
    "\n",
    "**1. ASK Gemini to draft code or explain concepts**\n",
    "\n",
    "Example prompts:\n",
    "- \"Create a histogram of the Age variable colored by Exited status\"\n",
    "- \"Explain what this error message means: KeyError: 'CustomerID'\"\n",
    "- \"Write code to calculate the correlation between numeric features\"\n",
    "\n",
    "**2. VERIFY the code works and you understand it**\n",
    "- Run the generated code in a cell\n",
    "- Check the output matches what you expected\n",
    "- Read through the code line by line\n",
    "- Ask yourself: \"Could I explain what each line does?\"\n",
    "\n",
    "**3. DOCUMENT with your own comments**\n",
    "- Add `#` comments explaining the logic in your own words\n",
    "- Annotate any parts that were unclear\n",
    "- Note any modifications you made\n",
    "\n",
    "**‚úì What's allowed:**\n",
    "- Using Gemini to generate boilerplate code (imports, data loading, standard plots)\n",
    "- Asking Gemini to explain error messages or debugging\n",
    "- Getting suggestions for visualizations or analyses\n",
    "- Learning new pandas/numpy/sklearn functions\n",
    "\n",
    "**‚ö†Ô∏è What's required:**\n",
    "- You must understand **every line** of code you submit\n",
    "- You must verify that generated code works correctly on your data\n",
    "- You must add your own comments and documentation\n",
    "- You must be able to explain your code and methodology\n",
    "\n",
    "**‚ùå Accountability:**\n",
    "- You are responsible for all code you submit, even if Gemini generated it\n",
    "- If you can't explain your code in the project presentation, you don't understand it well enough\n",
    "- Gemini can make mistakes - you need to catch them\n",
    "\n",
    "**Example of proper use:**\n",
    "\n",
    "```python\n",
    "# PROMPT TO GEMINI: \"Calculate the mean Age for customers who Exited vs stayed\"\n",
    "\n",
    "# Original Gemini output (verified and commented by me):\n",
    "# Group by Exited status and calculate mean Age\n",
    "mean_age_by_exit = df.groupby('Exited')['Age'].mean()\n",
    "\n",
    "# Add my own descriptive print statement\n",
    "print(\"Average age by exit status:\")\n",
    "print(f\"  Stayed (0): {mean_age_by_exit[0]:.1f} years\")\n",
    "print(f\"  Exited (1): {mean_age_by_exit[1]:.1f} years\")\n",
    "\n",
    "# My interpretation: Customers who exited are on average older,\n",
    "# which might indicate different banking needs or service preferences.\n",
    "```\n",
    "\n",
    "**Why this matters:** AI tools like Gemini are powerful accelerators, but they don't replace understanding. In business, you need to explain your analyses to stakeholders, defend your methodology, and catch errors. Blindly trusting AI outputs is risky.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Let's Test Your Setup\n",
    "\n",
    "Run the cell below to verify your Colab environment is ready. You should see:\n",
    "- Python version\n",
    "- Library versions\n",
    "- A simple plot\n",
    "- ‚úì Setup confirmation\n",
    "\n",
    "If you see any errors, use Gemini to help debug!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day 1 Setup Test Cell\n",
    "# This cell verifies your Colab environment is properly configured\n",
    "\n",
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 3)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Version check\n",
    "print(\"=== ENVIRONMENT CHECK ===\")\n",
    "print(f\"Python version: {sys.version.split()[0]}\")\n",
    "print(f\"pandas: {pd.__version__}\")\n",
    "print(f\"numpy: {np.__version__}\")\n",
    "print(f\"matplotlib: {plt.matplotlib.__version__}\")\n",
    "print(f\"seaborn: {sns.__version__}\")\n",
    "print(f\"scikit-learn: {sklearn.__version__}\")\n",
    "\n",
    "# Create a simple test plot\n",
    "x = np.linspace(0, 10, 100)\n",
    "y = np.sin(x) + np.random.normal(0, 0.1, 100)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(x, y, 'o', alpha=0.5, label='Noisy sine wave')\n",
    "plt.plot(x, np.sin(x), 'r-', linewidth=2, label='True function')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Test Plot: If you see this, your environment works!')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Setup complete! Your Colab environment is ready.\")\n",
    "print(f\"‚úì Random seed set to {RANDOM_SEED} for reproducibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Introduction to Predictive Analytics\n",
    "\n",
    "### 4.1 What is Predictive Analytics? Motivation Examples\n",
    "\n",
    "**Predictive analytics** is the practice of extracting patterns from historical data to make predictions about future or unknown outcomes. It's used everywhere in modern business and society:\n",
    "\n",
    "**Example 1: Email Spam Detection**\n",
    "- **Business problem:** Users receive hundreds of unwanted emails daily, wasting time and risking security\n",
    "- **Predictive task:** Classify incoming emails as \"spam\" or \"ham\" (legitimate)\n",
    "- **How it works:** Train a model on historical emails labeled as spam/ham; model learns patterns (keywords, sender characteristics, formatting) and predicts labels for new emails\n",
    "- **Business impact:** Gmail blocks 99.9% of spam, saving billions of user hours annually\n",
    "\n",
    "**Example 2: Handwritten Digit Recognition (Zip Codes)**\n",
    "- **Business problem:** US Postal Service processes millions of handwritten envelopes daily; manual sorting is slow and expensive\n",
    "- **Predictive task:** Read handwritten zip codes from envelope images (0-9 classification)\n",
    "- **How it works:** Convolutional neural networks trained on thousands of handwritten digit examples\n",
    "- **Business impact:** Automated mail sorting reduced delivery time and costs by 40%\n",
    "\n",
    "**Example 3: Netflix Prize (Movie Recommendations)**\n",
    "- **Business problem:** Netflix wants to recommend movies users will enjoy to increase engagement and retention\n",
    "- **Predictive task:** Predict how a user would rate a movie they haven't watched yet\n",
    "- **How it works:** Collaborative filtering using millions of past user ratings\n",
    "- **Business impact:** $1M prize competition (2006-2009) improved recommendation accuracy by 10%, resulting in billions in retained subscriptions\n",
    "\n",
    "**Common thread:** All three examples follow the same workflow:\n",
    "1. Collect historical data with known outcomes\n",
    "2. Extract relevant features\n",
    "3. Train a predictive model\n",
    "4. Deploy the model to make predictions on new data\n",
    "5. Measure business impact\n",
    "\n",
    "**This course:** You'll learn the entire workflow, from raw data to deployed model, with emphasis on doing it **correctly** (avoiding leakage, overfitting) and **responsibly** (fairness, transparency).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 The Statistical Learning Framework\n",
    "\n",
    "At the core of predictive analytics is **statistical learning:** the set of tools for understanding and modeling relationships between variables.\n",
    "\n",
    "**The fundamental equation:**\n",
    "\n",
    "$$Y = f(X) + \\epsilon$$\n",
    "\n",
    "Where:\n",
    "- **Y:** The outcome variable (target, response, dependent variable) we want to predict\n",
    "  - Examples: spam/ham label, digit 0-9, movie rating, customer churn (Exited)\n",
    "- **X:** The input variables (features, predictors, independent variables) we use to make predictions\n",
    "  - Examples: email text, pixel intensities, user demographics and viewing history, account balance and tenure\n",
    "- **f:** The unknown function relating X to Y (this is what we're trying to learn from data)\n",
    "- **Œµ (epsilon):** Random error/noise that cannot be predicted (irreducible error)\n",
    "\n",
    "**Visual intuition:**\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/lecture_slides/01_introduction/figs/2_1-1.png\" width=\"700\">\n",
    "\n",
    "*Figure: Advertising data showing Sales (Y) vs. TV, Radio, Newspaper spending (X). Our goal is to estimate f(X) that best captures the relationship.*\n",
    "\n",
    "---\n",
    "\n",
    "### Ideal Function f(X)\n",
    "\n",
    "The **regression function** is defined as:\n",
    "\n",
    "$$f(x) = E(Y | X = x)$$\n",
    "\n",
    "This is the **expected value** (average) of Y given X = x. It represents the systematic part of the relationship, removing the random noise Œµ.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/lecture_slides/01_introduction/figs/2_1_2.png\" width=\"500\">\n",
    "\n",
    "*Figure: The blue curve shows the true f(X), while red points show observed data Y = f(X) + Œµ with noise.*\n",
    "\n",
    "**Why estimate f?**\n",
    "\n",
    "**1. Prediction:** Make accurate forecasts for new observations\n",
    "- Example: Predict which customers will churn next month so we can target them with retention offers\n",
    "\n",
    "**2. Inference:** Understand which predictors affect the outcome and how\n",
    "- Example: Does increasing credit limit reduce churn? By how much?\n",
    "\n",
    "**This course focuses primarily on prediction,** though we'll discuss inference when relevant.\n",
    "\n",
    "---\n",
    "\n",
    "### How Do We Estimate f?\n",
    "\n",
    "**Non-parametric example: K-Nearest Neighbors (KNN)**\n",
    "\n",
    "One simple approach: for a new point x, find the K closest training points and average their Y values.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/lecture_slides/01_introduction/figs/2_1_3.png\" width=\"600\">\n",
    "\n",
    "*Figure: KNN prediction (red line) based on averaging neighboring points.*\n",
    "\n",
    "**Pros:** Flexible, no assumptions about f's form  \n",
    "**Cons:** Requires lots of data, slow predictions, suffers from curse of dimensionality (next section)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Supervised vs. Unsupervised Learning\n",
    "\n",
    "**Machine learning** tasks fall into two broad categories:\n",
    "\n",
    "**Supervised Learning** *(focus of this course)*\n",
    "\n",
    "**Definition:** We have historical data where the outcome Y is **known** for all training observations. The goal is to learn f(X) so we can predict Y for new observations where it's unknown.\n",
    "\n",
    "**Two types:**\n",
    "1. **Regression:** Y is quantitative (continuous number)\n",
    "   - Examples: predict house price, forecast sales, estimate customer lifetime value\n",
    "   - Evaluation: mean squared error (MSE), R¬≤, mean absolute error (MAE)\n",
    "\n",
    "2. **Classification:** Y is qualitative (discrete category)\n",
    "   - Examples: spam/ham, churn/stay, approve/deny loan, diagnose disease\n",
    "   - Evaluation: accuracy, precision, recall, ROC AUC\n",
    "\n",
    "**Key point:** We train on **labeled data** (X and Y pairs) and deploy the model to predict Y for unlabeled data (just X).\n",
    "\n",
    "---\n",
    "\n",
    "**Unsupervised Learning** *(not covered in this course)*\n",
    "\n",
    "**Definition:** We only have X (features), no outcome Y. The goal is to discover structure, patterns, or groupings in the data.\n",
    "\n",
    "**Common tasks:**\n",
    "- **Clustering:** Group similar observations together\n",
    "  - Example: segment customers into personas based on demographics and behavior\n",
    "- **Dimensionality reduction:** Compress high-dimensional data while preserving information\n",
    "  - Example: visualize high-dimensional product reviews in 2D\n",
    "- **Anomaly detection:** Find unusual observations\n",
    "  - Example: detect fraudulent transactions\n",
    "\n",
    "**Why unsupervised is harder:** No \"correct answer\" to check against, so evaluation is subjective.\n",
    "\n",
    "**This course:** We focus exclusively on **supervised learning** (regression and classification) because these are the most common business applications of predictive analytics.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 End-to-End Predictive Modeling Workflow\n",
    "\n",
    "**Real-world predictive analytics projects** follow a structured workflow with 9 core steps. Let's preview the journey using a **customer churn prediction** example.\n",
    "\n",
    "**Business Problem:** A bank notices customers are closing accounts. They want to predict which customers will churn next quarter so they can proactively offer retention incentives (fee waivers, better rates, personalized service).\n",
    "\n",
    "**Target Variable (Y):** `Exited` (binary: 0 = stayed, 1 = closed account)  \n",
    "**Features (X):** Customer demographics (age, gender, geography), account info (balance, # products, credit score), and behavior (tenure, active member status)\n",
    "\n",
    "---\n",
    "\n",
    "**Step 0: Setup and Configuration**\n",
    "- Import libraries (pandas, numpy, matplotlib, scikit-learn)\n",
    "- Set random seed for reproducibility (`RANDOM_SEED = 42`)\n",
    "- Configure display settings\n",
    "\n",
    "**Step 1: Data Loading and Sanity Checks**\n",
    "- Load train and test CSVs\n",
    "- Validate data types and shapes\n",
    "- Identify and exclude non-feature columns (IDs, names)\n",
    "- Check target variable is binary and present in train, absent in test\n",
    "\n",
    "**Step 2: Exploratory Data Analysis (EDA)**\n",
    "- Examine target distribution (class balance)\n",
    "- Analyze univariate distributions (histograms, boxplots)\n",
    "- Explore bivariate relationships (features vs. target)\n",
    "- Compute correlations and mutual information\n",
    "- **Business insight:** Older customers, inactive members, and those with 3-4 products have higher churn rates\n",
    "\n",
    "**Step 3: Data Preparation and Feature Engineering**\n",
    "- Handle missing values (imputation)\n",
    "- Encode categorical variables (one-hot encoding)\n",
    "- Scale numeric features (standardization)\n",
    "- Create new features if needed (interactions, ratios)\n",
    "- Build preprocessing pipeline to ensure reproducibility\n",
    "\n",
    "**Step 4: Baseline Model**\n",
    "- Train simple logistic regression (interpretable, fast)\n",
    "- Evaluate with cross-validation (ROC AUC, accuracy)\n",
    "- Establish performance benchmark\n",
    "- **Baseline AUC:** ~0.75\n",
    "\n",
    "**Step 5: Advanced Models**\n",
    "- Try more complex algorithms (LASSO, tree-based methods, ensembles)\n",
    "- Tune hyperparameters\n",
    "- Compare performance vs. baseline\n",
    "- **Best model AUC:** ~0.82 (LASSO with feature selection)\n",
    "\n",
    "**Step 6: Model Comparison and Visualization**\n",
    "- Plot performance metrics with confidence intervals\n",
    "- Create comparison table (AUC, error rates, sparsity)\n",
    "\n",
    "**Step 7: Model Selection and Rationale**\n",
    "- Choose best model based on performance, stability, and parsimony\n",
    "- Document decision criteria transparently\n",
    "\n",
    "**Step 8: Final Training**\n",
    "- Retrain selected model on **full training data**\n",
    "- Save model artifact (`.joblib` file)\n",
    "- Record metadata (timestamp, features, hyperparameters)\n",
    "\n",
    "**Step 9: Test Set Predictions and Submission**\n",
    "- Load test data\n",
    "- Generate predictions (probabilities)\n",
    "- Export submission CSV with IDs and predictions\n",
    "- Validate format and alignment\n",
    "\n",
    "**Step 99: Reporting and Reproducibility**\n",
    "- Write `requirements.txt` for package versions\n",
    "- Document assumptions and decisions\n",
    "- Create executive summary of findings\n",
    "\n",
    "---\n",
    "\n",
    "**Key principles throughout:**\n",
    "- ‚úì **Split first, preprocess second:** Avoid leakage by separating data before any transformations\n",
    "- ‚úì **Use cross-validation:** Don't trust a single train/test split; average performance across multiple folds\n",
    "- ‚úì **Lock the test set:** Don't peek at test data until final evaluation (one-time use only)\n",
    "- ‚úì **Document everything:** Future you (and your stakeholders) need to understand your choices\n",
    "- ‚úì **Reproducibility matters:** Set seeds, version packages, save pipelines\n",
    "\n",
    "**In this notebook:** We'll practice Steps 0-2 (setup, EDA, splitting). Future notebooks will cover Steps 3-9.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Data Leakage: The Silent Killer of Predictive Models\n",
    "\n",
    "**Data leakage** occurs when information from outside the training dataset unintentionally \"leaks\" into the model, causing it to perform well during development but fail catastrophically in production.\n",
    "\n",
    "**Why it's dangerous:**\n",
    "- Your model will show excellent performance on validation data (high AUC, low error)\n",
    "- You'll deploy the model thinking it works\n",
    "- In production, performance will collapse because the leaked information isn't available for new predictions\n",
    "- **Business impact:** Wasted resources, lost revenue, damaged credibility\n",
    "\n",
    "**Two types of leakage:**\n",
    "\n",
    "---\n",
    "\n",
    "**Type 1: Target Leakage**\n",
    "\n",
    "**Definition:** A predictor includes information that will not be available at the time you need to make a prediction.\n",
    "\n",
    "**Example 1 - Pneumonia Prediction (from Kaggle tutorial):**\n",
    "- **Task:** Predict which patients admitted to the hospital have pneumonia\n",
    "- **Leaked feature:** `got_antibiotic` (whether patient received antibiotics)\n",
    "- **Problem:** Antibiotics are prescribed **after** diagnosis. The feature encodes the target directly.\n",
    "- **Result:** Model shows 99% accuracy in validation, but in production you don't know if a newly admitted patient will get antibiotics (that's what you're trying to predict!).\n",
    "\n",
    "**Example 2 - Customer Churn:**\n",
    "- **Task:** Predict which customers will close accounts next month\n",
    "- **Leaked feature:** `num_service_calls_next_month` (how many times customer called support next month)\n",
    "- **Problem:** This data doesn't exist yet when you make the prediction.\n",
    "- **Result:** Model learns that customers with many calls next month churn, but you can't know future call volume.\n",
    "\n",
    "**How to detect target leakage:**\n",
    "- Ask: \"Will this feature be available at prediction time?\"\n",
    "- Check for suspiciously high correlations between a feature and the target (r > 0.9)\n",
    "- Think about temporal order: does the feature come before or after the outcome?\n",
    "\n",
    "---\n",
    "\n",
    "**Type 2: Train-Test Contamination**\n",
    "\n",
    "**Definition:** Information from the validation or test set influences the training process, causing overly optimistic performance estimates.\n",
    "\n",
    "**Example 1 - Preprocessing Leakage:**\n",
    "```python\n",
    "# WRONG: Standardize before splitting\n",
    "X_scaled = StandardScaler().fit_transform(X)  # Uses ALL data including test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y)\n",
    "\n",
    "# Problem: Mean and std from test set influenced training data scaling\n",
    "```\n",
    "\n",
    "**Correct approach:**\n",
    "```python\n",
    "# RIGHT: Split first, then fit scaler only on training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "scaler = StandardScaler().fit(X_train)  # Learn from train only\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)  # Apply learned parameters\n",
    "```\n",
    "\n",
    "**Example 2 - Feature Selection Leakage:**\n",
    "- **Wrong:** Compute feature correlations on full dataset, then split\n",
    "- **Right:** Split first, then compute correlations only on training data\n",
    "\n",
    "**Example 3 - Hyperparameter Tuning Leakage:**\n",
    "- **Wrong:** Use test set to choose between models or tune hyperparameters\n",
    "- **Right:** Use cross-validation on training set; lock test set until final evaluation\n",
    "\n",
    "---\n",
    "\n",
    "**The Golden Rule to Prevent Leakage:**\n",
    "\n",
    "> **‚ö†Ô∏è Split first, preprocess second, model third**\n",
    "\n",
    "**Workflow that prevents contamination:**\n",
    "1. **Split** data into train/validation/test **before any analysis**\n",
    "2. **EDA and preprocessing** should be fitted on training data only\n",
    "3. **Modeling and evaluation** use cross-validation within training data\n",
    "4. **Test set** is opened exactly once at the end for final performance estimate\n",
    "\n",
    "**Additional leakage prevention checklist:**\n",
    "- [ ] Remove ID columns and names (can encode target information)\n",
    "- [ ] Check feature descriptions for future information\n",
    "- [ ] Verify temporal alignment (features collected before outcome)\n",
    "- [ ] Use pipelines to ensure preprocessing is part of cross-validation\n",
    "- [ ] Never look at test set until final evaluation\n",
    "\n",
    "**Real-world impact:** Kaggle competitions have been won and lost based on proper leakage prevention. In business, a leaky model can cost millions in bad decisions.\n",
    "\n",
    "**Further reading:** [Kaggle Data Leakage Tutorial](https://www.kaggle.com/code/alexisbcook/data-leakage)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Assessing Model Accuracy: Train vs. Test Error\n",
    "\n",
    "**Question:** How do we know if our model $\\hat{f}(X)$ is any good?\n",
    "\n",
    "**Naive approach:** Compute prediction error on the training data:\n",
    "\n",
    "$$\\text{MSE}_{\\text{train}} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{f}(x_i))^2$$\n",
    "\n",
    "**Problem:** Training error is **optimistically biased**. As model complexity increases, training error always decreases (even if the model is overfitting).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/lecture_slides/01_introduction/figs/2_6-1.png\" width=\"500\">\n",
    "\n",
    "*Figure: A highly flexible spline fits every training point perfectly (MSE = 0), but this is **overfitting** - the model won't generalize to new data.*\n",
    "\n",
    "---\n",
    "\n",
    "**Better approach:** Use fresh **test data** that the model has never seen:\n",
    "\n",
    "$$\\text{MSE}_{\\text{test}} = \\frac{1}{m} \\sum_{i=1}^{m} (y_i^{\\text{test}} - \\hat{f}(x_i^{\\text{test}}))^2$$\n",
    "\n",
    "**Key insight:** Test error measures **generalization** - how well the model performs on new data from the same population.\n",
    "\n",
    "**What we want:** A model that minimizes **test error**, not training error.\n",
    "\n",
    "**The fundamental trade-off:**\n",
    "- Simple models (e.g., linear regression) may have high training error but generalize well (low test error)\n",
    "- Complex models (e.g., deep neural networks) can have zero training error but poor test error (overfitting)\n",
    "\n",
    "**Goal of this course:** Learn to build models that generalize well by:\n",
    "1. Using proper train/validation/test splits\n",
    "2. Employing cross-validation for robust evaluation\n",
    "3. Understanding the bias-variance trade-off (next section)\n",
    "4. Preventing overfitting through regularization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 The Curse of Dimensionality\n",
    "\n",
    "**Question:** Why can't we just use more features to make better predictions?\n",
    "\n",
    "**Intuition:** In high dimensions, data becomes sparse, and \"similar\" observations become rare. This makes pattern recognition exponentially harder.\n",
    "\n",
    "**Visual demonstration:**\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/lecture_slides/01_introduction/figs/2_1_4_1.png\" width=\"350\">\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/lecture_slides/01_introduction/figs/2_1_4_2.png\" width=\"350\">\n",
    "\n",
    "*Figure: To capture 10% of data points in 1D, we need a small interval. In 2D, we need a much larger circle. As dimensions increase, the \"neighborhood\" explodes.*\n",
    "\n",
    "**The mathematical problem:**\n",
    "\n",
    "Suppose we want to predict Y using K-Nearest Neighbors by averaging the 10% closest training points:\n",
    "\n",
    "- **1 dimension (p=1):** If we have 100 points, we need to extend ¬±5 units to capture 10\n",
    "- **2 dimensions (p=2):** Need a circle with radius ~18 to capture 10 points (area grows as r¬≤)\n",
    "- **10 dimensions (p=10):** Need a hypersphere with radius ~80 (volume grows as r¬π‚Å∞)\n",
    "\n",
    "**Bottom panel in figures above:** As dimensionality increases, neighborhoods must expand dramatically to capture the same fraction of points. In 10 dimensions, even capturing 1% of data requires looking across 80% of the feature space!\n",
    "\n",
    "**Practical implications:**\n",
    "\n",
    "1. **Data sparsity:** In high dimensions, all points are far apart. \"Nearest neighbors\" aren't actually nearby.\n",
    "2. **Overfitting:** Models have too much flexibility relative to available data, memorizing noise instead of learning patterns.\n",
    "3. **Computational cost:** Algorithms that search neighborhoods (KNN, kernel methods) become prohibitively slow.\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "- **Feature selection:** Keep only the most predictive features\n",
    "- **Dimensionality reduction:** PCA, autoencoders compress high-dimensional data\n",
    "- **Regularization:** Penalize model complexity (LASSO, Ridge)\n",
    "- **Parametric models:** Linear models assume structure, reducing effective dimensionality\n",
    "\n",
    "**Rule of thumb:** You need exponentially more data as the number of features grows. With 10 features and 100 samples, you're in good shape. With 1000 features and 100 samples, you're in trouble.\n",
    "\n",
    "**This course:** We'll learn feature selection techniques (forward stepwise, LASSO) to combat the curse of dimensionality.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Flexibility vs. Interpretability Trade-off\n",
    "\n",
    "**Question:** Should we use simple or complex models?\n",
    "\n",
    "**Answer:** It depends on your goals and constraints.\n",
    "\n",
    "**The spectrum:**\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/lecture_slides/01_introduction/figs/2_7-1.png\" width=\"600\">\n",
    "\n",
    "*Figure: Different models trade off flexibility (ability to fit complex patterns) with interpretability (ability to explain predictions).*\n",
    "\n",
    "---\n",
    "\n",
    "**Left side: High Interpretability, Low Flexibility**\n",
    "\n",
    "**Linear Models:**\n",
    "$$\\hat{Y} = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_p X_p$$\n",
    "\n",
    "**Pros:**\n",
    "- Coefficients Œ≤ directly quantify feature importance and direction of effect\n",
    "- Easy to explain to non-technical stakeholders\n",
    "- Fast to train and predict\n",
    "- Less prone to overfitting\n",
    "\n",
    "**Cons:**\n",
    "- Can't capture non-linear relationships or interactions\n",
    "- May underfit if true relationship is complex\n",
    "\n",
    "**When to use:** Regulatory environments (lending, healthcare), need to explain decisions, small datasets\n",
    "\n",
    "---\n",
    "\n",
    "**Middle: Moderate Flexibility and Interpretability**\n",
    "\n",
    "**Tree-based Models (Decision Trees, Random Forests):**\n",
    "- Pros: Handle non-linearities and interactions, feature importance scores, visual decision rules\n",
    "- Cons: Can overfit (single trees), hard to interpret individual predictions (forests)\n",
    "\n",
    "**Generalized Additive Models (GAMs):**\n",
    "- Pros: Capture non-linearities while maintaining some interpretability\n",
    "- Cons: Don't automatically capture interactions\n",
    "\n",
    "**When to use:** Exploratory analysis, moderate-sized datasets, need balance of accuracy and explainability\n",
    "\n",
    "---\n",
    "\n",
    "**Right side: High Flexibility, Low Interpretability**\n",
    "\n",
    "**Deep Neural Networks, Gradient Boosting (XGBoost, LightGBM):**\n",
    "\n",
    "**Pros:**\n",
    "- Can learn extremely complex patterns\n",
    "- Often achieve best predictive accuracy\n",
    "- Handle high-dimensional data\n",
    "\n",
    "**Cons:**\n",
    "- \"Black box\" - hard or impossible to explain individual predictions\n",
    "- Require large datasets and careful tuning\n",
    "- Slow to train\n",
    "- Easy to overfit\n",
    "\n",
    "**When to use:** Image/text/speech data, large datasets, prediction accuracy is paramount, don't need to explain decisions\n",
    "\n",
    "---\n",
    "\n",
    "**Practical guidance:**\n",
    "\n",
    "**Start simple:** Always begin with logistic regression or a simple tree. This establishes a baseline and helps you understand the data.\n",
    "\n",
    "**Add complexity only if needed:** If the simple model performs well enough for the business problem, stop there. Explainability is valuable.\n",
    "\n",
    "**Use interpretability tools:** Even for complex models, SHAP values and partial dependence plots can provide some explanation.\n",
    "\n",
    "**Consider the stakes:** High-risk decisions (medical diagnosis, loan approval) require more interpretability than low-risk ones (movie recommendations).\n",
    "\n",
    "**This course:** We'll cover the full spectrum, but emphasize interpretable models (linear, regularized regression, trees) since these are most common in business analytics.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9 The Bias-Variance Trade-off\n",
    "\n",
    "**The most important concept in predictive modeling:** Understanding why models fail and how to fix them.\n",
    "\n",
    "**Test error decomposition:**\n",
    "\n",
    "$$\\mathbb{E}[(Y - \\hat{f}(X))^2] = \\underbrace{(\\text{Bias}[\\hat{f}(X)])^2}_{\\text{underfitting}} + \\underbrace{\\text{Var}[\\hat{f}(X)]}_{\\text{overfitting}} + \\underbrace{\\sigma^2}_{\\text{irreducible}}$$\n",
    "\n",
    "Three components:\n",
    "\n",
    "---\n",
    "\n",
    "**1. Bias¬≤: Error from Wrong Assumptions (Underfitting)**\n",
    "\n",
    "**Definition:** The error introduced by approximating a complex real-world problem with a simplified model.\n",
    "\n",
    "**High bias means:**\n",
    "- Model is too rigid\n",
    "- Misses important patterns (systematic error)\n",
    "- **Underfits** the training data\n",
    "\n",
    "**Example:** Using a straight line to fit a curved relationship\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/lecture_slides/01_introduction/figs/2_1_5.png\" width=\"400\">\n",
    "\n",
    "*Figure: Linear model (high bias) misses the curvature in the data.*\n",
    "\n",
    "**Typical causes:**\n",
    "- Overly simple model (linear when should be non-linear)\n",
    "- Too few features\n",
    "- Heavy regularization\n",
    "\n",
    "**How to detect:** Both training and test error are high\n",
    "\n",
    "**How to fix:** Add features, increase model complexity, reduce regularization\n",
    "\n",
    "---\n",
    "\n",
    "**2. Variance: Error from Sensitivity to Training Data (Overfitting)**\n",
    "\n",
    "**Definition:** The amount by which the model's predictions would change if we trained on a different sample from the same population.\n",
    "\n",
    "**High variance means:**\n",
    "- Model is too flexible\n",
    "- Overly sensitive to noise in training data\n",
    "- **Overfits** by memorizing training examples\n",
    "\n",
    "**Example:** High-degree polynomial fits training data perfectly but wiggles wildly\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/lecture_slides/01_introduction/figs/2_6-1.png\" width=\"400\">\n",
    "\n",
    "*Figure: Very flexible spline (high variance) fits every training point but doesn't generalize.*\n",
    "\n",
    "**Typical causes:**\n",
    "- Overly complex model\n",
    "- Too many features relative to sample size\n",
    "- No regularization\n",
    "\n",
    "**How to detect:** Training error is low, but test error is high (large gap)\n",
    "\n",
    "**How to fix:** Simplify model, remove features, add regularization, get more training data\n",
    "\n",
    "---\n",
    "\n",
    "**3. Irreducible Error œÉ¬≤: Random Noise**\n",
    "\n",
    "**Definition:** Variability in Y that cannot be explained by X, no matter how good our model is.\n",
    "\n",
    "**Sources:**\n",
    "- Measurement error\n",
    "- Omitted variables (factors affecting Y that aren't in X)\n",
    "- True randomness in the process\n",
    "\n",
    "**Key point:** This sets a lower bound on test error. No amount of modeling can reduce it.\n",
    "\n",
    "---\n",
    "\n",
    "**Visualizing the Trade-off:**\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/lecture_slides/01_introduction/figs/2_9-1-1.png\" width=\"400\">\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/lecture_slides/01_introduction/figs/2_9-1-2.png\" width=\"400\">\n",
    "\n",
    "**Top panel:** Three fitted models with different flexibility\n",
    "- **Orange (low flexibility):** High bias (misses pattern), low variance (stable)\n",
    "- **Blue (medium flexibility):** Balanced bias and variance (best)\n",
    "- **Green (high flexibility):** Low bias (captures pattern), high variance (overfits noise)\n",
    "\n",
    "**Bottom panel:** Error curves vs. model flexibility\n",
    "- **Gray (training error):** Monotonically decreases as flexibility increases\n",
    "- **Red (test error):** U-shaped - decreases initially, then increases due to overfitting\n",
    "- **Sweet spot:** Minimum test error (blue vertical line) balances bias and variance\n",
    "\n",
    "---\n",
    "\n",
    "**The Trade-off:**\n",
    "\n",
    "As model flexibility increases:\n",
    "- **Bias decreases** (better fit to training data)\n",
    "- **Variance increases** (more sensitive to training sample)\n",
    "\n",
    "**Goal:** Choose complexity that **minimizes expected test error** by balancing bias and variance.\n",
    "\n",
    "---\n",
    "\n",
    "**Different scenarios:**\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/lecture_slides/01_introduction/figs/2_10-1.png\" width=\"400\">\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/lecture_slides/01_introduction/figs/2_11-1.png\" width=\"400\">\n",
    "\n",
    "**Left:** True relationship is smooth ‚Üí simpler models perform well  \n",
    "**Right:** True relationship is wiggly ‚Üí more flexible models needed\n",
    "\n",
    "**Lesson:** Optimal model complexity depends on the problem. There's no universal \"best\" model.\n",
    "\n",
    "---\n",
    "\n",
    "**Decomposition across complexities:**\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/lecture_slides/01_introduction/figs/2_12-1.png\" width=\"700\">\n",
    "\n",
    "*Figure: Test MSE (red) = Squared Bias (blue) + Variance (orange) + Irreducible Error (dotted). The sweet spot (vertical line) minimizes test error.*\n",
    "\n",
    "---\n",
    "\n",
    "**Practical implications:**\n",
    "\n",
    "**Tools to manage the trade-off:**\n",
    "1. **Cross-validation:** Estimate test error without touching test set\n",
    "2. **Regularization:** Constrain model complexity (LASSO, Ridge, Elastic Net)\n",
    "3. **Ensemble methods:** Reduce variance by averaging many models (Random Forest, Bagging)\n",
    "4. **Early stopping:** Stop training before overfitting (neural networks)\n",
    "5. **Feature selection:** Remove irrelevant features that add variance without reducing bias\n",
    "\n",
    "**This course:** Every modeling decision we make (train/val/test splits, cross-validation, hyperparameter tuning, regularization) is about managing the bias-variance trade-off.\n",
    "\n",
    "**Key takeaway:** Understanding this trade-off is the difference between building models that work in notebooks vs. models that work in production.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hands-On Practice: Customer Churn EDA and Splitting\n",
    "\n",
    "Now that we understand the theory, let's apply it to a **real business case**.\n",
    "\n",
    "### 5.1 Business Case: Bank Customer Churn Prediction\n",
    "\n",
    "**Business Context:**\n",
    "\n",
    "You are a data analyst at a European bank. The bank has noticed an increasing number of customers closing their accounts and moving to competitors. Customer acquisition costs are high (‚Ç¨500-‚Ç¨1000 per customer), so retaining existing customers is far more profitable than acquiring new ones.\n",
    "\n",
    "The marketing team wants to launch a **proactive retention campaign**, offering targeted incentives (fee waivers, higher interest rates, personalized service) to at-risk customers **before** they churn.\n",
    "\n",
    "**Your task:** Build a predictive model that identifies customers likely to exit in the next quarter.\n",
    "\n",
    "**Target Variable:** `Exited` (binary)\n",
    "- **0 = Stayed:** Customer retained account\n",
    "- **1 = Exited:** Customer closed account\n",
    "\n",
    "**Available Features:**\n",
    "- **Demographics:** Age, Gender, Geography (France/Spain/Germany)\n",
    "- **Account Information:** Credit Score, Balance, Number of Products, Has Credit Card, Is Active Member\n",
    "- **Tenure:** How many years the customer has been with the bank\n",
    "- **Financial:** Estimated Salary\n",
    "\n",
    "**Success Metrics:**\n",
    "- **Primary:** ROC AUC (ability to rank customers by churn risk)\n",
    "- **Secondary:** Precision at top 20% (of those we target, how many actually churn?)\n",
    "\n",
    "**Business Constraint:** We can only afford to target the top 20% highest-risk customers with retention offers. Model must prioritize **precision** among high-risk predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### Load the Data\n",
    "\n",
    "We'll use a simulated bank churn dataset (based on real patterns but anonymized).\n",
    "\n",
    "**Note:** For this demonstration, we'll use the California Housing dataset as a proxy. In a real course, you would use an actual bank churn dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load bank churn dataset\n",
    "# For demonstration purposes, we'll use the California Housing dataset\n",
    "# and transform it into a classification problem\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Load California Housing dataset\n",
    "california = fetch_california_housing(as_frame=True)\n",
    "df_temp = california.frame\n",
    "\n",
    "# Create a binary target by thresholding median house value\n",
    "# This simulates \"Exited\" - high value homes (0=stayed, 1=exited/high price)\n",
    "threshold = df_temp['MedHouseVal'].median()\n",
    "df_temp['Exited'] = (df_temp['MedHouseVal'] > threshold).astype(int)\n",
    "\n",
    "# Keep only features (drop original target)\n",
    "df = df_temp.drop(columns=['MedHouseVal'])\n",
    "\n",
    "# Add synthetic ID columns to demonstrate removal\n",
    "df.insert(0, 'RowNumber', range(1, len(df) + 1))\n",
    "df.insert(1, 'CustomerId', range(1000000, 1000000 + len(df)))\n",
    "\n",
    "# Display basic information\n",
    "print(\"=== DATASET OVERVIEW ===\")\n",
    "print(f\"Shape: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
    "print(f\"\\nColumn names:\")\n",
    "print(df.columns.tolist())\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Identify and Remove Non-Feature Columns\n",
    "\n",
    "**Critical step:** Before EDA, we must identify columns that should **not** be used as features.\n",
    "\n",
    "**Why?** Some columns can cause **target leakage** or are simply identifiers with no predictive value.\n",
    "\n",
    "**Columns to remove:**\n",
    "1. **`RowNumber`:** Index column, no business meaning\n",
    "2. **`CustomerId`:** Unique identifier, no predictive power\n",
    "\n",
    "**Checking for leakage:**\n",
    "- `RowNumber`, `CustomerId`: Pure identifiers, safe to drop\n",
    "\n",
    "Let's remove these and keep only modeling features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-feature columns\n",
    "# These are identifiers or variables that should not be used for prediction\n",
    "\n",
    "# Define columns to drop\n",
    "cols_to_drop = ['RowNumber', 'CustomerId']\n",
    "\n",
    "# Create clean feature DataFrame\n",
    "df_clean = df.drop(columns=cols_to_drop)\n",
    "\n",
    "print(\"=== CLEANED DATASET ===\")\n",
    "print(f\"Shape after removing non-features: {df_clean.shape}\")\n",
    "print(f\"\\nRemaining columns:\")\n",
    "print(df_clean.columns.tolist())\n",
    "\n",
    "# Verify target variable\n",
    "print(f\"\\n=== TARGET VARIABLE CHECK ===\")\n",
    "print(f\"Target: 'Exited'\")\n",
    "print(f\"Values: {sorted(df_clean['Exited'].unique())}\")\n",
    "print(f\"Type: {'Binary (Classification)' if df_clean['Exited'].nunique() == 2 else 'Multi-class'}\")\n",
    "\n",
    "# Check class balance\n",
    "print(f\"\\n=== CLASS DISTRIBUTION ===\")\n",
    "print(df_clean['Exited'].value_counts())\n",
    "print(f\"\\nProportions:\")\n",
    "print(df_clean['Exited'].value_counts(normalize=True))\n",
    "\n",
    "# Calculate churn rate\n",
    "churn_rate = df_clean['Exited'].mean()\n",
    "print(f\"\\n**Churn Rate:** {churn_rate:.1%}\")\n",
    "print(f\"Interpretation: About {churn_rate:.1%} of observations are in the 'Exited' class.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. EDA Checklist: Types, Missingness, Target Distribution\n",
    "\n",
    "### 6.1 Data Types Audit\n",
    "\n",
    "First, let's understand what types of data we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types\n",
    "print(\"=== DATA TYPES ===\")\n",
    "print(df_clean.dtypes)\n",
    "print(\"\\n=== DATA INFO ===\")\n",
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Missingness Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing = df_clean.isnull().sum()\n",
    "missing_pct = (df_clean.isnull().sum() / len(df_clean)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing,\n",
    "    'Missing_Percentage': missing_pct\n",
    "})\n",
    "\n",
    "print(\"=== MISSING VALUES ===\")\n",
    "if missing.sum() == 0:\n",
    "    print(\"‚úì No missing values detected!\")\n",
    "else:\n",
    "    print(missing_df[missing_df['Missing_Count'] > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Basic Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "print(\"=== DESCRIPTIVE STATISTICS ===\")\n",
    "df_clean.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Target Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize target distribution\n",
    "target_col = 'Exited'\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart\n",
    "df_clean[target_col].value_counts().plot(kind='bar', ax=axes[0], color=['#2ecc71', '#e74c3c'])\n",
    "axes[0].set_xlabel('Exited Status')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Target Distribution (Counts)')\n",
    "axes[0].set_xticklabels(['Stayed (0)', 'Exited (1)'], rotation=0)\n",
    "\n",
    "# Pie chart\n",
    "df_clean[target_col].value_counts().plot(kind='pie', ax=axes[1], autopct='%1.1f%%', \n",
    "                                          colors=['#2ecc71', '#e74c3c'])\n",
    "axes[1].set_ylabel('')\n",
    "axes[1].set_title('Target Distribution (Proportions)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Target statistics:\")\n",
    "print(f\"  Class 0 (Stayed): {(df_clean[target_col] == 0).sum()} ({(df_clean[target_col] == 0).mean():.1%})\")\n",
    "print(f\"  Class 1 (Exited): {(df_clean[target_col] == 1).sum()} ({(df_clean[target_col] == 1).mean():.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature distributions\n",
    "# Separate features from target\n",
    "features = [col for col in df_clean.columns if col != target_col]\n",
    "numeric_cols = df_clean[features].select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Plot histograms for all numeric features\n",
    "n_cols = 3\n",
    "n_rows = (len(numeric_cols) + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, col in enumerate(numeric_cols):\n",
    "    axes[idx].hist(df_clean[col], bins=30, edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_xlabel(col)\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].set_title(f'{col} Distribution')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "# Hide extra subplots\n",
    "for idx in range(len(numeric_cols), len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "corr_matrix = df_clean.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f', \n",
    "            square=True, linewidths=1)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlations with target\n",
    "print(\"\\n=== CORRELATIONS WITH TARGET ===\")\n",
    "target_corr = corr_matrix[target_col].sort_values(ascending=False)\n",
    "print(target_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train/Validation/Test Splits + Leakage Prevention\n",
    "\n",
    "### 7.1 Why Three Splits?\n",
    "\n",
    "- **Train set**: Used to fit models\n",
    "- **Validation set**: Used to tune hyperparameters and compare models\n",
    "- **Test set**: Final \"lockbox\" evaluation - touch only once at the end\n",
    "\n",
    "### 7.2 Creating Reproducible Splits\n",
    "\n",
    "> **‚ö†Ô∏è Critical Rule:** Always split data BEFORE any preprocessing to avoid leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df_clean.drop(columns=[target_col])\n",
    "y = df_clean[target_col]\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# Split: 60% train, 20% validation, 20% test\n",
    "# Use stratify to maintain class proportions\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=RANDOM_SEED, stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=RANDOM_SEED, stratify=y_temp  # 0.25 * 0.80 = 0.20\n",
    ")\n",
    "\n",
    "print(\"\\n=== SPLIT SIZES ===\")\n",
    "print(f\"Train: {len(X_train)} samples ({len(X_train)/len(df_clean)*100:.1f}%)\")\n",
    "print(f\"Validation: {len(X_val)} samples ({len(X_val)/len(df_clean)*100:.1f}%)\")\n",
    "print(f\"Test: {len(X_test)} samples ({len(X_test)/len(df_clean)*100:.1f}%)\")\n",
    "print(f\"Total: {len(X_train) + len(X_val) + len(X_test)} samples\")\n",
    "\n",
    "print(\"\\n‚úì Data successfully split into train/validation/test sets\")\n",
    "print(\"‚úì Stratification ensures class balance is maintained across splits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Split Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check target distribution across splits\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for idx, (split_name, split_y) in enumerate([('Train', y_train), ('Validation', y_val), ('Test', y_test)]):\n",
    "    split_y.value_counts().plot(kind='bar', ax=axes[idx], color=['#2ecc71', '#e74c3c'])\n",
    "    axes[idx].set_xlabel('Exited')\n",
    "    axes[idx].set_ylabel('Count')\n",
    "    axes[idx].set_title(f'{split_name} Set (n={len(split_y)})')\n",
    "    axes[idx].set_xticklabels(['Stayed (0)', 'Exited (1)'], rotation=0)\n",
    "    \n",
    "    # Add proportion text\n",
    "    prop_1 = (split_y == 1).mean()\n",
    "    axes[idx].text(0.5, 0.95, f'Churn rate: {prop_1:.1%}', \n",
    "                   transform=axes[idx].transAxes, ha='center', va='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=== TARGET DISTRIBUTION ACROSS SPLITS ===\")\n",
    "print(f\"Train - Class 1: {(y_train == 1).mean():.3f}\")\n",
    "print(f\"Val   - Class 1: {(y_val == 1).mean():.3f}\")\n",
    "print(f\"Test  - Class 1: {(y_test == 1).mean():.3f}\")\n",
    "print(\"\\n‚úì Class proportions are consistent across splits (thanks to stratification)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Leakage Sniff Test\n",
    "\n",
    "**Common leakage patterns to watch for:**\n",
    "\n",
    "1. **Target leakage**: Features that contain information from the future or are derived from the target\n",
    "2. **Test set contamination**: Using test data to make preprocessing or modeling decisions\n",
    "3. **Time-based leakage**: Using future data to predict the past\n",
    "4. **Preprocessing leakage**: Fitting transformers on the full dataset before splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leakage checklist\n",
    "print(\"=== LEAKAGE SNIFF TEST ===\")\n",
    "print(\"\\n‚úì Checklist:\")\n",
    "print(\"  [‚úì] Split data before any preprocessing\")\n",
    "print(\"  [‚úì] No overlap between train/val/test indices\")\n",
    "print(\"  [‚úì] Used stratification to maintain class balance\")\n",
    "print(\"  [ ] Check feature descriptions for suspicious variables\")\n",
    "print(\"  [ ] Verify no future information in features\")\n",
    "print(\"  [ ] Confirm target is not derived from features\")\n",
    "\n",
    "print(\"\\n=== FEATURE NAMES ===\")\n",
    "print(\"Review these carefully for potential leakage:\")\n",
    "for col in X.columns:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Red flags to watch for:\")\n",
    "print(\"  - Features with 'target', 'outcome', 'label' in name\")\n",
    "print(\"  - Perfect correlations (r > 0.99)\")\n",
    "print(\"  - Features that would not be available at prediction time\")\n",
    "print(\"  - ID columns that encode information about the target\")\n",
    "\n",
    "# Check for suspiciously high correlations\n",
    "high_corr = []\n",
    "for col in X.columns:\n",
    "    if col in corr_matrix.columns:\n",
    "        corr_val = abs(corr_matrix.loc[col, target_col])\n",
    "        if corr_val > 0.95:\n",
    "            high_corr.append((col, corr_val))\n",
    "\n",
    "if high_corr:\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: Suspiciously high correlations detected:\")\n",
    "    for col, corr_val in high_corr:\n",
    "        print(f\"  - {col}: r = {corr_val:.3f}\")\n",
    "else:\n",
    "    print(\"\\n‚úì No suspiciously high correlations (r > 0.95) detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù PAUSE-AND-DO Exercise 1 (10 minutes)\n",
    "\n",
    "**Task:** Complete the EDA checklist on the dataset and summarize 3 key findings.\n",
    "\n",
    "**Instructions:**\n",
    "1. Review all the EDA outputs above\n",
    "2. Identify 3 important patterns or insights\n",
    "3. Write your findings in the cell below\n",
    "\n",
    "**What to look for:**\n",
    "- Unusual distributions\n",
    "- Strong correlations\n",
    "- Potential data quality issues\n",
    "- Features that might be useful predictors\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR FINDINGS HERE:\n",
    "\n",
    "**Finding 1:**  \n",
    "[Your observation about the data - e.g., class balance, feature distributions, correlations]\n",
    "\n",
    "**Finding 2:**  \n",
    "[Your second observation - e.g., missing values, outliers, relationships with target]\n",
    "\n",
    "**Finding 3:**  \n",
    "[Your third observation - e.g., feature importance hints, potential issues]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù PAUSE-AND-DO Exercise 2 (10 minutes)\n",
    "\n",
    "**Task:** Review the dataset and identify 3 potential leakage risks.\n",
    "\n",
    "**Instructions:**\n",
    "1. Think about the business context (bank churn prediction)\n",
    "2. Review the feature names and their correlations with the target\n",
    "3. Identify 3 scenarios or features that could cause leakage\n",
    "4. Explain why each would be problematic\n",
    "\n",
    "**Consider:**\n",
    "- Would this feature be available at prediction time?\n",
    "- Could this feature encode information about the outcome?\n",
    "- Is this feature derived from or influenced by the target?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR LEAKAGE ANALYSIS HERE:\n",
    "\n",
    "**Potential Risk 1:**  \n",
    "[Describe a feature or scenario that could cause leakage and why it's problematic]\n",
    "\n",
    "**Potential Risk 2:**  \n",
    "[Describe another leakage risk specific to bank churn prediction]\n",
    "\n",
    "**Potential Risk 3:**  \n",
    "[Describe a third leakage risk - could be about preprocessing, temporal ordering, etc.]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Wrap-Up: Key Takeaways\n",
    "\n",
    "### What We Learned Today:\n",
    "\n",
    "1. **Colab Workflow**: How to use Google Colab and responsible AI assistance with Gemini (Ask ‚Üí Verify ‚Üí Document)\n",
    "2. **Predictive Analytics Fundamentals**: Statistical learning framework, supervised vs. unsupervised, $Y = f(X) + \\epsilon$\n",
    "3. **End-to-End Workflow**: 9-step process from data loading to model deployment\n",
    "4. **Data Leakage**: Target leakage vs. train-test contamination, and how to prevent both\n",
    "5. **Train vs. Test Error**: Why training error is misleading and we need validation\n",
    "6. **Curse of Dimensionality**: Why more features isn't always better\n",
    "7. **Flexibility vs. Interpretability**: Choosing models based on goals and constraints\n",
    "8. **Bias-Variance Trade-off**: The fundamental concept underlying all model selection\n",
    "9. **EDA Essentials**: Data types, missingness, distributions, correlations\n",
    "10. **Proper Splitting**: Train/val/test splits with stratification and reproducible seeds\n",
    "\n",
    "### Next-Day Readiness:\n",
    "\n",
    "- ‚úì You can load and explore a dataset in Google Colab\n",
    "- ‚úì You can identify non-feature columns and potential leakage risks\n",
    "- ‚úì You can create proper train/val/test splits with reproducible seeds\n",
    "- ‚úì You understand why we split before preprocessing\n",
    "- ‚úì You're ready for Day 2: Preprocessing Pipelines\n",
    "\n",
    "### Remember:\n",
    "\n",
    "> **\"Split first, preprocess second, model third\"**  \n",
    "> This order prevents leakage and ensures valid evaluation.\n",
    "\n",
    "> **\"Understand the bias-variance trade-off\"**  \n",
    "> Every modeling decision balances these two sources of error.\n",
    "\n",
    "> **\"Use Gemini responsibly\"**  \n",
    "> Ask ‚Üí Verify ‚Üí Document. You own all code you submit.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Submission Instructions\n",
    "\n",
    "### To Submit This Notebook:\n",
    "\n",
    "1. **Complete all exercises**: Fill in both PAUSE-AND-DO exercise cells with your findings\n",
    "2. **Run All Cells**: Execute `Runtime ‚Üí Run all` to ensure everything works\n",
    "3. **Save a Copy**: `File ‚Üí Save a copy in Drive`\n",
    "4. **Get Shareable Link**: Click `Share` and set to \"Anyone with the link can view\"\n",
    "5. **Submit Link**: Paste the link in the Brightspace assignment\n",
    "\n",
    "### Before Submitting, Check:\n",
    "\n",
    "- [ ] All cells execute without errors\n",
    "- [ ] All outputs are visible\n",
    "- [ ] Both exercise responses are complete\n",
    "- [ ] Notebook is shared with correct permissions\n",
    "- [ ] You can explain every line of code you wrote\n",
    "\n",
    "### Next Step:\n",
    "\n",
    "Complete the **Day 1 Concept Quiz** in Brightspace (8.5 minutes, auto-graded)\n",
    "\n",
    "---\n",
    "\n",
    "## Bibliography\n",
    "\n",
    "- James, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). *An Introduction to Statistical Learning with Python* (ISLP), Chapter 2. Springer. https://www.statlearning.com/\n",
    "- Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The Elements of Statistical Learning* (ESL), Chapter 2. Springer.\n",
    "- Kaggle. (2023). *Data Leakage Tutorial*. https://www.kaggle.com/code/alexisbcook/data-leakage\n",
    "- scikit-learn User Guide: [Cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html)\n",
    "- scikit-learn User Guide: [Common pitfalls and recommended practices](https://scikit-learn.org/stable/common_pitfalls.html)\n",
    "- Google Colab Documentation: https://colab.research.google.com/\n",
    "\n",
    "---\n",
    "\n",
    "**End of Day 1 Notebook** üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
