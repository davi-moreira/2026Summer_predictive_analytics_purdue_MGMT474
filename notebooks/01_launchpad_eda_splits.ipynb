{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1: Launchpad - Colab Workflow, Gemini Vibe-Coding, EDA, and Splitting Correctly\n",
    "\n",
    "**MGMT 47400 - Predictive Analytics**  \n",
    "**4-Week Online Course**  \n",
    "**Day 1 - Tuesday May 18, 2027**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/blob/main/notebooks/01_launchpad_eda_splits.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Operate course workflow in Google Colab (run-all, save-copy, submit link)\n",
    "2. Use Gemini in Colab to accelerate coding while preserving accountability (explain + verify)\n",
    "3. Perform structured EDA (types, missingness, target distribution, leakage sniff test)\n",
    "4. Create train/validation/test splits with reproducible seeds\n",
    "5. Identify obvious leakage patterns before modeling\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Installs, Imports, Seeds, Display Settings\n",
    "\n",
    "First, let's set up our environment with all necessary packages and configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install pandas numpy matplotlib seaborn scikit-learn --quiet\n",
    "\n",
    "# Core imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "\n",
    "# Display settings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 3)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"âœ“ Setup complete!\")\n",
    "print(f\"Random seed: {RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gemini Workflow Rules: Ask â†’ Verify â†’ Document\n",
    "\n",
    "### Using Gemini Responsibly in This Course\n",
    "\n",
    "**The \"Ask â†’ Verify â†’ Document\" Pattern:**\n",
    "\n",
    "1. **Ask**: Use Gemini to draft code or explain concepts\n",
    "2. **Verify**: Run the code, check outputs, understand what it does\n",
    "3. **Document**: Add comments explaining the logic in your own words\n",
    "\n",
    "**What's allowed:**\n",
    "- Using Gemini to generate boilerplate code\n",
    "- Asking Gemini to explain error messages\n",
    "- Getting suggestions for visualizations or analyses\n",
    "\n",
    "**What's required:**\n",
    "- You must understand every line of code you submit\n",
    "- You must verify that generated code works correctly\n",
    "- You must add your own comments and documentation\n",
    "\n",
    "**Accountability:**\n",
    "- You are responsible for all code you submit, even if Gemini generated it\n",
    "- Be prepared to explain your code and methodology\n",
    "- If you can't explain it, you don't understand it well enough yet\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Dataset\n",
    "\n",
    "We'll use a sample dataset to practice our EDA and splitting workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demonstration, we'll use the California Housing dataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Load data\n",
    "california = fetch_california_housing(as_frame=True)\n",
    "df = california.frame\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. EDA Checklist: Types, Missingness, Target Distribution\n",
    "\n",
    "### 4.1 Data Types Audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types\n",
    "print(\"=== DATA TYPES ===\")\n",
    "print(df.dtypes)\n",
    "print(\"\\n=== DATA INFO ===\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Missingness Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing,\n",
    "    'Missing_Percentage': missing_pct\n",
    "})\n",
    "\n",
    "print(\"=== MISSING VALUES ===\")\n",
    "print(missing_df[missing_df['Missing_Count'] > 0])\n",
    "\n",
    "if missing.sum() == 0:\n",
    "    print(\"âœ“ No missing values detected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Basic Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "print(\"=== DESCRIPTIVE STATISTICS ===\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Target Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize target distribution\n",
    "target_col = 'MedHouseVal'\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(df[target_col], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel(target_col)\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Target Distribution (Histogram)')\n",
    "axes[0].axvline(df[target_col].mean(), color='red', linestyle='--', label=f'Mean: {df[target_col].mean():.2f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot(df[target_col])\n",
    "axes[1].set_ylabel(target_col)\n",
    "axes[1].set_title('Target Distribution (Box Plot)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Target statistics:\")\n",
    "print(f\"  Mean: {df[target_col].mean():.3f}\")\n",
    "print(f\"  Median: {df[target_col].median():.3f}\")\n",
    "print(f\"  Std: {df[target_col].std():.3f}\")\n",
    "print(f\"  Min: {df[target_col].min():.3f}\")\n",
    "print(f\"  Max: {df[target_col].max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature distributions\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Plot histograms for all numeric features\n",
    "n_cols = 3\n",
    "n_rows = (len(numeric_cols) + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, col in enumerate(numeric_cols):\n",
    "    axes[idx].hist(df[col], bins=30, edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_xlabel(col)\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].set_title(f'{col} Distribution')\n",
    "\n",
    "# Hide extra subplots\n",
    "for idx in range(len(numeric_cols), len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "corr_matrix = df.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlations with target\n",
    "print(\"\\n=== CORRELATIONS WITH TARGET ===\")\n",
    "target_corr = corr_matrix[target_col].sort_values(ascending=False)\n",
    "print(target_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ PAUSE-AND-DO Exercise 1 (10 minutes)\n",
    "\n",
    "**Task:** Complete the EDA checklist on the dataset and summarize 3 key findings.\n",
    "\n",
    "**Instructions:**\n",
    "1. Review all the EDA outputs above\n",
    "2. Identify 3 important patterns or insights\n",
    "3. Write your findings in the cell below\n",
    "\n",
    "**What to look for:**\n",
    "- Unusual distributions\n",
    "- Strong correlations\n",
    "- Potential data quality issues\n",
    "- Features that might be useful predictors\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR FINDINGS HERE:\n",
    "\n",
    "**Finding 1:**  \n",
    "[Your observation]\n",
    "\n",
    "**Finding 2:**  \n",
    "[Your observation]\n",
    "\n",
    "**Finding 3:**  \n",
    "[Your observation]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train/Validation/Test Splits + Leakage Sniff Test\n",
    "\n",
    "### 5.1 Why Three Splits?\n",
    "\n",
    "- **Train set**: Used to fit models\n",
    "- **Validation set**: Used to tune hyperparameters and compare models\n",
    "- **Test set**: Final \"lockbox\" evaluation - touch only once at the end\n",
    "\n",
    "### 5.2 Creating Reproducible Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# Split: 60% train, 20% validation, 20% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=RANDOM_SEED  # 0.25 * 0.80 = 0.20\n",
    ")\n",
    "\n",
    "print(\"\\n=== SPLIT SIZES ===\")\n",
    "print(f\"Train: {len(X_train)} samples ({len(X_train)/len(df)*100:.1f}%)\")\n",
    "print(f\"Validation: {len(X_val)} samples ({len(X_val)/len(df)*100:.1f}%)\")\n",
    "print(f\"Test: {len(X_test)} samples ({len(X_test)/len(df)*100:.1f}%)\")\n",
    "print(f\"Total: {len(X_train) + len(X_val) + len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Split Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check target distribution across splits\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for idx, (split_name, split_y) in enumerate([('Train', y_train), ('Validation', y_val), ('Test', y_test)]):\n",
    "    axes[idx].hist(split_y, bins=30, edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_xlabel(target_col)\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].set_title(f'{split_name} Set (n={len(split_y)})')\n",
    "    axes[idx].axvline(split_y.mean(), color='red', linestyle='--', label=f'Mean: {split_y.mean():.2f}')\n",
    "    axes[idx].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=== TARGET DISTRIBUTION ACROSS SPLITS ===\")\n",
    "print(f\"Train mean: {y_train.mean():.3f}, std: {y_train.std():.3f}\")\n",
    "print(f\"Val mean: {y_val.mean():.3f}, std: {y_val.std():.3f}\")\n",
    "print(f\"Test mean: {y_test.mean():.3f}, std: {y_test.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Leakage Sniff Test\n",
    "\n",
    "**Common leakage patterns to watch for:**\n",
    "\n",
    "1. **Target leakage**: Features that contain information from the future or are derived from the target\n",
    "2. **Test set contamination**: Using test data to make preprocessing or modeling decisions\n",
    "3. **Time-based leakage**: Using future data to predict the past\n",
    "4. **Preprocessing leakage**: Fitting transformers on the full dataset before splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leakage checklist\n",
    "print(\"=== LEAKAGE SNIFF TEST ===\")\n",
    "print(\"\\nâœ“ Checklist:\")\n",
    "print(\"  [âœ“] Split data before any preprocessing\")\n",
    "print(\"  [âœ“] No overlap between train/val/test indices\")\n",
    "print(\"  [ ] Check feature descriptions for suspicious variables\")\n",
    "print(\"  [ ] Verify no future information in features\")\n",
    "print(\"  [ ] Confirm target is not derived from features\")\n",
    "\n",
    "print(\"\\n=== FEATURE NAMES ===\")\n",
    "print(\"Review these carefully for potential leakage:\")\n",
    "for col in X.columns:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "print(\"\\nâš ï¸ Red flags to watch for:\")\n",
    "print(\"  - Features with 'target', 'outcome', 'label' in name\")\n",
    "print(\"  - Perfect correlations (r > 0.99)\")\n",
    "print(\"  - Features that would not be available at prediction time\")\n",
    "print(\"  - ID columns that encode information about the target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ PAUSE-AND-DO Exercise 2 (10 minutes)\n",
    "\n",
    "**Task:** Create train/val/test splits and write 3 leakage risks specific to this dataset.\n",
    "\n",
    "**Instructions:**\n",
    "1. The splits are already created above - verify they look reasonable\n",
    "2. Review the feature names and descriptions\n",
    "3. Identify 3 potential leakage risks for this type of problem\n",
    "4. Write your analysis below\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR LEAKAGE ANALYSIS HERE:\n",
    "\n",
    "**Potential Risk 1:**  \n",
    "[Describe a feature or scenario that could cause leakage]\n",
    "\n",
    "**Potential Risk 2:**  \n",
    "[Describe another leakage risk]\n",
    "\n",
    "**Potential Risk 3:**  \n",
    "[Describe a third leakage risk]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Wrap-Up: Key Takeaways\n",
    "\n",
    "### What We Learned Today:\n",
    "\n",
    "1. **Colab Workflow**: How to use Google Colab for course assignments\n",
    "2. **Gemini Guidelines**: Ask â†’ Verify â†’ Document pattern for responsible AI assistance\n",
    "3. **EDA Essentials**: Data types, missingness, distributions, correlations\n",
    "4. **Proper Splitting**: Train/val/test splits with reproducible seeds\n",
    "5. **Leakage Awareness**: Common patterns to watch for\n",
    "\n",
    "### Next-Day Readiness:\n",
    "\n",
    "- âœ“ You can load and explore a dataset\n",
    "- âœ“ You can create proper train/val/test splits\n",
    "- âœ“ You can identify potential leakage risks\n",
    "- âœ“ You're ready for Day 2: Preprocessing Pipelines\n",
    "\n",
    "### Remember:\n",
    "\n",
    "> **\"Split first, preprocess second, model third\"**  \n",
    "> This order prevents leakage and ensures valid evaluation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Submission Instructions\n",
    "\n",
    "### To Submit This Notebook:\n",
    "\n",
    "1. **Run All Cells**: Execute `Runtime â†’ Run all` to ensure everything works\n",
    "2. **Save a Copy**: `File â†’ Save a copy in Drive`\n",
    "3. **Get Shareable Link**: Click `Share` and set to \"Anyone with the link can view\"\n",
    "4. **Submit Link**: Paste the link in the LMS assignment\n",
    "\n",
    "### Before Submitting, Check:\n",
    "\n",
    "- [ ] All cells execute without errors\n",
    "- [ ] All outputs are visible\n",
    "- [ ] Exercise responses are complete\n",
    "- [ ] Notebook is shared with correct permissions\n",
    "\n",
    "---\n",
    "\n",
    "## Bibliography\n",
    "\n",
    "- James, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). *An Introduction to Statistical Learning with Python* (ISLP). Springer.\n",
    "- scikit-learn User Guide: [Cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html)\n",
    "- scikit-learn User Guide: [Common pitfalls and recommended practices](https://scikit-learn.org/stable/common_pitfalls.html)\n",
    "- Kaggle Learn: [Data Leakage](https://www.kaggle.com/learn/data-leakage)\n",
    "\n",
    "---\n",
    "\n",
    "**End of Day 1 Notebook** ðŸŽ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
