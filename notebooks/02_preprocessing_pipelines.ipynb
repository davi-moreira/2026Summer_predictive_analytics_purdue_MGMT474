{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Setup and Preprocessing Pipelines (The Professional Way)\n",
    "\n",
    "<hr>\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/main/notebooks/figures/mgmt_474_ai_logo_02-modified.png\" width=\"200\"/>\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "# <center><a class=\"tocSkip\"></center>\n",
    "# <center>MGMT47400 Predictive Analytics</center>\n",
    "# <center>Professor: Davi Moreira </center>\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/davi-moreira/2026Summer_predictive_analytics_purdue_MGMT474/blob/main/notebooks/02_preprocessing_pipelines.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. Audit data types and fix common pandas pitfalls (strings, categories, dates)\n",
    "2. Handle missing values without leaking information\n",
    "3. Build a preprocessing + model Pipeline with `ColumnTransformer`\n",
    "4. Separate \"fit on train only\" logic from evaluation logic\n",
    "5. Use Gemini to draft pipeline code and then harden it (tests + comments)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n# !pip install pandas numpy matplotlib seaborn scikit-learn --quiet\n\n# Core imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LinearRegression\nimport warnings\n\n# Display settings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', None)\npd.set_option('display.precision', 3)\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (10, 6)\n\n# Set random seed for reproducibility\nRANDOM_SEED = 474\nnp.random.seed(RANDOM_SEED)\n\nprint(\"\u2713 Setup complete!\")\nprint(f\"Random seed: {RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset and Create Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load California Housing dataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "california = fetch_california_housing(as_frame=True)\n",
    "df = california.frame\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFeatures: {df.columns.tolist()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop(columns=['MedHouseVal'])\n",
    "y = df['MedHouseVal']\n",
    "\n",
    "# Split: 60% train, 20% validation, 20% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(\"=== SPLIT SIZES ===\")\n",
    "print(f\"Train: {len(X_train)} samples ({len(X_train)/len(df)*100:.1f}%)\")\n",
    "print(f\"Validation: {len(X_val)} samples ({len(X_val)/len(df)*100:.1f}%)\")\n",
    "print(f\"Test: {len(X_test)} samples ({len(X_test)/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Audit Report Function\n",
    "\n",
    "Let's create a function to systematically audit our data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_report(df, name=\"Dataset\"):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive data audit report.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        The dataframe to audit\n",
    "    name : str\n",
    "        Name of the dataset for reporting\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Report with types, missingness, and unique counts\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"DATA AUDIT REPORT: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    print(f\"\\nShape: {df.shape[0]:,} rows \u00d7 {df.shape[1]} columns\")\n",
    "    \n",
    "    # Build report dataframe\n",
    "    report = pd.DataFrame({\n",
    "        'dtype': df.dtypes,\n",
    "        'missing_count': df.isnull().sum(),\n",
    "        'missing_pct': (df.isnull().sum() / len(df) * 100).round(2),\n",
    "        'unique_count': df.nunique(),\n",
    "        'unique_pct': (df.nunique() / len(df) * 100).round(2)\n",
    "    })\n",
    "    \n",
    "    print(\"\\n=== COLUMN AUDIT ===\")\n",
    "    print(report)\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\n=== SUMMARY ===\")\n",
    "    print(f\"Total missing values: {df.isnull().sum().sum():,}\")\n",
    "    print(f\"Columns with missing data: {(df.isnull().sum() > 0).sum()}\")\n",
    "    print(f\"Numeric columns: {df.select_dtypes(include=[np.number]).shape[1]}\")\n",
    "    print(f\"Non-numeric columns: {df.select_dtypes(exclude=[np.number]).shape[1]}\")\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Run audit on training data\n",
    "train_report = make_data_report(X_train, \"Training Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd PAUSE-AND-DO Exercise 1 (10 minutes)\n",
    "\n",
    "**Task:** Implement `make_data_report(df)` that returns types, missingness %, and unique counts.\n",
    "\n",
    "**Instructions:**\n",
    "1. The function is already implemented above\n",
    "2. Run it on `X_train`, `X_val`, and `X_test`\n",
    "3. Document any differences you observe between splits\n",
    "4. Write your observations in the cell below\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run audit on all splits\n",
    "val_report = make_data_report(X_val, \"Validation Set\")\n",
    "test_report = make_data_report(X_test, \"Test Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR OBSERVATIONS HERE:\n",
    "\n",
    "**Observation 1:**  \n",
    "[Document any differences in data types across splits]\n",
    "\n",
    "**Observation 2:**  \n",
    "[Document any missingness patterns]\n",
    "\n",
    "**Observation 3:**  \n",
    "[Document any unique value differences]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocessing Pipeline Template\n",
    "\n",
    "### 4.1 Identify Column Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numeric and categorical columns\n",
    "numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "print(\"=== FEATURE TYPES ===\")\n",
    "print(f\"\\nNumeric features ({len(numeric_features)}):\")\n",
    "for feat in numeric_features:\n",
    "    print(f\"  - {feat}\")\n",
    "\n",
    "print(f\"\\nCategorical features ({len(categorical_features)}):\")\n",
    "if categorical_features:\n",
    "    for feat in categorical_features:\n",
    "        print(f\"  - {feat}\")\n",
    "else:\n",
    "    print(\"  (none)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Build Preprocessing Pipeline\n",
    "\n",
    "**Pipeline Design Principles:**\n",
    "1. Fit transformers ONLY on training data\n",
    "2. Apply the same transformation to validation and test\n",
    "3. Handle numeric and categorical features separately\n",
    "4. Chain all steps together to prevent leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric preprocessing: impute + scale\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical preprocessing: impute + encode\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Combine transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'  # Drop any columns not specified\n",
    ")\n",
    "\n",
    "print(\"\u2713 Preprocessor pipeline created!\")\n",
    "print(f\"\\nNumeric features: {len(numeric_features)}\")\n",
    "print(f\"Categorical features: {len(categorical_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Full Pipeline: Preprocessing + Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full pipeline: preprocessing + model\n",
    "full_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "print(\"=== FULL PIPELINE ===\")\n",
    "print(full_pipeline)\n",
    "\n",
    "# Fit on training data ONLY\n",
    "print(\"\\nFitting pipeline on training data...\")\n",
    "full_pipeline.fit(X_train, y_train)\n",
    "print(\"\u2713 Pipeline fitted!\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "train_score = full_pipeline.score(X_train, y_train)\n",
    "val_score = full_pipeline.score(X_val, y_val)\n",
    "\n",
    "print(f\"\\n=== SCORES ===\")\n",
    "print(f\"Train R\u00b2: {train_score:.4f}\")\n",
    "print(f\"Validation R\u00b2: {val_score:.4f}\")\n",
    "print(f\"Difference: {train_score - val_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Inspecting the Pipeline\n",
    "\n",
    "Understanding what features are created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names after transformation\n",
    "try:\n",
    "    feature_names = full_pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
    "    print(f\"=== TRANSFORMED FEATURES ===\")\n",
    "    print(f\"Total features after preprocessing: {len(feature_names)}\")\n",
    "    print(f\"\\nFirst 10 features:\")\n",
    "    for i, name in enumerate(feature_names[:10], 1):\n",
    "        print(f\"  {i}. {name}\")\n",
    "    \n",
    "    if len(feature_names) > 10:\n",
    "        print(f\"  ... and {len(feature_names) - 10} more\")\n",
    "except:\n",
    "    print(\"Feature names not available (older sklearn version)\")\n",
    "    print(f\"Estimated features: {len(numeric_features)} numeric features (scaled)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd PAUSE-AND-DO Exercise 2 (10 minutes)\n",
    "\n",
    "**Task:** Create a full sklearn Pipeline and run one validation score.\n",
    "\n",
    "**Instructions:**\n",
    "1. The pipeline is already created above\n",
    "2. Modify the pipeline to try different imputation strategies:\n",
    "   - Change median to mean for numeric features\n",
    "   - Try different imputation for categorical features\n",
    "3. Compare the validation scores\n",
    "4. Document your findings below\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Experiment with different preprocessing strategies\n",
    "\n",
    "# Example: Try mean imputation instead of median\n",
    "numeric_transformer_v2 = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Changed from median\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor_v2 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer_v2, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "full_pipeline_v2 = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_v2),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Fit and evaluate\n",
    "full_pipeline_v2.fit(X_train, y_train)\n",
    "train_score_v2 = full_pipeline_v2.score(X_train, y_train)\n",
    "val_score_v2 = full_pipeline_v2.score(X_val, y_val)\n",
    "\n",
    "print(\"=== COMPARISON ===\")\n",
    "print(f\"\\nOriginal (median imputation):\")\n",
    "print(f\"  Train R\u00b2: {train_score:.4f}\")\n",
    "print(f\"  Val R\u00b2: {val_score:.4f}\")\n",
    "\n",
    "print(f\"\\nAlternative (mean imputation):\")\n",
    "print(f\"  Train R\u00b2: {train_score_v2:.4f}\")\n",
    "print(f\"  Val R\u00b2: {val_score_v2:.4f}\")\n",
    "\n",
    "print(f\"\\nDifference in validation score: {val_score_v2 - val_score:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR FINDINGS HERE:\n",
    "\n",
    "**Finding 1:**  \n",
    "[Compare the two imputation strategies]\n",
    "\n",
    "**Finding 2:**  \n",
    "[Discuss which performs better and why]\n",
    "\n",
    "**Finding 3:**  \n",
    "[Any other observations about the pipeline]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gemini Prompt Cards for Pipeline Generation\n",
    "\n",
    "### Example Prompts:\n",
    "\n",
    "**Prompt 1: Generate Pipeline**\n",
    "```\n",
    "Create a scikit-learn pipeline that:\n",
    "1. Imputes missing values (median for numeric, most_frequent for categorical)\n",
    "2. Scales numeric features using StandardScaler\n",
    "3. Encodes categorical features using OneHotEncoder\n",
    "4. Fits a LinearRegression model\n",
    "\n",
    "Use ColumnTransformer to handle different feature types separately.\n",
    "```\n",
    "\n",
    "**Prompt 2: Debug Pipeline**\n",
    "```\n",
    "I'm getting an error when fitting my pipeline: [error message]\n",
    "My pipeline code is: [code]\n",
    "Help me debug this issue and explain what's wrong.\n",
    "```\n",
    "\n",
    "**Prompt 3: Extend Pipeline**\n",
    "```\n",
    "I have a working pipeline with imputation and scaling.\n",
    "How can I add polynomial features (degree=2) only to numeric features\n",
    "while keeping the categorical encoding unchanged?\n",
    "```\n",
    "\n",
    "**Remember:** After using Gemini, always:\n",
    "1. Verify the code runs without errors\n",
    "2. Check the output makes sense\n",
    "3. Add your own comments explaining each step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Pipeline Done Right Checklist\n",
    "\n",
    "Before moving on, verify your pipeline meets these standards:\n",
    "\n",
    "### \u2713 Pre-fitting Checks:\n",
    "- [ ] Split data BEFORE building pipeline\n",
    "- [ ] Identified all numeric and categorical features\n",
    "- [ ] Handled missing values with appropriate strategy\n",
    "- [ ] Separate transformers for different feature types\n",
    "\n",
    "### \u2713 Fitting Checks:\n",
    "- [ ] Pipeline fitted ONLY on training data\n",
    "- [ ] No data leakage from validation or test sets\n",
    "- [ ] All preprocessing steps are inside the pipeline\n",
    "- [ ] Model is the last step in the pipeline\n",
    "\n",
    "### \u2713 Evaluation Checks:\n",
    "- [ ] Evaluated on validation set (not test set)\n",
    "- [ ] Training and validation scores are reasonable\n",
    "- [ ] No major signs of overfitting (huge train/val gap)\n",
    "- [ ] Pipeline can transform new data without refitting\n",
    "\n",
    "### \u2713 Code Quality:\n",
    "- [ ] All parameters are explicit (no hidden defaults)\n",
    "- [ ] Steps are named clearly\n",
    "- [ ] Comments explain \"why\" not just \"what\"\n",
    "- [ ] Can explain every step if asked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Wrap-Up: Key Takeaways\n\n### What We Learned Today:\n\n1. **Data Auditing**: Systematic checks for types, missingness, and unique values\n2. **Preprocessing Patterns**: Separate handling for numeric vs categorical features\n3. **Pipeline Architecture**: ColumnTransformer + Pipeline prevents leakage\n4. **Fit/Transform Discipline**: Fit on train, transform on train/val/test\n5. **Gemini Integration**: Use AI to draft, then verify and document\n\n### Critical Rules:\n\n> **\"Fit ONLY on training data\"**  \n> Any statistics (mean, median, categories) must be computed from training data only.\n\n> **\"Pipeline wraps everything\"**  \n> If you do it manually, you risk leakage. Put it in the pipeline.\n\n### Next Steps:\n\n- The next notebook will cover regression metrics and baseline models\n- We'll use today's pipeline structure for all future models\n- Start thinking about your project dataset\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Submission Instructions\n\n### To Submit This Notebook:\n\n1. **Run All Cells**: Execute `Runtime \u2192 Run all` to ensure everything works\n2. **Save a Copy**: `File \u2192 Save a copy in Drive`\n3. **Get Shareable Link**: Click `Share` and set to \"Anyone with the link can view\"\n4. **Submit Link**: Paste the link in the LMS assignment\n\n### Before Submitting, Check:\n\n- [ ] All cells execute without errors\n- [ ] All outputs are visible\n- [ ] Exercise responses are complete\n- [ ] Pipeline checklist is verified\n- [ ] Notebook is shared with correct permissions\n\n---\n\n## Bibliography\n\n- scikit-learn User Guide: [Pipelines and composite estimators](https://scikit-learn.org/stable/modules/compose.html)\n- scikit-learn User Guide: [ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html)\n- scikit-learn User Guide: [Preprocessing data](https://scikit-learn.org/stable/modules/preprocessing.html)\n- Pedregosa et al. (2011). \"Scikit-learn: Machine Learning in Python.\" *Journal of Machine Learning Research*, 12, 2825-2830.\n- James, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). *An Introduction to Statistical Learning with Python* (ISLP). Springer.\n\n---\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "Thank you!\n",
    "\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}