<!DOCTYPE html>
<html lang="en"><head>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/tabby.min.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.8.25">

  <meta name="author" content="Professor: Davi Moreira">
  <title>MGMT 47400: Predictive Analytics (Summer 2026 - 4-Week Intensive) –  MGMT 47400: Predictive Analytics </title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/theme/quarto-96a67a1340eb436771fd03060be7db16.css">
  <link href="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title"><span style="font-size: 100%;"> MGMT 47400: Predictive Analytics </span></h1>
  <p class="subtitle"><span style="font-size: 150%;"> Resampling Methods </span></p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Professor: Davi Moreira 
</div>
</div>
</div>

</section>
<section id="overview" class="slide level2 center">
<h2>Overview</h2>
<div class="columns">
<div class="column" style="text-align: center; justify-content: center; align-items: center;">
<ul>
<li>Motivation</li>
<li>Training Error versus Test Error</li>
<li>Validation-Set Approach</li>
<li>Cross-Validation</li>
<li>Cross-Validation for Classification Problems</li>
</ul>
</div><div class="column" style="text-align: center; justify-content: center; align-items: center;">
<ul>
<li>Bootstrap</li>
<li>More on Bootstrap</li>
<li>Can the Bootstrap Estimate Prediction Error?</li>
</ul>
</div></div>
<p><br></p>

<aside><div>
<p><em>This lecture content is inspired by and replicates the material from <a href="https://www.statlearning.com/">An Introduction to Statistical Learning</a>.</em></p>
</div></aside></section>
<section id="recap" class="slide level2 center">
<h2>Recap</h2>
<p><br></p>
<div style="font-size: 80%;">
<div class="columns">
<div class="column" style="width:50%;">
<details>
<summary>
<strong>Why is linear regression generally not suitable for estimating probabilities in a classification task, even for binary outcomes?</strong>
</summary>
<p style="font-size:80%;">
Linear regression can produce predictions that fall outside the [0, 1] range, which are not valid probabilities. Logistic regression, on the other hand, uses a link function that ensures its output always lies between 0 and 1, making it appropriate for probability estimation.
</p>
</details>
<details>
<summary>
<strong>What is the core principle of Maximum Likelihood Estimation (MLE)? </strong>
</summary>
<p style="font-size:80%;">
MLE is a method used to estimate the parameters of a statistical model by finding the parameter values that maximize the likelihood function. This means choosing the parameters that make the observed data most probable under the assumed model.
</p>
</details>
<details>
<summary>
<strong>What is the softmax function, and why is it essential in multinomial logistic regression? </strong>
</summary>
<p style="font-size:80%;">
The softmax function converts a vector of raw scores (logits) into a probability distribution over multiple classes. It is essential because it ensures that the output probabilities for all classes sum to 1, making it suitable for multiclass classification tasks where the response has more than two categories.
</p>
</details>
<details>
<summary>
<strong>What is the difference between a discriminative model and a generative model in classification? </strong>
</summary>
<p style="font-size:80%;">
A discriminative model (e.g., Logistic Regression) directly models the conditional probability <span class="math inline">\(Pr(Y|X)\)</span>. A generative model (e.g., LDA, Naïve Bayes) models the class-conditional distribution <span class="math inline">\(Pr(X|Y)\)</span> and the prior <span class="math inline">\(Pr(Y)\)</span>, then uses Bayes’ theorem to derive <span class="math inline">\(Pr(Y|X)\)</span>.
</p>
</details>
</div><div class="column" style="width:50%;">
<details>
<summary>
<strong>Under what key assumption is Linear Discriminant Analysis (LDA) applied, and how does this affect its decision boundaries?</strong>
</summary>
<p style="font-size:80%;">
LDA assumes that the class-conditional distributions of the predictors are Gaussian and share a common covariance matrix across all classes. This assumption leads to decision boundaries that are linear.
</p>
</details>
<details>
<summary>
<strong>When would Quadratic Discriminant Analysis (QDA) be preferred over LDA, and what is the main reason for this preference?</strong>
</summary>
<p style="font-size:70%;">
QDA is preferred over LDA when the class-conditional covariance matrices are significantly different for each class. This allows QDA to capture more complex relationships in the data, resulting in quadratic (curved) decision boundaries, albeit with potentially higher variance due to more parameters.
</p>
</details>
<details>
<summary>
<strong>What is the “Naïve” assumption in Naïve Bayes classification, and why is it often made despite its potential inaccuracy?</strong>
</summary>
<p style="font-size:70%;">
The “Naïve” assumption is that the features are conditionally independent given the class. This assumption is often made because it greatly simplifies the modeling of high-dimensional densities by allowing one-dimensional densities to be multiplied, making the model computationally efficient and robust even when the assumption is not perfectly met.
</p>
</details>
<details>
<summary>
<strong>What is the ROC curve and how to interpret it?</strong>
</summary>
<p style="font-size:70%;">
The ROC curve plots True Positive Rate versus False Positive Rate as you vary the score threshold; curves closer to the upper-left are better, the diagonal is random. AUC summarizes ranking quality (0.5=random, 1=perfect) and equals the probability a random positive scores above a random negative. Pick the operating threshold by costs and class mix (optimal ROC slope <span class="math inline">\(=\frac{C_{FP}\pi_-}{C_{FN}\pi_+}\)</span>); if costs are similar, maximize Youden’s <span class="math inline">\(J=\text{TPR}-\text{FPR}\)</span>. Use PR curves when positives are rare and precision matters, and remember ROC/AUC don’t assess calibration—pair with reliability metrics.
</p>
</details>
</div></div>
</div>
</section>
<section>
<section id="motivation" class="title-slide slide level1 center" data-background-color="#cfb991">
<h1>Motivation</h1>

</section>
<section id="prediction" class="slide level2 center">
<h2>Prediction</h2>
<ul>
<li class="fragment"><strong>Goal</strong>: Build predictors and classifiers to make accurate predictions from data.</li>
<li class="fragment"><strong>Challenge</strong>: How do we evaluate our predictions?</li>
</ul>
<div class="fragment">
<p><strong>Ideal Scenario: New Data</strong></p>
<ul>
<li class="fragment">The best way to test predictions is to use <strong>new, independent data</strong> from the population.</li>
<li class="fragment"><strong>Problem</strong>: New data isn’t always available.</li>
</ul>
</div>
</section>
<section id="why-not-use-training-data" class="slide level2 center">
<h2>Why Not Use Training Data?</h2>
<ul>
<li class="fragment">Using training data for evaluation is <strong>not reliable</strong>.
<ul>
<li class="fragment">Models tend to perform better on data they’ve already seen.</li>
<li class="fragment">This leads to overly <strong>optimistic</strong> results.</li>
</ul></li>
</ul>
</section>
<section id="solution-resampling-methods" class="slide level2 center">
<h2>Solution: Resampling methods</h2>
<ul>
<li class="fragment"><p>Cross-validation and the Bootstrap are two <em>resampling</em> methods.</p></li>
<li class="fragment"><p>These methods allows us to evaluate the performance of our predictors using the available data without relying on additional samples.</p></li>
<li class="fragment"><p>They refit a model of interest to samples formed from the training set, in order to obtain additional information about the fitted model.</p></li>
<li class="fragment"><p>For example, they provide estimates of test-set prediction error, and the standard deviation and bias of our parameter estimates.</p></li>
</ul>
</section></section>
<section>
<section id="training-error-versus-test-error" class="title-slide slide level1 center" data-background-color="#cfb991">
<h1>Training Error versus Test Error</h1>

</section>
<section id="training-error-versus-test-error-1" class="slide level2 center">
<h2>Training Error versus Test Error</h2>
<ul>
<li class="fragment"><p>Recall the distinction between the <em>test error</em> and the <em>training error</em>:</p></li>
<li class="fragment"><p>The <em>test error</em> is the average error that results from using a statistical learning method to predict the response on a new observation, one that was not used in training the method.</p></li>
<li class="fragment"><p>The <em>training error</em> can be easily calculated by applying the statistical learning method to the observations used in its training.</p></li>
<li class="fragment"><p>But the training error rate often is quite different from the test error rate, and in particular, the former can <em>dramatically underestimate</em> the latter.</p></li>
</ul>
</section>
<section id="training--versus-test-set-performance" class="slide level2 center">
<h2>Training- versus Test-Set Performance</h2>
<div style="font-size: 50%;">
<div class="columns">
<div class="column" style="width:60%;">
<p><br></p>
<p><br></p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figs/5_1_4-1.png" class="quarto-figure quarto-figure-center" style="width:75.0%"></p>
</figure>
</div>
</div>
</div>
<ul>
<li class="fragment"><strong>Horizontal Axis</strong>: Represents <strong>model complexity</strong> (low to high).
<ul>
<li class="fragment">Low complexity: Simpler models with fewer parameters (e.g., fitting a straight line or using a few features).</li>
<li class="fragment">High complexity: More complex models with many parameters (e.g., higher-degree polynomials or many features).</li>
</ul></li>
<li class="fragment"><strong>Vertical Axis</strong>: Represents <strong>prediction error</strong>.
<ul>
<li class="fragment">Lower values indicate better predictive performance.</li>
</ul></li>
</ul>
</div><div class="column" style="width:40%;">
<ol type="1">
<li class="fragment"><strong>Training Error (Blue Curve)</strong>:
<ul>
<li class="fragment">Starts high at low complexity because simple models underfit the training data.</li>
<li class="fragment">Decreases steadily as the model becomes more complex, fitting the training data better.</li>
<li class="fragment">Continues to decline even as the model becomes overly complex.</li>
</ul></li>
<li class="fragment"><strong>Test Error (Red Curve)</strong>:
<ul>
<li class="fragment">Starts high at low complexity due to underfitting (failure to generalize).</li>
<li class="fragment">Decreases as complexity increases and the model starts capturing relevant patterns.</li>
<li class="fragment">Reaches a <strong>minimum</strong> at the optimal complexity (sweet spot).</li>
<li class="fragment">Increases again at high complexity due to overfitting (model captures noise instead of general patterns).</li>
</ul></li>
</ol>
<ul>
<li class="fragment"><p><strong>Key Concepts</strong></p></li>
<li class="fragment"><p><strong>Bias-Variance Tradeoff</strong>:</p>
<ul>
<li class="fragment"><strong>High Bias (Left Side)</strong>: Simple models fail to capture the true structure of the data.</li>
<li class="fragment"><strong>High Variance (Right Side)</strong>: Complex models become overly tailored to the training data and fail to generalize.</li>
</ul></li>
<li class="fragment"><p><strong>Optimal Complexity</strong>:</p>
<ul>
<li class="fragment">Located where the <strong>test error</strong> is minimized.</li>
<li class="fragment">Balances bias and variance for the best generalization performance.</li>
</ul></li>
<li class="fragment"><p>The <strong>Goal</strong> is to select a model complexity that minimizes test error to ensure good predictive performance on unseen data.</p></li>
</ul>
</div></div>
</div>
</section>
<section id="precision-and-accuracy" class="slide level2 center">
<h2>Precision and Accuracy</h2>
<div class="columns">
<div class="column" style="text-align: center; justify-content: center; align-items: center;">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figs/precision_accuracy.png" class="quarto-figure quarto-figure-center" style="width:80.0%"></p>
</figure>
</div>
</div>
</div>
</div><div class="column" style="text-align: center; justify-content: center; align-items: center;">
<div style="font-size: 70%;">
<ul>
<li class="fragment"><p><strong>Precision</strong>: Refers to the consistency or reliability of the model’s predictions.</p></li>
<li class="fragment"><p><strong>Accuracy</strong>: Refers to how close the model’s predictions are to the true values.</p></li>
</ul>
<div class="fragment">
<p>In the context of regression:</p>
<ul>
<li class="fragment"><strong>High Precision, Low Accuracy</strong>: Predictions are consistent but biased.</li>
<li class="fragment"><strong>High Precision, High Accuracy</strong>: Predictions are both consistent and valid.</li>
<li class="fragment"><strong>Low Precision, Low Accuracy</strong>: Predictions are neither consistent nor valid.</li>
<li class="fragment"><strong>Low Precision, High Accuracy</strong>: Predictions are valid on average but have high variability.</li>
</ul>
</div>
</div>
</div></div>
</section>
<section id="more-on-prediction-error-estimates" class="slide level2 center">
<h2>More on Prediction-Error Estimates</h2>
<ul>
<li class="fragment">Best solution: test the model with a large test set.
<ul>
<li class="fragment">However, it is not very often available.</li>
</ul></li>
<li class="fragment">In the absence of a large test set, some methods make a <em>mathematical adjustment</em> to the training error rate in order to estimate the test error rate.
<ul>
<li class="fragment">These include the <em>Cp statistic</em>, <em>AIC</em>, and <em>BIC</em>.</li>
</ul></li>
<li class="fragment">In this lecture we consider a class of methods that estimate the test error by <em>holding out</em> a subset of the training observations from the fitting process, and then applying the statistical learning method to those held-out observations.</li>
</ul>
</section></section>
<section>
<section id="validation-set-approach" class="title-slide slide level1 center" data-background-color="#cfb991">
<h1>Validation-Set Approach</h1>

</section>
<section id="validation-set-approach-1" class="slide level2 center">
<h2>Validation-Set Approach</h2>
<ul>
<li class="fragment"><p>Here we randomly divide the available set of samples into two parts: a <em>training set</em> and a <em>validation</em> or <em>hold-out set</em>.</p></li>
<li class="fragment"><p>The model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set.</p></li>
<li class="fragment"><p>The resulting validation-set error provides an estimate of the test error.</p>
<ul>
<li class="fragment">This is typically assessed using the <em>Mean Squared Error (MSE)</em> in the case of a quantitative response and Misclassification Rate in the case of a qualitative (discrete) response.</li>
</ul></li>
</ul>
</section>
<section id="the-validation-process" class="slide level2 center">
<h2>The Validation Process</h2>

<img data-src="figs/5_1-1.png" class="quarto-figure quarto-figure-center r-stretch" style="width:65.0%"><ul>
<li class="fragment"><p><strong>A random splitting of the original dataset into two halves (two-fold validation):</strong></p>
<ul>
<li class="fragment">Left part is the training set</li>
<li class="fragment">Right part is the validation set</li>
</ul></li>
</ul>
</section>
<section id="example-automobile-data" class="slide level2 center">
<h2>Example: Automobile Data</h2>
<div style="font-size: 80%;">
<ul>
<li><p>Want to compare linear vs higher-order polynomial terms in a linear regression.</p></li>
<li><p>We randomly split the 392 observations into two sets:</p>
<ul>
<li>A training set containing 196 of the data points.</li>
<li>A validation set containing the remaining 196 observations.</li>
</ul></li>
</ul>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figs/5_2-1.png" class="quarto-figure quarto-figure-center" style="width:65.0%"></p>
</figure>
</div>
</div>
</div>
<p>Left panel shows single split; right panel shows multiple splits.</p>
</div>
</section>
<section id="drawbacks-of-validation-set-approach" class="slide level2 center">
<h2>Drawbacks of Validation Set Approach</h2>
<ul>
<li class="fragment"><p>The validation estimate of the test error can be highly variable, depending on precisely which observations are included in the training set and which observations are included in the validation set.</p></li>
<li class="fragment"><p>In the validation approach, only a subset of the observations — those that are included in the training set rather than in the validation set — are used to fit the model.</p></li>
<li class="fragment"><p>This suggests that the validation set error may tend to <em>overestimate</em> the test error for the model fit on the entire data set. <em>Why?</em></p>
<ul>
<li class="fragment">Having more data generally leads to lower error because it provides more information for training the model.</li>
<li class="fragment">For example, training on <strong>200 observations</strong> is typically preferable to <strong>100 observations</strong>, as larger datasets improve accuracy.</li>
<li class="fragment">However, when the training set is reduced (e.g., during validation), error estimates can be higher since smaller datasets may fail to capture all patterns in the data.</li>
<li class="fragment">This limitation highlights the drawbacks of simple validation.</li>
<li class="fragment"><strong>Cross-validation</strong> addresses this issue by efficiently using the data to produce more accurate and reliable error estimates.</li>
</ul></li>
</ul>
</section></section>
<section>
<section id="cross-validation" class="title-slide slide level1 center" data-background-color="#cfb991">
<h1>Cross-Validation</h1>

</section>
<section id="k-fold-cross-validation" class="slide level2 center">
<h2><a href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)" target="_blank">K-Fold Cross-Validation</a></h2>
<ul>
<li class="fragment"><p><em>Widely used approach</em> for estimating test error.</p></li>
<li class="fragment"><p>Estimates can be used to select the best model and to give an idea of the test error of the final chosen model.</p></li>
<li class="fragment"><p>The idea is to randomly divide the data into <span class="math inline">\(K\)</span> equal-sized parts. We leave out part <span class="math inline">\(k\)</span>, fit the model to the other <span class="math inline">\(K-1\)</span> parts (combined), and then obtain predictions for the left-out <span class="math inline">\(k\)</span>-th part.</p></li>
<li class="fragment"><p>This is done in turn for each part <span class="math inline">\(k = 1, 2, \ldots, K\)</span>, and then the results are combined.</p></li>
</ul>
</section>
<section id="k-fold-cross-validation-in-detail" class="slide level2 center">
<h2>K-Fold Cross-Validation in Detail</h2>
<p>Divide data into <span class="math inline">\(K\)</span> roughly equal-sized parts (<span class="math inline">\(K = 5\)</span> here).</p>
<p><br></p>

<img data-src="figs/grid_search_cross_validation.png" class="quarto-figure quarto-figure-center r-stretch" style="width:55.0%"><p><br></p>
</section>
<section id="k-fold-cross-validation-in-detail-1" class="slide level2 center">
<h2>K-Fold Cross-Validation in Detail</h2>
<p>Divide data into <span class="math inline">\(K\)</span> roughly equal-sized parts (<span class="math inline">\(K = 3\)</span> here).</p>
<p><br></p>

<img data-src="figs/KfoldCV.gif" class="quarto-figure quarto-figure-center r-stretch" style="width:30.0%"><p><br></p>
<center>
<a href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)" target="_blank">Wiki</a>
</center>
<p><br></p>
</section>
<section id="k-fold-cross-validation-in-algebra" class="slide level2 center">
<h2>K-Fold Cross-Validation: in algebra</h2>
<ul>
<li class="fragment"><p>Let the <span class="math inline">\(K\)</span> parts be <span class="math inline">\(C_1, C_2, \ldots, C_K\)</span>, where <span class="math inline">\(C_k\)</span> denotes the indices of the observations in part <span class="math inline">\(k\)</span>. There are <span class="math inline">\(n_k\)</span> observations in part <span class="math inline">\(k\)</span>: if <span class="math inline">\(N\)</span> is a multiple of <span class="math inline">\(K\)</span>, then <span class="math inline">\(n_k = n / K\)</span>.</p></li>
<li class="fragment"><p>Compute the <strong>cross-validations error rate</strong>:</p></li>
</ul>
<div class="fragment">
<p><span class="math display">\[
  \text{CV}_{(K)} = \sum_{k=1}^{K} \frac{n_k}{n} \text{MSE}_k
\]</span></p>
<p>where <span class="math inline">\(\text{MSE}_k = \frac{\sum_{i \in C_k} (y_i - \hat{y}_i)^2}{n_k}\)</span>, and <span class="math inline">\(\hat{y}_i\)</span> is the fit for observation <span class="math inline">\(i\)</span>, obtained from the data with part <span class="math inline">\(k\)</span> removed.</p>
<ul>
<li class="fragment">Special case: Setting <span class="math inline">\(K = n\)</span> yields <span class="math inline">\(n\)</span>-fold or <em>leave-one-out cross-validation</em> (LOOCV).</li>
</ul>
</div>
</section>
<section id="leave-one-out-cross-validation-loocv" class="slide level2 center">
<h2>Leave-One-Out Cross-Validation (LOOCV)</h2>
<p><br></p>

<img data-src="figs/LOOCV.gif" class="quarto-figure quarto-figure-center r-stretch" style="width:30.0%"><p><br></p>
<center>
<a href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)" target="_blank">Wiki</a>
</center>
<p><br></p>
</section>
<section id="a-nice-special-case" class="slide level2 center">
<h2>A Nice Special Case!</h2>
<p>With least-squares linear or polynomial regression, an amazing shortcut makes the cost of LOOCV the same as that of a single model fit! The following formula holds:</p>
<p><span class="math display">\[
  \text{CV}_{(n)} = \frac{1}{n} \sum_{i=1}^{n} \left( \frac{y_i - \hat{y}_i}{1 - h_i} \right)^2,
\]</span></p>
<p>where <span class="math inline">\(\hat{y}_i\)</span> is the <span class="math inline">\(i\)</span>-th fitted value from the original least-squares fit, and <span class="math inline">\(h_i\)</span> is the leverage (diagonal of the “hat” matrix; see book for details). This is like the ordinary MSE, except the <span class="math inline">\(i\)</span>-th residual is divided by <span class="math inline">\(1 - h_i\)</span>.</p>
<ul>
<li class="fragment"><p>LOOCV is sometimes useful, but typically doesn’t <em>shake up</em> the data enough. The estimates from each fold are highly correlated, and hence their average can have high variance.</p></li>
<li class="fragment"><p><strong>A better choice is</strong> <span class="math inline">\(K = 5\)</span> or <span class="math inline">\(K = 10\)</span>.</p></li>
</ul>
</section>
<section id="example-auto-data-revisited" class="slide level2 center">
<h2>Example: Auto Data Revisited</h2>

<img data-src="figs/5_4-1.png" class="quarto-figure quarto-figure-center r-stretch" style="width:65.0%"><ul>
<li class="fragment">Left plot: Similar to the two halve validation;</li>
<li class="fragment">Right plot: Tenfold cross validation. With 10 different partitions of the data to train and test the model we see there is not much variability. The results are consistent, in contrast to the result when we divided into two parts.</li>
</ul>
</section>
<section id="true-and-estimated-test-mse-for-the-simulated-data" class="slide level2 center">
<h2>True and Estimated Test MSE for the Simulated Data</h2>
<div style="font-size: 60%;">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figs/5_6-1.png" class="quarto-figure quarto-figure-center" style="width:65.0%"></p>
</figure>
</div>
</div>
</div>
<ul>
<li class="fragment"><p>The plot presents the cross-validation estimates and true test error rates that result from applying smoothing splines to the simulated data sets illustrated in Figures 2.9–2.11 of Chapter 2 of the book.</p></li>
<li class="fragment"><p>The true test MSE is displayed in blue.</p></li>
<li class="fragment"><p>The black dashed and orange solid lines respectively show the estimated LOOCV and 10-fold CV estimates.</p></li>
<li class="fragment"><p>In all three plots, the two cross-validation estimates are very similar.</p></li>
<li class="fragment"><p>Right-hand panel: the true test MSE and the cross-validation curves are almost identical.</p></li>
<li class="fragment"><p>Center panel: the two sets of curves are similar at the lower degrees of flexibility, while the CV curves overestimate the test set MSE for higher degrees of flexibility.</p></li>
<li class="fragment"><p>Left-hand panel: the CV curves have the correct general shape, but they underestimate the true test MSE.</p></li>
</ul>
</div>
<!---With cross-validation, our goal might be to determine how well a given statistical learning procedure can be expected to perform on independent data; in this case, the actual estimate of the test MSE is
of interest. But at other times we are interested only in the location of the minimum point in the estimated test MSE curve. This is because we might be performing cross-validation on a number of statistical learning methods, or on a single method using different levels of flexibility, in order to identify the method that results in the lowest test error.
--->
</section>
<section id="potential-issues-with-cross-validation" class="slide level2 center">
<h2>Potential Issues with Cross-Validation</h2>
<ul>
<li class="fragment"><p>Since each training set is only <span class="math inline">\(\frac{K - 1}{K}\)</span> as big as the original training set, the estimates of prediction error will typically be biased upward. <em>Why?</em></p></li>
<li class="fragment"><p>This bias is minimized when <span class="math inline">\(K = n\)</span> (LOOCV), but this estimate has high variance, as noted earlier.</p></li>
<li class="fragment"><p><span class="math inline">\(K = 5\)</span> or <span class="math inline">\(10\)</span> provides a good compromise for this bias-variance tradeoff.</p></li>
</ul>
</section></section>
<section>
<section id="cross-validation-for-classification-problems" class="title-slide slide level1 center" data-background-color="#cfb991">
<h1>Cross-Validation for Classification Problems</h1>

</section>
<section id="cross-validation-for-classification-problems-1" class="slide level2 center">
<h2>Cross-Validation for Classification Problems</h2>
<div style="font-size: 80%;">
<p>We divide the data into <span class="math inline">\(K\)</span> roughly equal-sized parts <span class="math inline">\(C_1, C_2, \ldots, C_K\)</span>. <span class="math inline">\(C_k\)</span> denotes the indices of the observations in part <span class="math inline">\(k\)</span>. There are <span class="math inline">\(n_k\)</span> observations in part <span class="math inline">\(k\)</span>: if <span class="math inline">\(n\)</span> is a multiple of <span class="math inline">\(K\)</span>, then <span class="math inline">\(n_k = n / K\)</span>.</p>
<ul>
<li class="fragment">Compute the <strong>cross-validation misclassification error</strong>:</li>
</ul>
<div class="fragment">
<p><span class="math display">\[
  \text{CV}_K = \sum_{k=1}^{K} \frac{n_k}{n} \text{Err}_k
\]</span></p>
<p>where <span class="math inline">\(\text{Err}_k = \frac{\sum_{i \in C_k} I(y_i \neq \hat{y}_i)}{n_k}\)</span>.</p>
<ul>
<li class="fragment">The estimated standard deviation of <span class="math inline">\(\text{CV}_K\)</span> is:</li>
</ul>
</div>
<div class="fragment">
<p><span class="math display">\[
  \widehat{\text{SE}}(\text{CV}_K) = \sqrt{\frac{1}{K} \sum_{k=1}^{K} \frac{(\text{Err}_k - \overline{\text{Err}_k})^2}{K - 1}}
\]</span></p>
<ul>
<li class="fragment"><p>This is a useful estimate, but strictly speaking, not quite valid. <em>Why not?</em></p>
<ul>
<li class="fragment">We compute the standard errors assuming these were independent observations, but they are not strictly independent as they share some training samples. So there’s some correlation between them.</li>
</ul></li>
</ul>
</div>
</div>
</section></section>
<section>
<section id="cross-validation-right-and-wrong" class="title-slide slide level1 center" data-background-color="#cfb991">
<h1>Cross-Validation: Right and Wrong</h1>

</section>
<section id="the-setting" class="slide level2 center">
<h2>The Setting</h2>
<ul>
<li class="fragment"><p><strong>High‐dimensional data</strong>: We have 50 samples (observations) but 5000 predictors (features). In many modern applications—such as genomics—it is typical to have many more predictors than observations.</p></li>
<li class="fragment"><p><strong>Goal</strong>: Two‐class classification</p></li>
<li class="fragment"><p>Feature selection (Step 1): We first look at the correlation of each of the 5000 predictors with the class labels, and we pick the 100 “best” predictors—the ones that exhibit the largest correlation with the class labels.</p></li>
<li class="fragment"><p>Model fitting (Step 2): Once those top 100 are chosen, we fit a classifier (e.g., logistic regression) using only those top 100 predictors.</p></li>
<li class="fragment"><p>The question is how to <strong>estimate the true test error</strong> of this two‐step procedure.</p></li>
</ul>
</section>
<section id="the-tempting-but-wrong-approach" class="slide level2 center">
<h2>The Tempting (but Wrong) Approach</h2>
<p>A common mistake is to <strong>ignore Step 1</strong> when doing cross‐validation and to apply cross‐validation <em>only</em> to Step 2. That is, one might simply take the already‐selected 100 features and then do, say, 10‐fold cross‐validation on the logistic regression.</p>
<ul>
<li class="fragment"><p><strong>Why people do this</strong>: It seems natural to say, “Now that we have our 100 features, let’s cross‐validate the classifier we fit with these 100 features.”</p></li>
<li class="fragment"><p><strong>What goes wrong</strong>: By the time you pick those 100 “best” features, the data set has already “seen” all the labels in the process of ranking and filtering. This filtering step is actually part of training, because it <em>used</em> the outcome labels to choose features.</p></li>
</ul>
<div class="fragment">
<p>Skipping Step 1 in the cross‐validation will invariably produce an <strong>overly optimistic</strong> (often <em>wildly</em> optimistic) estimate of test error.</p>
</div>
</section>
<section id="why-it-is-wrong-data-leakage" class="slide level2 center">
<h2>Why It Is Wrong: Data Leakage</h2>
<ol type="1">
<li class="fragment"><p><strong>Data leakage</strong>: The crucial point is that feature selection (filtering) depends on the relationship between each feature and the class labels. Hence, it is not “just a preprocessing step”—it is using the label information. Thus, Step 1 is part of the model‐building process.</p></li>
<li class="fragment"><p><strong>Overfitting by cherry‐picking</strong>: With thousands of predictors, even if none is truly predictive, by sheer chance some predictors will appear correlated with the class labels in the sample. Selecting only the strongest correlations can give the illusion that the model has learned meaningful structure, when in fact it is just capturing random noise.</p></li>
<li class="fragment"><p><strong>An extreme illustration</strong>: If you simulate data where the class labels are purely random (true error = 50%), but you pick the top 100 out of 5000 or 5 million random features, then do cross‐validation <em>only</em> after you have chosen those top 100, you can easily see cross‐validation estimates near 0% error—clearly a false, biased result.</p></li>
</ol>
</section>
<section id="the-correct-right-way-to-apply-crossvalidation" class="slide level2 center">
<h2>The Correct (Right) Way to Apply Cross‐Validation</h2>
<div style="font-size: 80%;">
<p>The key principle is that <em>any step that uses the outcome labels must occur inside the cross‐validation loop</em>. Concretely:</p>
<ol type="1">
<li class="fragment"><p><strong>Split</strong> the data into training/validation folds (e.g., 10‐fold CV).</p></li>
<li class="fragment"><p><strong>For each fold</strong>:</p>
<ul>
<li class="fragment">Treat that fold as a hold‐out set.<br>
</li>
<li class="fragment">On the <em>remaining</em> training folds, perform the <em>entire</em> procedure:
<ol type="1">
<li class="fragment"><strong>Feature selection</strong> (filtering to the top 100 based on correlation with the class labels in the training folds only).<br>
</li>
<li class="fragment"><strong>Fit</strong> the classifier (e.g., logistic regression) to those top 100 features in those training folds.<br>
</li>
</ol></li>
<li class="fragment">Finally, <strong>evaluate</strong> the trained model on the hold‐out fold—<em>with only the 100 features selected from the training folds</em>.</li>
</ul></li>
<li class="fragment"><p><strong>Repeat</strong> for each fold, then average the error rates (or other metrics).</p></li>
</ol>
<div class="fragment">
<p>By doing this, <em>each hold‐out fold is kept separate</em> from both feature selection and model training. This ensures that Step 1 (feature selection) is “relearned” anew in each training subset, just as Step 2 (the classifier) is. As a result, the cross‐validation error you compute properly reflects how the entire procedure—from filtering out thousands of features down to fitting the logistic model—would perform on truly unseen data.</p>
</div>
</div>
</section>
<section id="summary" class="slide level2 center">
<h2>Summary</h2>
<ul>
<li class="fragment"><p><strong>Wrong</strong>: Select your 100 predictors once using <em>all</em> the data, then cross‐validate only the final classifier. This leads to overly optimistic, biased estimates of test error because it ignores that you used the labels in selecting those 100 predictors.</p></li>
<li class="fragment"><p><strong>Right</strong>: Wrap the entire two‐step process (selection <strong>and</strong> model fitting) inside the cross‐validation loop. Each fold’s feature‐selection step must be done <em>without</em> knowledge of the hold‐out fold’s labels.</p></li>
</ul>
<div class="fragment">
<p>Following this correct approach is essential whenever one performs early filtering, variable selection, hyperparameter tuning, or any other step that uses the outcome labels. Such steps must be regarded as part of the training process and repeated inside each cross‐validation iteration.</p>
</div>
</section></section>
<section>
<section id="bootstrap" class="title-slide slide level1 center" data-background-color="#cfb991">
<h1>Bootstrap</h1>

</section>
<section id="the-bootstrap" class="slide level2 center">
<h2>The Bootstrap</h2>
<ul>
<li class="fragment"><p>The <em>bootstrap</em> is a flexible and powerful statistical tool that can be used to quantify the uncertainty associated with a given estimator or statistical learning method.</p></li>
<li class="fragment"><p>For example, it can provide an estimate of the standard error of a coefficient, or a confidence interval for that coefficient.</p></li>
</ul>
</section>
<section id="where-does-the-name-come-from" class="slide level2 center">
<h2>Where Does the Name Come From?</h2>
<ul>
<li class="fragment"><p>The use of the term <em>bootstrap</em> derives from the phrase <em>to pull oneself up by one’s bootstraps</em>, widely thought to be based on one of the eighteenth-century <em>The Surprising Adventures of Baron Munchausen</em> by Rudolph Erich Raspe:</p>
<blockquote>
<p>The Baron had fallen to the bottom of a deep lake. Just when it looked like all was lost, he thought to pick himself up by his own bootstraps.</p>
</blockquote></li>
<li class="fragment"><p>It is not the same as the term <em>bootstrap</em> used in computer science, meaning to “boot” a computer from a set of core instructions, though the derivation is similar.</p></li>
</ul>
</section>
<section id="example" class="slide level2 center">
<h2>Example</h2>
<ul>
<li class="fragment"><p>Suppose that we wish to invest a fixed sum of money in two financial assets that yield returns of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, respectively, where <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are random quantities.</p></li>
<li class="fragment"><p>We will invest a fraction <span class="math inline">\(\alpha\)</span> of our money in <span class="math inline">\(X\)</span>, and will invest the remaining <span class="math inline">\(1 - \alpha\)</span> in <span class="math inline">\(Y\)</span>.</p></li>
<li class="fragment"><p>We wish to choose <span class="math inline">\(\alpha\)</span> to minimize the total risk, or variance, of our investment. In other words, we want to minimize <span class="math inline">\(\text{Var}(\alpha X + (1 - \alpha) Y).\)</span></p></li>
<li class="fragment"><p>One can show that the value that minimizes the risk is given by:</p></li>
</ul>
<div class="fragment">
<p><span class="math display">\[
  \alpha = \frac{\sigma_Y^2 - \sigma_{XY}}{\sigma_X^2 + \sigma_Y^2 - 2\sigma_{XY}},
\]</span></p>
<p>where <span class="math inline">\(\sigma_X^2 = \text{Var}(X)\)</span>, <span class="math inline">\(\sigma_Y^2 = \text{Var}(Y)\)</span>, and <span class="math inline">\(\sigma_{XY} = \text{Cov}(X, Y)\)</span>.</p>
</div>
</section>
<section id="example-continued" class="slide level2 center">
<h2>Example Continued</h2>
<ul>
<li class="fragment"><p>But the values of <span class="math inline">\(\sigma_X^2\)</span>, <span class="math inline">\(\sigma_Y^2\)</span>, and <span class="math inline">\(\sigma_{XY}\)</span> are unknown.</p></li>
<li class="fragment"><p>We can compute estimates for these quantities, <span class="math inline">\(\hat{\sigma}_X^2\)</span>, <span class="math inline">\(\hat{\sigma}_Y^2\)</span>, and <span class="math inline">\(\hat{\sigma}_{XY}\)</span>, using a data set that contains measurements for <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p></li>
<li class="fragment"><p>We can then estimate the value of <span class="math inline">\(\alpha\)</span> that minimizes the variance of our investment using:</p></li>
</ul>
<div class="fragment">
<p><span class="math display">\[
  \hat{\alpha} = \frac{\hat{\sigma}_Y^2 - \hat{\sigma}_{XY}}{\hat{\sigma}_X^2 + \hat{\sigma}_Y^2 - 2\hat{\sigma}_{XY}}.
\]</span></p>
</div>
</section>
<section id="example-continued-1" class="slide level2 center">
<h2>Example Continued</h2>
<div style="font-size: 80%;">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figs/5_9-1.png" class="quarto-figure quarto-figure-center" style="width:40.0%"></p>
</figure>
</div>
</div>
</div>
<p>Each panel displays 100 simulated returns for investments X and Y. From left to right and top to bottom, the resulting estimates for <span class="math inline">\(\alpha\)</span>, the fraction to minimize the total risk, are 0.576, 0.532, 0.657, and 0.651.</p>
<p><br></p>
</div>
</section>
<section id="example-continued-2" class="slide level2 center">
<h2>Example Continued</h2>
<div style="font-size: 80%;">
<ul>
<li class="fragment"><p>To estimate the standard deviation of <span class="math inline">\(\hat{\alpha}\)</span>, we repeated the process of simulating 100 paired observations of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, and estimating <span class="math inline">\(\alpha\)</span> 1,000 times.</p></li>
<li class="fragment"><p>We thereby obtained 1,000 estimates for <span class="math inline">\(\alpha\)</span>, which we can call <span class="math inline">\(\hat{\alpha}_1, \hat{\alpha}_2, \ldots, \hat{\alpha}_{1000}\)</span>.</p></li>
</ul>
<div class="fragment">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="figs/5_10-1.png" class="quarto-figure quarto-figure-center" style="width:65.0%"></p>
</figure>
</div>
</div>
</div>
</div>
<ul>
<li class="fragment"><p>The left-hand panel of the Figure displays a histogram of the resulting estimates.</p></li>
<li class="fragment"><p>For these simulations, the parameters were set to <span class="math inline">\(\sigma_X^2 = 1, \, \sigma_Y^2 = 1.25, \, \sigma_{XY} = 0.5,\)</span> and so we know that the true value of <span class="math inline">\(\alpha\)</span> is 0.6 (indicated by the red line).</p></li>
</ul>
</div>
</section>
<section id="example-continued-3" class="slide level2 center">
<h2>Example Continued</h2>
<p>The mean over all 1,000 estimates for <span class="math inline">\(\alpha\)</span> is:</p>
<p><span class="math display">\[
  \bar{\alpha} = \frac{1}{1000} \sum_{r=1}^{1000} \hat{\alpha}_r = 0.5996,
\]</span></p>
<p>very close to <span class="math inline">\(\alpha = 0.6\)</span>, and the standard deviation of the estimates is:</p>
<div class="fragment">
<p><span class="math display">\[
  \sqrt{\frac{1}{1000 - 1} \sum_{r=1}^{1000} (\hat{\alpha}_r - \bar{\alpha})^2} = 0.083.
\]</span></p>
<ul>
<li class="fragment"><p>This gives us a very good idea of the accuracy of <span class="math inline">\(\hat{\alpha}\)</span>: <span class="math inline">\(\text{SE}(\hat{\alpha}) \approx 0.083\)</span>.</p></li>
<li class="fragment"><p>So roughly speaking, for a random sample from the population, we would expect <span class="math inline">\(\hat{\alpha}\)</span> to differ from <span class="math inline">\(\alpha\)</span> by approximately 0.08, on average.</p></li>
</ul>
</div>
</section>
<section id="example-results" class="slide level2 center">
<h2>Example Results</h2>

<img data-src="figs/5_10-1.png" class="quarto-figure quarto-figure-center r-stretch" style="width:65.0%"><p><strong>Left</strong>: A histogram of the estimates of <span class="math inline">\(\alpha\)</span> obtained by generating 1,000 simulated data sets from the true population.</p>
<p><strong>Center</strong>: A histogram of the estimates of <span class="math inline">\(\alpha\)</span> obtained from 1,000 bootstrap samples from a single data set.</p>
<p><strong>Right</strong>: The estimates of <span class="math inline">\(\alpha\)</span> displayed in the left and center panels are shown as boxplots.</p>
<ul>
<li class="fragment">In each panel, the pink line indicates the true value of <span class="math inline">\(\alpha\)</span>.</li>
</ul>
</section>
<section id="now-back-to-the-real-world" class="slide level2 center">
<h2>Now Back to the Real World</h2>
<ul>
<li class="fragment"><p>The procedure outlined above cannot be applied, because for real data we cannot generate new samples from the original population.</p></li>
<li class="fragment"><p>However, the bootstrap approach allows us to use a computer to mimic the process of obtaining new data sets, so that we can estimate the variability of our estimate without generating additional samples.</p></li>
<li class="fragment"><p>Rather than repeatedly obtaining independent data sets <em>from the population</em>, <em>we instead obtain distinct data sets by repeatedly sampling observations from the original data set</em> <strong>with replacement</strong>.</p></li>
<li class="fragment"><p>Each of these “bootstrap data sets” is created by sampling <strong>with replacement</strong>, and is the <strong>same size</strong> as our original dataset. As a result, some observations may appear more than once in a given bootstrap data set and some not at all.</p></li>
</ul>
</section>
<section id="example-with-just-3-observations" class="slide level2 center">
<h2>Example with Just 3 Observations</h2>

<img data-src="figs/5_11-1.png" class="quarto-figure quarto-figure-center r-stretch" style="width:65.0%"><ul>
<li class="fragment">A graphical illustration of the bootstrap approach on a small sample containing <span class="math inline">\(n = 3\)</span> observations.</li>
<li class="fragment">Each bootstrap data set contains <span class="math inline">\(n\)</span> observations, sampled <strong>with replacement</strong> from the original data set.</li>
<li class="fragment">Each bootstrap data set is used to obtain an estimate of <span class="math inline">\(\alpha\)</span>.</li>
</ul>
</section>
<section id="bootstrap-standard-error" class="slide level2 center">
<h2>Bootstrap Standard Error</h2>
<ul>
<li class="fragment"><p>Denoting the first bootstrap data set by <span class="math inline">\(Z^{*1}\)</span>, we use <span class="math inline">\(Z^{*1}\)</span> to produce a new bootstrap estimate for <span class="math inline">\(\alpha\)</span>, which we call <span class="math inline">\(\hat{\alpha}^{*1}\)</span>.</p></li>
<li class="fragment"><p>This procedure is repeated <span class="math inline">\(B\)</span> times for some large value of <span class="math inline">\(B\)</span> (say 100 or 1000), in order to produce <span class="math inline">\(B\)</span> different bootstrap data sets, <span class="math inline">\(Z^{*1}, Z^{*2}, \ldots, Z^{*B}\)</span>, and <span class="math inline">\(B\)</span> corresponding <span class="math inline">\(\alpha\)</span> estimates, <span class="math inline">\(\hat{\alpha}^{*1}, \hat{\alpha}^{*2}, \ldots, \hat{\alpha}^{*B}\)</span>.</p></li>
<li class="fragment"><p>We estimate the standard error of these bootstrap estimates using the formula:</p></li>
</ul>
<div class="fragment">
<p><span class="math display">\[
SE_B(\hat{\alpha}) = \sqrt{\frac{1}{B - 1} \sum_{r=1}^B (\hat{\alpha}^{*r} - \bar{\alpha}^{*})^2}.
\]</span></p>
<ul>
<li class="fragment"><p>This serves as an estimate of the standard error of <span class="math inline">\(\hat{\alpha}\)</span> estimated from the original data set. See center and right panels of Figure on slide 29. Bootstrap results are in blue.</p></li>
<li class="fragment"><p>For this example <span class="math inline">\(SE_B(\hat{\alpha}) = 0.087\)</span>.</p></li>
</ul>
</div>
</section></section>
<section>
<section id="more-on-bootstrap" class="title-slide slide level1 center" data-background-color="#cfb991">
<h1>More on Bootstrap</h1>

</section>
<section id="a-general-picture-for-the-bootstrap" class="slide level2 center">
<h2>A General Picture for the Bootstrap</h2>

<img data-src="figs/5_1_2-1.png" class="quarto-figure quarto-figure-center r-stretch" style="width:85.0%"><div style="font-size: 50%;">
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Real World</strong></p>
<ol type="1">
<li><strong>Population</strong> <span class="math inline">\(P\)</span>
<ul>
<li>We imagine there is a true, unknown population (or data‐generating process).<br>
</li>
<li>In practice, we typically do <em>not</em> have direct access to all of <span class="math inline">\(P\)</span>.</li>
</ul></li>
<li><strong>Random Sampling</strong>
<ul>
<li>We draw a finite sample <span class="math inline">\(Z = (z_1, z_2, \dots, z_n)\)</span> from the population <span class="math inline">\(P\)</span>.<br>
</li>
<li>This sample <span class="math inline">\(Z\)</span> is our observed dataset (often called the “training data” in applied work).</li>
</ul></li>
<li><strong>Estimate</strong> <span class="math inline">\(f(Z)\)</span>
<ul>
<li>From this observed data <span class="math inline">\(Z\)</span>, we compute a statistic or estimate, denoted <span class="math inline">\(f(Z)\)</span>.<br>
</li>
<li>Examples might include a mean, a regression coefficient, or (in the investment example) an optimal allocation parameter <span class="math inline">\(\alpha\)</span>.</li>
</ul></li>
</ol>
<p>In short, the Real World side shows how our single dataset <span class="math inline">\(Z\)</span> arrives by randomly sampling from the true population <span class="math inline">\(P\)</span>.</p>
</div><div class="column" style="width:50%;">
<p><strong>Bootstrap World</strong></p>
<ol type="1">
<li><strong>Estimated Population</strong> <span class="math inline">\(\hat{P}\)</span>
<ul>
<li>Because we usually cannot sample repeatedly from the real population <span class="math inline">\(P\)</span>, the bootstrap creates a stand‐in population <span class="math inline">\(\hat{P}\)</span>. We ‘replace’ the population by our sample.</li>
<li><span class="math inline">\(\hat{P}\)</span> is the <strong>empirical distribution function</strong> of the observed data <span class="math inline">\(Z\)</span>. Informally, it assigns probability <span class="math inline">\(\tfrac{1}{n}\)</span> to each observed point in <span class="math inline">\(Z\)</span>.</li>
</ul></li>
<li><strong>Random Sampling from</strong> <span class="math inline">\(\hat{P}\)</span>
<ul>
<li>To mimic drawing new data from the real population, we instead draw (with replacement) from <span class="math inline">\(\hat{P}\)</span>.<br>
</li>
<li>This produces a <em>bootstrap dataset</em> <span class="math inline">\(Z^* = (z_1^*, z_2^*, \dots, z_n^*)\)</span>. Each <span class="math inline">\(z_i^*\)</span> is sampled (with replacement) from among the original observed points <span class="math inline">\(\{z_1, \dots, z_n\}\)</span>.</li>
</ul></li>
<li><strong>Bootstrap Estimate</strong> <span class="math inline">\(f(Z^*)\)</span>
<ul>
<li>We compute the same statistic (or estimator) on each bootstrap sample, giving <span class="math inline">\(f(Z^*)\)</span>.<br>
</li>
<li>By repeating this bootstrap sampling many times, we obtain a distribution of estimates <span class="math inline">\(\{f(Z^*_1), f(Z^*_2), \dots\}\)</span>. This approximates how <span class="math inline">\(f(Z)\)</span> would vary if we could repeatedly resample from the <em>true</em> population.</li>
</ul></li>
</ol>
<!---
Hence, the Bootstrap World side shows how the empirical distribution $\hat{P}$ and resampling with replacement enable us to emulate “new” datasets—thereby providing a practical way to estimate variability, confidence intervals, or standard errors of our statistic $f(Z)$ without ever returning to the true population $P$.
--->
</div></div>
</div>
</section>
<section id="the-bootstrap-in-general" class="slide level2 center">
<h2>The Bootstrap in General</h2>
<ul>
<li class="fragment"><p>In more complex data situations, figuring out the appropriate way to generate bootstrap samples can require some thought.</p></li>
<li class="fragment"><p>For example, if the data is a time series, we can’t simply sample the observations with replacement (<strong>why not?</strong>).</p>
<ul>
<li class="fragment">The main reason we typically cannot simply resample individual points with replacement in a time series is that time‐ordered data exhibits serial dependence. That is, adjacent observations (e.g., today’s stock price and yesterday’s stock price) are correlated in ways that we lose if we treat all observations as independent units and shuffle them arbitrarily.</li>
<li class="fragment">A simple i.i.d. bootstrap would ignore the natural ordering of the data points (and the correlations it encodes), thereby violating a crucial assumption about the structure of time‐series data.</li>
</ul></li>
<li class="fragment"><p>We can instead create blocks of consecutive observations and sample those with replacements. Then we paste together sampled blocks to obtain a bootstrap dataset.</p></li>
</ul>
</section>
<section id="other-uses-of-the-bootstrap" class="slide level2 center">
<h2>Other Uses of the Bootstrap</h2>
<ul>
<li class="fragment"><p>Primarily used to obtain standard errors of an estimate.</p></li>
<li class="fragment"><p>Also provides approximate confidence intervals for a population parameter. For example, looking at the histogram in the middle panel of the figure on slide 29, the 5% and 95% quantiles of the 1,000 values is (0.43, 0.72).</p></li>
<li class="fragment"><p>This represents an approximate 90% confidence interval for the true α. <em>How do we interpret this confidence interval?</em></p></li>
<li class="fragment"><p>The above interval is called a <strong>Bootstrap Percentile</strong> confidence interval. It is the simplest method (among many approaches) for obtaining a confidence interval from the bootstrap.</p></li>
</ul>
</section></section>
<section>
<section id="can-the-bootstrap-estimate-prediction-error" class="title-slide slide level1 center" data-background-color="#cfb991">
<h1>Can the Bootstrap Estimate Prediction Error?</h1>

</section>
<section id="can-the-bootstrap-estimate-prediction-error-1" class="slide level2 center">
<h2>Can the Bootstrap Estimate Prediction Error?</h2>
<ul>
<li class="fragment"><p>In cross-validation, each of the <span class="math inline">\(K\)</span> validation folds is distinct from the other <span class="math inline">\(K-1\)</span> folds used for training: <em>there is no overlap.</em> This is crucial for its success. <strong>Why?</strong></p>
<ul>
<li class="fragment">There is a clear separation, no overlap, between the train and the test sets.</li>
</ul></li>
<li class="fragment"><p>To estimate prediction error using the bootstrap, we could think about using <strong>each bootstrap dataset as our training sample</strong>, and <strong>the original sample as our validation sample</strong>.</p></li>
<li class="fragment"><p>But each bootstrap sample has significant overlap with the original data. About two-thirds of the original data points appear in each bootstrap sample.</p></li>
<li class="fragment"><p>This will cause the bootstrap to seriously underestimate the true prediction error.</p></li>
<li class="fragment"><p>The other way around— with the original sample as the training sample, and the bootstrap dataset as the validation sample— is worse!</p></li>
</ul>
</section>
<section id="removing-the-overlap" class="slide level2 center">
<h2>Removing the Overlap</h2>
<ul>
<li class="fragment"><p>Can partly fix this problem by only using predictions for those observations that did not (by chance) occur in the current bootstrap sample.</p></li>
<li class="fragment"><p>But the method gets complicated, and in the end, cross-validation provides a simpler, more attractive approach for estimating prediction error.</p></li>
</ul>
<!---
## Pre-validation

-   In microarray and other genomic studies, an important problem is to compare a predictor of disease outcome derived from a large number of “biomarkers” to standard clinical predictors.

-   Comparing them on the same dataset that was used to derive the biomarker predictor can lead to results strongly biased in favor of the biomarker predictor.

-   *Pre-validation* can be used to make a fairer comparison between the two sets of predictors.

## Motivating Example

An example of this problem arose in the paper of van’t Veer *et al.* *Nature* (2002). Their microarray data has 4918 genes measured over 78 cases, taken from a study of breast cancer. There are 44 cases in the good prognosis group and 34 in the poor prognosis group. A “microarray” predictor was constructed as follows:

1.  **70 genes were selected**, having the largest absolute correlation with the 78 class labels.

2.  **Using these 70 genes**, a nearest-centroid classifier $C(x)$ was constructed.

3.  **Applying the classifier** to the 78 microarrays gave a dichotomous predictor $z_i = C(x_i)$ for each case $i$.

## Results

Comparison of the microarray predictor with some clinical predictors, using logistic regression with outcome *prognosis*:

| Model      | Coef   | Stand. Err. | Z score | p-value |
|------------|--------|-------------|---------|---------|
| **Re-use** |        |             |         |         |
| microarray | 4.096  | 1.092       | 3.753   | 0.000   |
| angio      | 1.208  | 0.816       | 1.482   | 0.069   |
| er         | -0.554 | 1.044       | -0.530  | 0.298   |
| grade      | -0.697 | 1.003       | -0.695  | 0.243   |
| pr         | 1.214  | 1.057       | 1.149   | 0.125   |
| age        | -1.593 | 0.911       | -1.748  | 0.040   |
| size       | 1.483  | 0.732       | 2.026   | 0.021   |

| Model             | Coef   | Stand. Err. | Z score | p-value |
|-------------------|--------|-------------|---------|---------|
| **Pre-validated** |        |             |         |         |
| microarray        | 1.549  | 0.675       | 2.296   | 0.011   |
| angio             | 1.589  | 0.682       | 2.329   | 0.010   |
| er                | -0.617 | 0.894       | -0.690  | 0.245   |
| grade             | 0.719  | 0.720       | 0.999   | 0.159   |
| pr                | 0.537  | 0.863       | 0.622   | 0.267   |
| age               | -1.471 | 0.701       | -2.099  | 0.018   |
| size              | 0.998  | 0.594       | 1.681   | 0.046   |

## Idea behind Pre-validation

-   Designed for comparison of adaptively derived predictors to fixed, pre-defined predictors.

-   The idea is to form a "pre-validated" version of the adaptive predictor: specifically, a "fairer" version that hasn’t "seen" the response $y$.

## Pre-validation Process


::: {.cell layout-align="center"}
::: {.cell-output-display}
![](figs/5_1_3-1.png){fig-align='center' width=65%}
:::
:::



-   Observations are used to create predictors, with some data omitted.
-   Pre-validated predictors are derived without access to the response.
-   Logistic regression is applied to pre-validated predictors and fixed predictors.

## Pre-validation in Detail for This Example

1.  Divide the cases up into $K = 13$ equal-sized parts of 6 cases each.

2.  Set aside one of the parts. Using only the data from the other 12 parts:

    -   Select the features having an absolute correlation of at least 0.3 with the class labels.
    -   Form a nearest centroid classification rule.

3.  Use the rule to predict the class labels for the 13th part.

4.  Repeat steps 2 and 3 for each of the 13 parts, yielding a “pre-validated” microarray predictor $\tilde{z}_i$ for each of the 78 cases.

5.  Fit a logistic regression model to the pre-validated microarray predictor and the 6 clinical predictors.

## The Bootstrap versus Permutation Tests

-   **Bootstrap**:
    -   Samples from the estimated population and uses the results to estimate standard errors and confidence intervals.
-   **Permutation Methods**:
    -   Sample from an estimated *null* distribution for the data.
    -   Used to estimate p-values and False Discovery Rates for hypothesis tests.
-   **Bootstrap for Null Hypothesis Testing**:
    -   Can test a null hypothesis in simple situations.
    -   Example: If $\theta = 0$ is the null hypothesis, check whether the confidence interval for $\theta$ contains zero.
-   **Adapting Bootstrap for Null Distribution**:
    -   Can adapt bootstrap to sample from a null distribution.
    -   See Efron and Tibshirani, *An Introduction to the Bootstrap* (1993), Chapter 16.
    -   However, there is no real advantage over permutation tests.

--->
</section></section>
<section>
<section id="summary-1" class="title-slide slide level1 center" data-background-color="#cfb991">
<h1>Summary</h1>

</section>
<section id="summary-2" class="slide level2 center">
<h2>Summary</h2>
<div style="font-size: 60%;">
<div class="columns">
<div class="column" style="width:50%;">
<h3 id="resampling-methods">Resampling Methods</h3>
<ul>
<li>Cross-validation and Bootstrap allow evaluation of model performance using existing data.</li>
<li>They provide estimates of:
<ul>
<li>Test-set prediction error</li>
<li>Standard deviation and bias of parameter estimates.</li>
</ul></li>
</ul>
<h3 id="training-vs-test-error">Training vs Test Error</h3>
<ul>
<li>Training error decreases with model complexity.</li>
<li>Test error decreases, then increases due to <strong>bias-variance tradeoff</strong>:
<ul>
<li><strong>High Bias</strong>: Simple models underfit the data.</li>
<li><strong>High Variance</strong>: Complex models overfit the training data.</li>
</ul></li>
<li>Optimal complexity minimizes test error.</li>
</ul>
</div><div class="column" style="width:50%;">
<h3 id="validation-set-approach-2">Validation-Set Approach</h3>
<ul>
<li>Divides data into training and validation sets.</li>
<li>Validation error provides an estimate of test error but:
<ul>
<li>Can vary based on data split.</li>
<li>May overestimate test error due to smaller training sets.</li>
</ul></li>
</ul>
<h3 id="cross-validation-1">Cross-Validation</h3>
<ul>
<li><strong>K-Fold Cross-Validation</strong>:
<ul>
<li>Divides data into <span class="math inline">\(K\)</span> folds for iterative training and testing.</li>
<li>Balances bias and variance (e.g., <span class="math inline">\(K = 5\)</span> or <span class="math inline">\(10\)</span>).</li>
</ul></li>
<li><strong>Leave-One-Out Cross-Validation (LOOCV)</strong>:
<ul>
<li>Uses one data point as validation in each iteration.</li>
<li>Low bias but high variance.</li>
</ul></li>
</ul>
<h3 id="bootstrap-1">Bootstrap</h3>
<ul>
<li>Estimates variability and uncertainty of parameter estimates.</li>
<li>Generates multiple samples with replacement from the dataset.</li>
<li>Provides approximate confidence intervals and standard errors.</li>
</ul>
</div></div>
</div>
</section></section>
<section id="thank-you" class="title-slide slide level1 center" data-background-color="#cfb991">
<h1>Thank you!</h1>


</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">
<p>Predictive Analytics</p>
</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":false},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'slide',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'fade',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1600,

        height: 900,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    
    <script>
      // htmlwidgets need to know to resize themselves when slides are shown/hidden.
      // Fire the "slideenter" event (handled by htmlwidgets.js) when the current
      // slide changes (different for each slide format).
      (function () {
        // dispatch for htmlwidgets
        function fireSlideEnter() {
          const event = window.document.createEvent("Event");
          event.initEvent("slideenter", true, true);
          window.document.dispatchEvent(event);
        }

        function fireSlideChanged(previousSlide, currentSlide) {
          fireSlideEnter();

          // dispatch for shiny
          if (window.jQuery) {
            if (previousSlide) {
              window.jQuery(previousSlide).trigger("hidden");
            }
            if (currentSlide) {
              window.jQuery(currentSlide).trigger("shown");
            }
          }
        }

        // hookup for slidy
        if (window.w3c_slidy) {
          window.w3c_slidy.add_observer(function (slide_num) {
            // slide_num starts at position 1
            fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);
          });
        }

      })();
    </script>

    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
          const outerScaffold = trigger.parentElement.cloneNode(true);
          const codeEl = outerScaffold.querySelector('code');
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp('/' + window.location.host + '/');
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>